{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:23.886336Z",
     "start_time": "2018-08-02T17:09:22.772583Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import itertools\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "import nept\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "from loading_data import get_data\n",
    "from analyze_tuning_curves import get_only_tuning_curves\n",
    "from analyze_decode_bytrial import decode_trial\n",
    "from analyze_decode import get_decoded_zones\n",
    "from utils_maze import find_zones, get_trials, get_zones, get_trial_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:23.891627Z",
     "start_time": "2018-08-02T17:09:23.886336Z"
    }
   },
   "outputs": [],
   "source": [
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"check_decode\")\n",
    "if not os.path.exists(output_filepath):\n",
    "    os.makedirs(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:37:35.090177Z",
     "start_time": "2018-08-02T17:37:35.086202Z"
    }
   },
   "outputs": [],
   "source": [
    "import info.r063d2 as info\n",
    "import info.r063d6 as r063d6\n",
    "infos = [r063d6]\n",
    "\n",
    "from run import spike_sorted_infos\n",
    "# infos = spike_sorted_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:23.925607Z",
     "start_time": "2018-08-02T17:09:23.918610Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_tuning_curves(info, tuning_curves):\n",
    "    xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "    multiple_tuning_curves = np.zeros(tuning_curves[0].shape)\n",
    "    cmap = plt.cm.get_cmap('bone_r', 25)\n",
    "    plt.figure()\n",
    "\n",
    "    for tuning_curve in tuning_curves:\n",
    "        multiple_tuning_curves += tuning_curve\n",
    "\n",
    "    pp = plt.pcolormesh(xx, yy, multiple_tuning_curves, cmap=cmap)\n",
    "    plt.colorbar(pp)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    title = info.session_id + '-tuning_curve-all'\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:23.935604Z",
     "start_time": "2018-08-02T17:09:23.927606Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_counts(counts):\n",
    "    fig = plt.figure(figsize=(6, 7))\n",
    "    ax = plt.subplot(111)\n",
    "    pp = plt.pcolormesh(counts.data.T, cmap='bone_r')\n",
    "    plt.colorbar(pp)\n",
    "    ax.set_xticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.get_yaxis().tick_left()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:23.945595Z",
     "start_time": "2018-08-02T17:09:23.936600Z"
    }
   },
   "outputs": [],
   "source": [
    "shuffled_id = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:24.565099Z",
     "start_time": "2018-08-02T17:09:23.947594Z"
    }
   },
   "outputs": [],
   "source": [
    "events, position, spikes, _, _ = get_data(info)\n",
    "\n",
    "phase = info.task_times[\"phase3\"]\n",
    "trials = get_trials(events, phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T18:03:25.042287Z",
     "start_time": "2018-08-02T18:03:16.150478Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_errors = []\n",
    "all_proportions = []\n",
    "\n",
    "session_ids = []\n",
    "\n",
    "for info in infos:\n",
    "    session_ids.append(info.session_id)\n",
    "\n",
    "    session_errors = []\n",
    "    session_proportions = []\n",
    "\n",
    "    for trial in trials[:10]:\n",
    "        epoch_of_interest = phase.excludes(trial)\n",
    "\n",
    "        tuning_curves = get_only_tuning_curves(position, \n",
    "                                               spikes, \n",
    "                                               info.xedges, \n",
    "                                               info.yedges, \n",
    "                                               epoch_of_interest)\n",
    "\n",
    "        if shuffled_id:\n",
    "            tuning_curves = np.random.permutation(tuning_curves)\n",
    "\n",
    "    #     plot_tuning_curves(info, tuning_curves)\n",
    "\n",
    "        sliced_position = position.time_slice(trial.start, trial.stop)\n",
    "    #     print(\"n_times in trial:\", sliced_position.n_samples)\n",
    "\n",
    "        sliced_spikes = [spiketrain.time_slice(trial.start, \n",
    "                                               trial.stop) for spiketrain in spikes]\n",
    "\n",
    "\n",
    "        # limit position and spikes to only times when the subject is running\n",
    "        run_epoch = nept.run_threshold(sliced_position, thresh=8., t_smooth=0.8)\n",
    "        sliced_position = sliced_position[run_epoch]\n",
    "    #     print(\"n_times running:\", sliced_position.n_samples)\n",
    "    #     plt.plot(sliced_position.x, sliced_position.y, \"k.\")\n",
    "    #     plt.show()\n",
    "\n",
    "        n_spikes = 0\n",
    "        for spiketrain in sliced_spikes:\n",
    "            n_spikes += spiketrain.n_spikes\n",
    "    #     print(\"n_spikes in trial:\", n_spikes)\n",
    "\n",
    "        sliced_spikes = [spiketrain.time_slice(run_epoch.start, \n",
    "                                               run_epoch.stop) for spiketrain in sliced_spikes]\n",
    "\n",
    "        n_spikes = 0\n",
    "        for spiketrain in sliced_spikes:\n",
    "            n_spikes += spiketrain.n_spikes\n",
    "    #     print(\"n_spikes running:\", n_spikes)\n",
    "\n",
    "    #     epochs_interest = nept.Epoch(np.array([sliced_position.time[0], sliced_position.time[-1]]))\n",
    "\n",
    "        counts = nept.bin_spikes(sliced_spikes, sliced_position.time, dt=0.025, window=0.025,\n",
    "                                 gaussian_std=0.0075, normalized=False)\n",
    "\n",
    "    #     plot_counts(counts)\n",
    "\n",
    "        min_neurons = 3\n",
    "\n",
    "        tc_shape = tuning_curves.shape\n",
    "        decoding_tc = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "        likelihood = nept.bayesian_prob(counts, decoding_tc, binsize=0.025, min_neurons=min_neurons)\n",
    "    #     print(\"bins with prob:\", likelihood.size - np.sum(np.isnan(likelihood)))\n",
    "\n",
    "        # Find decoded location based on max likelihood for each valid timestep\n",
    "        xcenters = (info.xedges[1:] + info.xedges[:-1]) / 2.\n",
    "        ycenters = (info.yedges[1:] + info.yedges[:-1]) / 2.\n",
    "        xy_centers = nept.cartesian(xcenters, ycenters)\n",
    "\n",
    "        decoded = nept.decode_location(likelihood, xy_centers, counts.time)\n",
    "\n",
    "    #     print(\"n_decoded:\", decoded.n_samples)\n",
    "        session_proportions.append(decoded.n_samples/len(counts.time))\n",
    "    #     print(\"Proportion decoded: %.2f\" % (decoded.n_samples/len(counts.time)))\n",
    "\n",
    "    #     session_decoded.append(decoded)\n",
    "\n",
    "    #     # Remove nans from likelihood and reshape for plotting\n",
    "    #     keep_idx = np.sum(np.isnan(likelihood), axis=1) < likelihood.shape[1]\n",
    "    #     likelihood = likelihood[keep_idx]\n",
    "    #     likelihood = likelihood.reshape(np.shape(likelihood)[0], tc_shape[1], tc_shape[2])\n",
    "\n",
    "    #     session_likelihoods.append(likelihood)\n",
    "\n",
    "    #     n_active_neurons = np.asarray([n_active if n_active >= min_neurons else 0 \n",
    "    #                                    for n_active in np.sum(counts.data >= 1, axis=1)])\n",
    "    #     n_active_neurons = n_active_neurons[keep_idx]\n",
    "    #     session_n_active.append(n_active_neurons)\n",
    "\n",
    "        f_xy = scipy.interpolate.interp1d(sliced_position.time, sliced_position.data.T, kind=\"nearest\")\n",
    "        counts_xy = f_xy(decoded.time)\n",
    "        true_position = nept.Position(np.hstack((counts_xy[0][..., np.newaxis],\n",
    "                                                 counts_xy[1][..., np.newaxis])),\n",
    "                                      decoded.time)\n",
    "\n",
    "    #     session_actual.append(true_position)\n",
    "\n",
    "        trial_errors = true_position.distance(decoded)\n",
    "        session_errors.extend(trial_errors)\n",
    "    #     print(\"mean error: %.fcm\" % np.mean(trial_errors))\n",
    "\n",
    "#     print(\"Proportion decoded: %.2f\" % np.mean(session_proportions))\n",
    "#     print(\"mean error: %.fcm\" % np.mean(session_errors))\n",
    "\n",
    "    all_errors.append(session_errors)\n",
    "    all_proportions.append(session_proportions)\n",
    "    #     for error, x, y in zip(trial_errors, true_position.x, true_position.y):\n",
    "    #         x_idx = nept.find_nearest_idx(xcenters, x)\n",
    "    #         y_idx = nept.find_nearest_idx(ycenters, y)\n",
    "    #         error_byactual_position[y_idx][x_idx] += error\n",
    "    #         n_byactual_position[y_idx][x_idx] += 1\n",
    "\n",
    "    #     session_errors.append(trial_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T18:04:56.980455Z",
     "start_time": "2018-08-02T18:04:56.974460Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(all_proportions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T18:05:11.506850Z",
     "start_time": "2018-08-02T18:05:11.407250Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pos = np.arange(len(session_ids))\n",
    "plt.bar(y_pos, np.mean(all_proportions, axis=1), align='center', alpha=0.7)\n",
    "plt.xticks(y_pos, session_ids, rotation=90, fontsize=10)\n",
    "plt.ylabel('Proportion')\n",
    "plt.title(\"Samples decoded with %d cm bins\" % 12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.051973Z",
     "start_time": "2018-08-02T17:09:26.042978Z"
    }
   },
   "outputs": [],
   "source": [
    "counts.time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.061967Z",
     "start_time": "2018-08-02T17:09:26.053972Z"
    }
   },
   "outputs": [],
   "source": [
    "likelihood.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.072961Z",
     "start_time": "2018-08-02T17:09:26.062966Z"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_prob(counts, tuning_curves, binsize, min_neurons, min_spikes=1):\n",
    "    \"\"\"Computes the bayesian probability of location based on spike counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    counts : nept.AnalogSignal\n",
    "        Where each inner array is the number of spikes (int) in each bin for an individual neuron.\n",
    "    tuning_curves : np.array\n",
    "        Where each inner array is the tuning curve (floats) for an individual neuron.\n",
    "    binsize : float\n",
    "        Size of the time bins.\n",
    "    min_neurons : int\n",
    "        Mininum number of neurons active in a given bin.\n",
    "    min_spikes : int\n",
    "        Mininum number of spikes in a given bin.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prob : np.array\n",
    "        Where each inner array is the probability (floats) for an individual neuron by location bins.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If a bin does not meet the min_neuron/min_spikes requirement, that bin's probability\n",
    "    is set to nan. To convert it to 0s instead, use : prob[np.isnan(prob)] = 0 on the output.\n",
    "\n",
    "    \"\"\"\n",
    "    n_time_bins = np.shape(counts.time)[0]\n",
    "    n_position_bins = np.shape(tuning_curves)[1]\n",
    "\n",
    "    likelihood = np.empty((n_time_bins, n_position_bins)) * np.nan\n",
    "\n",
    "    # Ignore warnings when inf created in this loop\n",
    "    error_settings = np.seterr(over='ignore')\n",
    "    for idx in range(n_position_bins):\n",
    "        valid_idx = tuning_curves[:, idx] > 1  # log of 1 or less is negative or invalid\n",
    "        if np.any(valid_idx):\n",
    "            # event_rate is the lambda in this poisson distribution\n",
    "            event_rate = tuning_curves[valid_idx, idx, np.newaxis].T ** counts.data[:, valid_idx]\n",
    "            prior = np.exp(-binsize * np.sum(tuning_curves[valid_idx, idx]))\n",
    "\n",
    "            # Below is the same as\n",
    "            # likelihood[:, idx] = np.prod(event_rate, axis=0) * prior * (1/n_position_bins)\n",
    "            # only less likely to have floating point issues, though slower\n",
    "            likelihood[:, idx] = np.exp(np.sum(np.log(event_rate), axis=1)) * prior * (1/n_position_bins)\n",
    "    np.seterr(**error_settings)\n",
    "\n",
    "    # Set any inf value to be largest float\n",
    "    largest_float = np.finfo(float).max\n",
    "    likelihood[np.isinf(likelihood)] = largest_float\n",
    "    likelihood /= np.nansum(likelihood, axis=1)[..., np.newaxis]\n",
    "    \n",
    "    print(likelihood.shape)\n",
    "\n",
    "    # Remove bins with too few neurons that that are active\n",
    "    # a neuron is considered active by having at least min_spikes in a bin\n",
    "    n_active_neurons = np.sum(counts.data >= min_spikes, axis=1)\n",
    "    likelihood[n_active_neurons < min_neurons] = np.nan\n",
    "\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.082955Z",
     "start_time": "2018-08-02T17:09:26.074959Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_location(likelihood, pos_centers, time_centers):\n",
    "    \"\"\"Finds the decoded location based on the centers of the position bins.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    likelihood : np.array\n",
    "        With shape(n_timebins, n_positionbins)\n",
    "    pos_centers : np.array\n",
    "    time_centers : np.array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    decoded : nept.Position\n",
    "        Estimate of decoded position.\n",
    "\n",
    "    \"\"\"\n",
    "    keep_idx = np.sum(np.isnan(likelihood), axis=1) < likelihood.shape[1]\n",
    "    likelihood = likelihood[keep_idx]\n",
    "\n",
    "    max_decoded_idx = np.nanargmax(likelihood, axis=1)\n",
    "\n",
    "    decoded_data = pos_centers[max_decoded_idx]\n",
    "\n",
    "    decoded_time = time_centers[keep_idx]\n",
    "\n",
    "    return nept.Position(decoded_data, decoded_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.131927Z",
     "start_time": "2018-08-02T17:09:26.084954Z"
    }
   },
   "outputs": [],
   "source": [
    "likelihood = bayesian_prob(counts, decoding_tc, binsize=0.025, min_neurons=min_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.139922Z",
     "start_time": "2018-08-02T17:09:26.133926Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_centers = xy_centers\n",
    "time_centers = counts.time\n",
    "dl = decode_location(likelihood, pos_centers, time_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.147918Z",
     "start_time": "2018-08-02T17:09:26.141921Z"
    }
   },
   "outputs": [],
   "source": [
    "dl.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.158911Z",
     "start_time": "2018-08-02T17:09:26.148917Z"
    }
   },
   "outputs": [],
   "source": [
    "keep_idx = np.sum(np.isnan(likelihood), axis=1) < likelihood.shape[1]\n",
    "np.sum(keep_idx)\n",
    "ll = likelihood[keep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.304828Z",
     "start_time": "2018-08-02T17:09:26.160910Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.305827Z",
     "start_time": "2018-08-02T17:09:22.835Z"
    }
   },
   "outputs": [],
   "source": [
    "ll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.306826Z",
     "start_time": "2018-08-02T17:09:22.838Z"
    }
   },
   "outputs": [],
   "source": [
    "max_decoded_idx = np.nanargmax(ll, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.307826Z",
     "start_time": "2018-08-02T17:09:22.842Z"
    }
   },
   "outputs": [],
   "source": [
    "max_decoded_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.308826Z",
     "start_time": "2018-08-02T17:09:22.845Z"
    }
   },
   "outputs": [],
   "source": [
    "cc = nept.AnalogSignal(counts.data[10:20], counts.time[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.309825Z",
     "start_time": "2018-08-02T17:09:22.849Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 7))\n",
    "ax = plt.subplot(111)\n",
    "pp = plt.pcolormesh(cc.data.T, cmap='bone_r')\n",
    "plt.colorbar(pp)\n",
    "ax.set_xticks([])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.get_yaxis().tick_left()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.310824Z",
     "start_time": "2018-08-02T17:09:22.853Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(np.sum(counts.data >=1, axis=1) >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.311823Z",
     "start_time": "2018-08-02T17:09:22.855Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(counts.data >=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.312824Z",
     "start_time": "2018-08-02T17:09:22.859Z"
    }
   },
   "outputs": [],
   "source": [
    "likelihood.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T17:09:26.313823Z",
     "start_time": "2018-08-02T17:09:22.865Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(np.sum(likelihood >0, axis=1) >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
