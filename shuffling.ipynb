{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T15:47:45.333995Z",
     "start_time": "2018-09-14T15:47:38.045175Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import scalebar\n",
    "import nept\n",
    "\n",
    "from loading_data import get_data\n",
    "from analyze_tuning_curves import get_only_tuning_curves\n",
    "from utils_plotting import plot_over_space\n",
    "from utils_maze import get_zones, get_bin_centers, get_matched_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T15:47:45.352947Z",
     "start_time": "2018-09-14T15:47:45.336957Z"
    }
   },
   "outputs": [],
   "source": [
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"trials\", \"decoding\", \"shuffled\")\n",
    "if not os.path.exists(output_filepath):\n",
    "    os.makedirs(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T15:47:45.993927Z",
     "start_time": "2018-09-14T15:47:45.355945Z"
    }
   },
   "outputs": [],
   "source": [
    "import info.r063d2 as r063d2\n",
    "import info.r063d3 as r063d3\n",
    "infos = [r063d2, r063d3]\n",
    "from run import analysis_infos\n",
    "# infos = analysis_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T15:47:46.004893Z",
     "start_time": "2018-09-14T15:47:45.996920Z"
    }
   },
   "outputs": [],
   "source": [
    "def bin_spikes(spikes, time, dt, window=None, gaussian_std=None, normalized=True):\n",
    "    \"\"\"Bins spikes using a sliding window.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spikes: list\n",
    "        Of nept.SpikeTrain\n",
    "    time: np.array\n",
    "    window: float or None\n",
    "        Length of the sliding window, in seconds. If None, will default to dt.\n",
    "    dt: float\n",
    "    gaussian_std: float or None\n",
    "    normalized: boolean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    binned_spikes: nept.AnalogSignal\n",
    "\n",
    "    \"\"\"\n",
    "    if window is None:\n",
    "        window = dt\n",
    "\n",
    "    bin_edges = time\n",
    "\n",
    "    given_n_bins = window / dt\n",
    "    n_bins = int(round(given_n_bins))\n",
    "    if abs(n_bins - given_n_bins) > 0.01:\n",
    "        warnings.warn(\"dt does not divide window evenly. \"\n",
    "                      \"Using window %g instead.\" % (n_bins*dt))\n",
    "\n",
    "    if normalized:\n",
    "        square_filter = np.ones(n_bins) * (1 / n_bins)\n",
    "    else:\n",
    "        square_filter = np.ones(n_bins)\n",
    "\n",
    "    counts = np.zeros((len(spikes), len(bin_edges) - 1))\n",
    "    for idx, spiketrain in enumerate(spikes):\n",
    "        counts[idx] = np.convolve(np.histogram(spiketrain.time, bins=bin_edges)[0].astype(float),\n",
    "                                  square_filter, mode=\"same\")\n",
    "\n",
    "    if gaussian_std is not None:\n",
    "        counts = nept.gaussian_filter(counts, gaussian_std, dt=dt, normalized=normalized, axis=1)\n",
    "\n",
    "    return nept.AnalogSignal(counts, bin_edges[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T20:23:40.994928Z",
     "start_time": "2018-09-14T20:23:40.975957Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_summary_individual(info, likelihood_true, likelihood_shuffs, position, lfp, spikes, start, stop, \n",
    "                            zones, maze_segments, colours, filepath=None, savefig=False):\n",
    "    buffer=0.1\n",
    "    \n",
    "    means = [np.nansum(likelihood[zones[trajectory]]) for trajectory in maze_segments]\n",
    "    \n",
    "#     combined_shuff = {trajectory: [] for trajectory in maze_segments}\n",
    "#     for likelihood_shuff in likelihood_shuffs:\n",
    "#         for trajectory in maze_segments:\n",
    "#             combined_shuff[trajectory].append(np.nansum(likelihood_shuffs[zones[trajectory]], axis=0))\n",
    "    means_shuff = [np.nansum(likelihood_shuff[zones[trajectory]], axis=0) for trajectory in maze_segments]\n",
    "    sems_shuff = [scipy.stats.sem(likelihood_shuff[zones[trajectory]], axis=0, nan_policy=\"omit\") for trajectory in maze_segments]\n",
    "\n",
    "    sliced_spikes = [spiketrain.time_slice(start-buffer, stop+buffer) for spiketrain in spikes]\n",
    "\n",
    "    rows = len(sliced_spikes)\n",
    "    add_rows = int(rows / 8)\n",
    "\n",
    "    ms = 600 / rows\n",
    "    mew = 0.7\n",
    "    spike_loc = 1\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs1 = gridspec.GridSpec(3, 2)\n",
    "    gs1.update(wspace=0.3, hspace=0.3)\n",
    "\n",
    "    ax1 = plt.subplot(gs1[1:, 0])\n",
    "    for idx, neuron_spikes in enumerate(sliced_spikes):\n",
    "        ax1.plot(neuron_spikes.time, np.ones(len(neuron_spikes.time)) + (idx * spike_loc), '|',\n",
    "                 color='k', ms=ms, mew=mew)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2 = plt.subplot(gs1[0, 0], sharex=ax1)\n",
    "\n",
    "    swr_highlight = \"#fc4e2a\"\n",
    "    start_idx = nept.find_nearest_idx(lfp.time, start - buffer)\n",
    "    stop_idx = nept.find_nearest_idx(lfp.time, stop + buffer)\n",
    "    ax2.plot(lfp.time[start_idx:stop_idx], lfp.data[start_idx:stop_idx], color=\"k\", lw=0.3, alpha=0.9)\n",
    "\n",
    "    start_idx = nept.find_nearest_idx(lfp.time, start)\n",
    "    stop_idx = nept.find_nearest_idx(lfp.time, stop)\n",
    "    ax2.plot(lfp.time[start_idx:stop_idx], lfp.data[start_idx:stop_idx], color=swr_highlight, lw=0.6)\n",
    "    ax2.axis(\"off\")\n",
    "    \n",
    "    ax1.axvline(lfp.time[start_idx], linewidth=1, color=swr_highlight)\n",
    "    ax1.axvline(lfp.time[stop_idx], linewidth=1, color=swr_highlight)\n",
    "    ax1.axvspan(lfp.time[start_idx], lfp.time[stop_idx], alpha=0.5, color=swr_highlight)\n",
    "\n",
    "    scalebar.add_scalebar(ax2, matchy=False, bbox_transform=fig.transFigure,\n",
    "                          bbox_to_anchor=(0.25, 0.05), units='ms')\n",
    "\n",
    "    likelihood[np.isnan(likelihood)] = 0\n",
    "    \n",
    "    xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "    xcenters, ycenters = get_bin_centers(info)\n",
    "    xxx, yyy = np.meshgrid(xcenters, ycenters)\n",
    "\n",
    "    maze_highlight = \"#fed976\"\n",
    "    ax3 = plt.subplot(gs1[0, 1])\n",
    "    sliced_position = position.time_slice(info.task_times[\"phase3\"].starts, info.task_times[\"phase3\"].stops)\n",
    "    ax3.plot(sliced_position.x, sliced_position.y, \".\", color=maze_highlight, ms=1, alpha=0.2)\n",
    "    pp = ax3.pcolormesh(xx, yy, likelihood_true, cmap='bone_r')\n",
    "    ax3.contour(xxx, yyy, zones[\"u\"], levels=0, colors=colours[\"u\"])\n",
    "    ax3.contour(xxx, yyy, zones[\"shortcut\"], levels=0, colors=colours[\"shortcut\"])\n",
    "    ax3.contour(xxx, yyy, zones[\"novel\"], levels=0, colors=colours[\"novel\"])\n",
    "    plt.colorbar(pp)\n",
    "    ax3.axis('off')\n",
    "\n",
    "    ax4 = plt.subplot(gs1[1:2, 1])\n",
    "    n = np.arange(len(maze_segments))\n",
    "    ax4.bar(n, means, \n",
    "            color=[colours[\"u\"], colours[\"shortcut\"], colours[\"novel\"], colours[\"other\"]], edgecolor='k')\n",
    "    ax4.set_xticks(n)\n",
    "    ax4.set_xticklabels([], rotation=90)\n",
    "    ax4.set_ylim([0, 1.])\n",
    "    ax4.set_title(\"True proportion\", fontsize=14)\n",
    "    \n",
    "    ax5 = plt.subplot(gs1[2:, 1], sharey=ax4)\n",
    "    n = np.arange(len(maze_segments))\n",
    "    ax5.bar(n, np.nanmean(means_shuff, axis=0), \n",
    "            yerr=np.nanmean(sems_shuff, axis=0), \n",
    "            color=[colours[\"u\"], colours[\"shortcut\"], colours[\"novel\"], colours[\"other\"]], edgecolor='k')\n",
    "    ax5.set_xticks(n)\n",
    "    ax5.set_xticklabels(maze_segments, rotation=90)\n",
    "    ax5.set_ylim([0, 1.])\n",
    "    ax5.set_title(\"Shuffled proportion\", fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if savefig:\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T20:23:51.137835Z",
     "start_time": "2018-09-14T20:23:50.169008Z"
    }
   },
   "outputs": [],
   "source": [
    "tuning_curves = tuning_curves_fromdata\n",
    "tuning_curves = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "start = phase_swrs[task_time].starts[0]\n",
    "stop = phase_swrs[task_time].stops[0]\n",
    "\n",
    "likelihood = get_likelihood(spikes, tuning_curves, tc_shape, start, stop)\n",
    "\n",
    "likelihood_shuffs = []\n",
    "\n",
    "for _ in range(3):\n",
    "    tuning_curves_shuff = np.random.permutation(tuning_curves_fromdata)\n",
    "    tuning_curves_shuff = tuning_curves_shuff.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "    likelihood_shuff = get_likelihood(spikes, tuning_curves_shuff, tc_shape, start, stop)\n",
    "    \n",
    "    likelihood_shuffs.append(likelihood_shuff)\n",
    "\n",
    "filename = info.session_id + \"_\" + task_time + \"_summary-swr\" + str(i) + \"_sum.png\"\n",
    "filepath = os.path.join(output_filepath, \"swr\", filename)\n",
    "plot_summary_individual(info, likelihood, likelihood_shuffs, position, lfp, spikes, start, stop,\n",
    "                        zones, maze_segments, colours, filepath, savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T17:02:29.077235Z",
     "start_time": "2018-09-14T17:02:29.069239Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_likelihood_overspace(info, position, likelihoods, zones, colours, filepath=None):\n",
    "    \n",
    "    xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "    xcenters, ycenters = get_bin_centers(info)\n",
    "    xxx, yyy = np.meshgrid(xcenters, ycenters)\n",
    "    \n",
    "    sliced_position = position.time_slice(info.task_times[\"phase3\"].starts, info.task_times[\"phase3\"].stops)\n",
    "    plt.plot(sliced_position.x, sliced_position.y, \"b.\", ms=1, alpha=0.2)\n",
    "    pp = plt.pcolormesh(xx, yy, np.nanmean(likelihoods, axis=0), vmax=0.2, cmap='bone_r')\n",
    "    plt.contour(xxx, yyy, zones[\"u\"], levels=0, colors=colours[\"u\"], corner_mask=False)\n",
    "    plt.contour(xxx, yyy, zones[\"shortcut\"], levels=0, colors=colours[\"shortcut\"], corner_mask=False)\n",
    "    plt.contour(xxx, yyy, zones[\"novel\"], levels=0, colors=colours[\"novel\"], corner_mask=False)\n",
    "\n",
    "    plt.colorbar(pp)\n",
    "    plt.axis('off')\n",
    "    if filepath is not None:\n",
    "        plt.savefig(os.path.join(output_filepath, filename+\".png\"))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T17:02:29.346037Z",
     "start_time": "2018-09-14T17:02:29.340040Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_likelihood(spikes, tuning_curves, tc_shape, start, stop):\n",
    "    sliced_spikes = [spiketrain.time_slice(start, stop) for spiketrain in spikes]\n",
    "    t_window = stop-start # 0.1 for running, 0.025 for swr\n",
    "    counts = bin_spikes(sliced_spikes, np.array([start, stop]), dt=t_window, window=t_window,\n",
    "                        gaussian_std=0.0075, normalized=False)\n",
    "    likelihood = nept.bayesian_prob(counts, tuning_curves, binsize=t_window, min_neurons=3, min_spikes=1)\n",
    "\n",
    "    return likelihood.reshape(tc_shape[1], tc_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T17:02:29.794968Z",
     "start_time": "2018-09-14T17:02:29.779976Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_combined(summary_likelihoods, n_all_swrs, task_times, maze_segments, n_sessions, colours, filename=None):\n",
    "    \n",
    "    trajectory_means = {key: [] for key in maze_segments}\n",
    "    trajectory_sems = {key: [] for key in maze_segments}\n",
    "    max_val = 0\n",
    "    \n",
    "    for trajectory in maze_segments:\n",
    "        for task_time in task_times:\n",
    "            if len(summary_likelihoods[task_time][trajectory]) > 0:\n",
    "                max_val = max(np.max(summary_likelihoods[task_time][trajectory]), max_val)\n",
    "\n",
    "        trajectory_means[trajectory] = [np.nanmean(summary_likelihoods[task_time][trajectory]) for task_time in task_times]\n",
    "        trajectory_sems[trajectory] = [scipy.stats.sem(summary_likelihoods[task_time][trajectory]) for task_time in task_times] \n",
    "\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    gs1 = gridspec.GridSpec(1, 4)\n",
    "    gs1.update(wspace=0.3, hspace=0.)\n",
    "\n",
    "    n = np.arange(len(task_times))\n",
    "    ax1 = plt.subplot(gs1[0])\n",
    "    ax1.bar(n, trajectory_means[\"u\"], yerr=trajectory_sems[\"u\"], color=colours[\"u\"])\n",
    "    ax2 = plt.subplot(gs1[1])\n",
    "    ax2.bar(n, trajectory_means[\"shortcut\"], yerr=trajectory_sems[\"shortcut\"], color=colours[\"shortcut\"])\n",
    "    ax3 = plt.subplot(gs1[2])\n",
    "    ax3.bar(n, trajectory_means[\"novel\"], yerr=trajectory_sems[\"novel\"], color=colours[\"novel\"])\n",
    "    ax4 = plt.subplot(gs1[3])\n",
    "    ax4.bar(n, trajectory_means[\"other\"], yerr=trajectory_sems[\"other\"], color=colours[\"other\"])\n",
    "    \n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ax.set_ylim([0, max_val])\n",
    "        \n",
    "        ax.set_xticks(np.arange(len(task_times)))\n",
    "        ax.set_xticklabels(task_times, rotation = 90)\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "        for i, task_time in enumerate(task_times):\n",
    "            ax.text(i, 0.01, str(n_all_swrs[task_time]), ha=\"center\", fontsize=14)\n",
    "\n",
    "    for ax in [ax2, ax3, ax4]:\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "    plt.text(1., max_val, \"n sessions: \"+ str(n_sessions), horizontalalignment='left', \n",
    "             verticalalignment='top', fontsize=14)\n",
    "\n",
    "    fig.suptitle(filename, fontsize=18)\n",
    "#     ax1.set_ylabel(\"Proportion\")\n",
    "\n",
    "    legend_elements = [Patch(facecolor=colours[\"u\"], edgecolor='k', label=\"u\"),\n",
    "                       Patch(facecolor=colours[\"shortcut\"], edgecolor='k', label=\"shortcut\"),\n",
    "                       Patch(facecolor=colours[\"novel\"], edgecolor='k', label=\"novel\"),\n",
    "                       Patch(facecolor=colours[\"other\"], edgecolor='k', label=\"other\")]\n",
    "    plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1.0))\n",
    "    \n",
    "    gs1.tight_layout(fig)\n",
    "    \n",
    "    if filename is not None:\n",
    "        plt.savefig(os.path.join(output_filepath, filename+\".png\"))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T17:02:30.494779Z",
     "start_time": "2018-09-14T17:02:30.484785Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_stacked_summary(summary_likelihoods, n_all_swrs, task_times, maze_segments, n_sessions, colours, filename=None):\n",
    "    \n",
    "    trajectory_means = {key: [] for key in maze_segments}\n",
    "    trajectory_sems = {key: [] for key in maze_segments}\n",
    "\n",
    "    for trajectory in maze_segments:              \n",
    "        trajectory_means[trajectory] = [np.nanmean(summary_likelihoods[task_time][trajectory]) for task_time in task_times]\n",
    "        trajectory_sems[trajectory] = [scipy.stats.sem(summary_likelihoods[task_time][trajectory]) for task_time in task_times] \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    n = np.arange(len(task_times))\n",
    "    pu = plt.bar(n, trajectory_means[\"u\"], yerr=trajectory_sems[\"u\"], color=colours[\"u\"])\n",
    "    ps = plt.bar(n, trajectory_means[\"shortcut\"], yerr=trajectory_sems[\"shortcut\"],\n",
    "                 bottom=trajectory_means[\"u\"], color=colours[\"shortcut\"])\n",
    "    pn = plt.bar(n, trajectory_means[\"novel\"], yerr=trajectory_sems[\"novel\"],\n",
    "                 bottom=np.array(trajectory_means[\"u\"])+np.array(trajectory_means[\"shortcut\"]), color=colours[\"novel\"])\n",
    "    po = plt.bar(n, trajectory_means[\"other\"], yerr=trajectory_sems[\"other\"],\n",
    "                 bottom=np.array(trajectory_means[\"u\"])+np.array(trajectory_means[\"shortcut\"])+np.array(trajectory_means[\"novel\"]), \n",
    "                 color=colours[\"other\"])\n",
    "    plt.xticks(n, task_times)\n",
    "    plt.title(filename)\n",
    "\n",
    "    for i, task_time in enumerate(task_times):\n",
    "        ax.text(i, 0.01, str(n_all_swrs[task_time]), ha=\"center\", fontsize=14)\n",
    "        \n",
    "    plt.text(2.8, -0.15, \"n sessions: \"+ str(n_sessions), fontsize=14)\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if filename is not None:\n",
    "        plt.savefig(os.path.join(output_filepath, filename+\".png\"))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T19:13:21.453724Z",
     "start_time": "2018-09-14T19:13:21.446729Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_likelihoods(tuning_curves_fromdata, tc_shape, spikes, phase_swrs, shuffled_id=False):\n",
    "    \n",
    "    if shuffled_id:\n",
    "        tuning_curves = np.random.permutation(tuning_curves_fromdata)\n",
    "    else:\n",
    "        tuning_curves = tuning_curves_fromdata\n",
    "\n",
    "    tuning_curves = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "    likelihoods_sum = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    raw_likelihoods = {task_time: [] for task_time in task_times}\n",
    "\n",
    "    for i, task_time in enumerate(task_times):\n",
    "        for (start, stop) in zip(phase_swrs[task_time].starts, phase_swrs[task_time].stops):\n",
    "            likelihood = get_likelihood(spikes, tuning_curves, tc_shape, start, stop)\n",
    "            raw_likelihoods[task_time].append(likelihood)\n",
    "\n",
    "            for trajectory in maze_segments:\n",
    "                likelihoods_sum[task_time][trajectory].append(np.nansum(likelihood[zones[trajectory]]))\n",
    "\n",
    "    return likelihoods_sum, raw_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T19:13:49.297502Z",
     "start_time": "2018-09-14T19:13:21.976550Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set the random seed so results are consistent between runs\n",
    "np.random.seed(0)\n",
    "n_shuffles = 3\n",
    "percentile_thresh = 80\n",
    "\n",
    "colours = dict()\n",
    "colours[\"u\"] = \"#2b8cbe\"\n",
    "colours[\"shortcut\"] = \"#31a354\"\n",
    "colours[\"novel\"] = \"#d95f0e\"\n",
    "colours[\"other\"] = \"#bdbdbd\"\n",
    "\n",
    "# swr params\n",
    "z_thresh = 2.0\n",
    "power_thresh = 3.0\n",
    "merge_thresh = 0.02\n",
    "min_length = 0.05\n",
    "swr_thresh = (140.0, 250.0)\n",
    "\n",
    "task_times = [\"prerecord\", \"pauseA\", \"pauseB\", \"postrecord\"]\n",
    "maze_segments = [\"u\", \"shortcut\", \"novel\", \"other\"]\n",
    "\n",
    "n_sessions = len(infos)\n",
    "# likelihoods_sum = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "all_likelihoods_true = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "all_likelihoods_shuff = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "all_likelihoods_proportion = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "all_likelihoods_true_passthresh = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "\n",
    "n_all_swrs = {task_time: 0 for task_time in task_times}\n",
    "    \n",
    "for info in infos:\n",
    "    print(info.session_id)\n",
    "    events, position, spikes, lfp, _ = get_data(info)\n",
    "    \n",
    "    tuning_curves_fromdata = get_only_tuning_curves(info,\n",
    "                                           position,\n",
    "                                           spikes,\n",
    "                                           info.task_times[\"phase3\"])\n",
    "    \n",
    "    tc_shape = tuning_curves_fromdata.shape\n",
    "    \n",
    "    # Define zones\n",
    "    zones = dict()\n",
    "    zones[\"u\"], zones[\"shortcut\"], zones[\"novel\"] = get_zones(info, position, subset=True)\n",
    "    combined_zones = zones[\"u\"] + zones[\"shortcut\"] + zones[\"novel\"]\n",
    "    zones[\"other\"] = ~combined_zones\n",
    "\n",
    "    # Find SWRs for the whole session\n",
    "    \n",
    "    swrs = nept.detect_swr_hilbert(lfp, fs=info.fs, thresh=swr_thresh, z_thresh=z_thresh,\n",
    "                                   power_thresh=power_thresh, merge_thresh=merge_thresh, min_length=min_length)\n",
    "    swrs = nept.find_multi_in_epochs(spikes, swrs, min_involved=4)\n",
    "\n",
    "    rest_epochs = nept.rest_threshold(position, thresh=12., t_smooth=0.8)\n",
    "\n",
    "    # Restrict SWRs to those during epochs of interest during rest    \n",
    "    phase_swrs = dict()\n",
    "    n_swrs = {task_time: 0 for task_time in task_times}\n",
    "    \n",
    "    for task_time in task_times:\n",
    "        epochs_of_interest = info.task_times[task_time].intersect(rest_epochs)\n",
    "\n",
    "        phase_swrs[task_time] = epochs_of_interest.overlaps(swrs)\n",
    "        phase_swrs[task_time] = phase_swrs[task_time][phase_swrs[task_time].durations >= 0.05]\n",
    "        \n",
    "        n_swrs[task_time] += phase_swrs[task_time].n_epochs\n",
    "        n_all_swrs[task_time] += phase_swrs[task_time].n_epochs\n",
    "    \n",
    "    session_likelihoods_true, raw_likelihoods_true = compute_likelihoods(tuning_curves_fromdata, \n",
    "                                                   tc_shape, \n",
    "                                                   spikes, \n",
    "                                                   phase_swrs, \n",
    "                                                   shuffled_id=False)\n",
    " \n",
    "    combined_likelihoods_shuff = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    raw_likelihoods_shuffs = {task_time: [] for task_time in task_times}\n",
    "    \n",
    "    for i_shuffle in range(n_shuffles):\n",
    "        session_likelihoods_shuff, raw_likelihoods_shuff = compute_likelihoods(tuning_curves_fromdata,\n",
    "                                                        tc_shape,\n",
    "                                                        spikes,\n",
    "                                                        phase_swrs,\n",
    "                                                        shuffled_id=True)\n",
    "        # TODO: reshape raw_likelihoods_shuffs???\n",
    "        for task_time in task_times:\n",
    "            raw_likelihoods_shuffs[task_time].append(raw_likelihoods_shuff[task_time])\n",
    "            for trajectory in maze_segments:\n",
    "                combined_likelihoods_shuff[task_time][trajectory].append(np.array(session_likelihoods_shuff[task_time][trajectory]))\n",
    "    \n",
    "\n",
    "    compareshuffle = {task_time: {trajectory: 0 for trajectory in maze_segments} for task_time in task_times}\n",
    "    percentiles = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    passedshuffthresh = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    \n",
    "    keep_idx = {task_time: [] for task_time in task_times}\n",
    "    \n",
    "    for task_time in task_times:\n",
    "        for trajectory in maze_segments:\n",
    "            for idx, event in enumerate(range(len(session_likelihoods_true[task_time][trajectory]))):\n",
    "                percentile = scipy.stats.percentileofscore(np.sort(np.array(combined_likelihoods_shuff[task_time][trajectory])[:,event]), \n",
    "                                                           session_likelihoods_true[task_time][trajectory][event])\n",
    "                percentiles[task_time][trajectory].append(percentile)\n",
    "                if percentile >= percentile_thresh:\n",
    "                    compareshuffle[task_time][trajectory] += 1\n",
    "                    keep_idx[task_time].append(idx)\n",
    "\n",
    "    morelikelythanshuffle_proportion = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    mean_combined_likelihoods_shuff = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "\n",
    "    for task_time in task_times:\n",
    "        for trajectory in maze_segments:\n",
    "            passedshuffthresh[task_time][trajectory] = np.array(session_likelihoods_true[task_time][trajectory])[np.sort(np.unique(keep_idx[task_time]))]\n",
    "            \n",
    "            morelikelythanshuffle_proportion[task_time][trajectory].append(compareshuffle[task_time][trajectory] / len(session_likelihoods_true[task_time][trajectory]))\n",
    "            mean_combined_likelihoods_shuff[task_time][trajectory] = np.nanmean(combined_likelihoods_shuff[task_time][trajectory], axis=0)\n",
    "            \n",
    "            all_likelihoods_true[task_time][trajectory].extend(session_likelihoods_true[task_time][trajectory])\n",
    "            all_likelihoods_true_passthresh[task_time][trajectory].extend(passedshuffthresh[task_time][trajectory])\n",
    "            all_likelihoods_shuff[task_time][trajectory].extend(mean_combined_likelihoods_shuff[task_time][trajectory])\n",
    "            all_likelihoods_proportion[task_time][trajectory].extend(morelikelythanshuffle_proportion[task_time][trajectory])\n",
    "            \n",
    "            # plot percentiles\n",
    "            fig, ax = plt.subplots()\n",
    "            n = np.arange(len(percentiles[task_time][trajectory]))\n",
    "            plt.bar(n, np.sort(percentiles[task_time][trajectory]), color=colours[trajectory])\n",
    "            ax.axhline(percentile_thresh, ls=\"--\", lw=1.5, color=\"k\")\n",
    "            title = info.session_id + \" individual SWR percentile with shuffle\" + str(n_shuffles) + \" for \" + task_time + \" \" + trajectory\n",
    "            plt.title(title, fontsize=11)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_filepath, \"percentiles\", title))\n",
    "            plt.close()\n",
    "            \n",
    "        filename = info.session_id+\"-average-likelihood-overspace_\"+task_time\n",
    "        if len(session_likelihoods_true[task_time]) > 0:\n",
    "            plot_likelihood_overspace(info, position, raw_likelihoods_true[task_time], zones, colours, filename)\n",
    "            \n",
    "        filename = info.session_id+\"-average-likelihood-overspace_\"+task_time+\"-shuffled\"\n",
    "        if len(session_likelihoods_true[task_time]) > 0:\n",
    "            plot_likelihood_overspace(info, position, raw_likelihoods_shuff[task_time], zones, colours, filename)\n",
    "            \n",
    "    filename = info.session_id + \" proportion of SWRs above \"+str(percentile_thresh)+\" percentile\"\n",
    "    plot_combined(morelikelythanshuffle_proportion, n_swrs, task_times, maze_segments, n_sessions=1, colours=colours, filename=filename)\n",
    "\n",
    "    filename = info.session_id + \" average posteriors during SWRs_sum-shuffled\"+str(n_shuffles)\n",
    "    plot_combined(mean_combined_likelihoods_shuff, n_swrs, task_times, maze_segments, n_sessions=1, colours=colours, filename=filename)\n",
    "\n",
    "    filename = info.session_id + \" average posteriors during SWRs_sum-true\"\n",
    "    plot_combined(session_likelihoods_true, n_swrs, task_times, maze_segments, n_sessions=1, colours=colours, filename=filename)\n",
    "    \n",
    "    filename = info.session_id + \" average posteriors during SWRs_sum-true_passthresh\"\n",
    "    plot_combined(passedshuffthresh, n_swrs, task_times, maze_segments, n_sessions=1, colours=colours, filename=filename)\n",
    "    \n",
    "    for task_time in task_times:\n",
    "        for i, (raw_true, raw_shuffs) in enumerate(zip(raw_likelihoods_true[task_time], raw_likelihoods_shuffs[task_time])):\n",
    "            filename = info.session_id + \"_\" + task_time + \"_summary-swr\" + str(i) + \".png\"\n",
    "            filepath = os.path.join(output_filepath, \"swr\", filename)\n",
    "            plot_summary_individual(info, raw_true, raw_shuffs, position, lfp, spikes, start, stop,\n",
    "                                    zones, maze_segments, colours, filepath, savefig=True)\n",
    "    \n",
    "filename = \"Average posteriors during SWRs_sum-shuffled\"+str(n_shuffles)\n",
    "plot_combined(all_likelihoods_shuff, n_all_swrs, task_times, maze_segments, n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "filename = \"Average posteriors during SWRs_sum-true\"\n",
    "plot_combined(all_likelihoods_true, n_all_swrs, task_times, maze_segments, n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "filename = \"Average posteriors during SWRs_sum-true_passthresh\"\n",
    "plot_combined(all_likelihoods_true_passthresh, n_all_swrs, task_times, maze_segments, n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "filename = \"Average posteriors during SWRs_sum-stacked-shuffled\"+str(n_shuffles)\n",
    "plot_stacked_summary(all_likelihoods_shuff, n_all_swrs, task_times, maze_segments, n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "filename = \"Average posteriors during SWRs_sum-stacked-true\"\n",
    "plot_stacked_summary(all_likelihoods_true, n_all_swrs, task_times, maze_segments, n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "filename = \"Average posteriors during SWRs_sum-stacked-true_passthresh\"\n",
    "plot_stacked_summary(all_likelihoods_true_passthresh, n_all_swrs, task_times, maze_segments, n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "filename = \"Proportion of SWRs above the \"+str(percentile_thresh)+\" percentile (shuffle\" + str(n_shuffles) + \")\"\n",
    "plot_combined(all_likelihoods_proportion, n_swrs, task_times, maze_segments, n_sessions=len(infos), colours=colours, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T20:02:17.141073Z",
     "start_time": "2018-09-14T20:02:17.136096Z"
    }
   },
   "outputs": [],
   "source": [
    "len(raw_likelihoods_true[task_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T20:25:10.486803Z",
     "start_time": "2018-09-14T20:25:10.480806Z"
    }
   },
   "outputs": [],
   "source": [
    "len(np.array(raw_likelihoods_shuffs[task_time][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T20:03:31.965180Z",
     "start_time": "2018-09-14T20:03:31.960201Z"
    }
   },
   "outputs": [],
   "source": [
    "len(raw_likelihoods_shuffs[task_time][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T19:29:28.541017Z",
     "start_time": "2018-09-14T19:29:28.528007Z"
    }
   },
   "outputs": [],
   "source": [
    "len(combined_shuff[task_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T20:01:00.525126Z",
     "start_time": "2018-09-14T20:01:00.496162Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_likelihoods_true[task_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T19:32:36.822258Z",
     "start_time": "2018-09-14T19:32:36.799271Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for task_time in task_times:\n",
    "    for idx in range(phase_swrs[task_time].n_epochs):\n",
    "        filename = info.session_id + \"_\" + task_time + \"_summary-swr\" + str(i) + \".png\"\n",
    "        filepath = os.path.join(output_filepath, \"swr\", filename)\n",
    "        plot_summary_individual(info, raw_likelihoods_true[task_time][idx], \n",
    "                                raw_likelihoods_shuffs[task_time][idx], \n",
    "                                position, lfp, spikes, \n",
    "                                phase_swrs[task_time].starts[idx], \n",
    "                                phase_swrs[task_time].stops[idx],\n",
    "                                zones, maze_segments, colours, filepath, savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T19:33:37.828138Z",
     "start_time": "2018-09-14T19:33:37.823140Z"
    }
   },
   "outputs": [],
   "source": [
    "len(raw_likelihoods_shuffs[\"pauseA\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T19:34:01.764140Z",
     "start_time": "2018-09-14T19:34:01.759160Z"
    }
   },
   "outputs": [],
   "source": [
    "len(raw_likelihoods_true[\"pauseA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T21:04:49.673174Z",
     "start_time": "2018-09-13T18:22:56.544Z"
    }
   },
   "outputs": [],
   "source": [
    "1/0\n",
    "with open(pickled_path, 'wb') as fileobj:\n",
    "    pickle.dump(output, fileobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
