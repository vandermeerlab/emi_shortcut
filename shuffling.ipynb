{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T18:28:41.608211Z",
     "start_time": "2018-09-20T18:28:40.508215Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "import pickle\n",
    "import os\n",
    "import scalebar\n",
    "import nept\n",
    "\n",
    "from loading_data import get_data\n",
    "from analyze_tuning_curves import get_only_tuning_curves\n",
    "from utils_maze import get_zones, get_bin_centers\n",
    "from analyze_decode_swrs import bin_spikes, plot_summary_individual, plot_likelihood_overspace, get_likelihood, plot_combined, plot_stacked_summary, get_likelihoods, save_likelihoods, pickle_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T18:28:41.615035Z",
     "start_time": "2018-09-20T18:28:41.608211Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"trials\", \"decoding\", \"shuffled\")\n",
    "if not os.path.exists(output_filepath):\n",
    "    os.makedirs(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T18:28:41.623031Z",
     "start_time": "2018-09-20T18:28:41.616035Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T19:14:52.306862Z",
     "start_time": "2018-09-20T19:01:48.836148Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_decoded_swr_plots(infos, group):\n",
    "    plot_individual = False\n",
    "    update_cache = False\n",
    "\n",
    "    n_shuffles = 100\n",
    "    percentile_thresh = 99\n",
    "\n",
    "    colours = dict()\n",
    "    colours[\"u\"] = \"#2b8cbe\"\n",
    "    colours[\"shortcut\"] = \"#31a354\"\n",
    "    colours[\"novel\"] = \"#d95f0e\"\n",
    "    colours[\"other\"] = \"#bdbdbd\"\n",
    "\n",
    "    # swr params\n",
    "    z_thresh = 2.0\n",
    "    power_thresh = 3.0\n",
    "    merge_thresh = 0.02\n",
    "    min_length = 0.05\n",
    "    swr_thresh = (140.0, 250.0)\n",
    "\n",
    "    task_times = [\"prerecord\", \"pauseA\", \"pauseB\", \"postrecord\"]\n",
    "    maze_segments = [\"u\", \"shortcut\", \"novel\", \"other\"]\n",
    "\n",
    "    n_sessions = len(infos)\n",
    "    all_likelihoods_true = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    all_likelihoods_shuff = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    all_likelihoods_proportion = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    all_likelihoods_true_passthresh = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    all_likelihoods_true_passthresh_n_swr = {task_time: 0 for task_time in task_times}\n",
    "    all_compareshuffle = {task_time: {trajectory: 0 for trajectory in maze_segments} for task_time in task_times}\n",
    "\n",
    "    n_all_swrs = {task_time: 0 for task_time in task_times}\n",
    "\n",
    "    for info in infos:\n",
    "        print(info.session_id)\n",
    "\n",
    "        events, position, spikes, lfp, _ = get_data(info)\n",
    "\n",
    "        # Define zones\n",
    "        zones = dict()\n",
    "        zones[\"u\"], zones[\"shortcut\"], zones[\"novel\"] = get_zones(info, position, subset=True)\n",
    "        combined_zones = zones[\"u\"] + zones[\"shortcut\"] + zones[\"novel\"]\n",
    "        zones[\"other\"] = ~combined_zones\n",
    "\n",
    "        # Find SWRs for the whole session\n",
    "        swrs_path = os.path.join(pickle_filepath, info.session_id+\"_swrs.pkl\")\n",
    "\n",
    "        # Remove previous pickle if update_cache\n",
    "        if update_cache:\n",
    "            if os.path.exists(swrs_path):\n",
    "                os.remove(swrs_path)\n",
    "\n",
    "        # Load pickle if it exists, otherwise compute and pickle\n",
    "        if os.path.exists(swrs_path):\n",
    "            print(\"Loading pickled true likelihoods...\")\n",
    "            with open(swrs_path, 'rb') as fileobj:\n",
    "                swrs = pickle.load(fileobj)\n",
    "        else:\n",
    "            swrs = nept.detect_swr_hilbert(lfp, fs=info.fs, thresh=swr_thresh, z_thresh=z_thresh,\n",
    "                                           power_thresh=power_thresh, merge_thresh=merge_thresh, min_length=min_length)\n",
    "            swrs = nept.find_multi_in_epochs(spikes, swrs, min_involved=4)\n",
    "\n",
    "        rest_epochs = nept.rest_threshold(position, thresh=12., t_smooth=0.8)\n",
    "\n",
    "        # Restrict SWRs to those during epochs of interest during rest\n",
    "        phase_swrs = dict()\n",
    "        n_swrs = {task_time: 0 for task_time in task_times}\n",
    "\n",
    "        for task_time in task_times:\n",
    "            epochs_of_interest = info.task_times[task_time].intersect(rest_epochs)\n",
    "\n",
    "            phase_swrs[task_time] = epochs_of_interest.overlaps(swrs)\n",
    "            phase_swrs[task_time] = phase_swrs[task_time][phase_swrs[task_time].durations >= 0.05]\n",
    "\n",
    "            n_swrs[task_time] += phase_swrs[task_time].n_epochs\n",
    "            n_all_swrs[task_time] += phase_swrs[task_time].n_epochs\n",
    "\n",
    "        raw_path_true = os.path.join(pickle_filepath, info.session_id+\"_raw-likelihoods_true.pkl\")\n",
    "        sum_path_true = os.path.join(pickle_filepath, info.session_id+\"_sum-likelihoods_true.pkl\")\n",
    "\n",
    "        # Remove previous pickle if update_cache\n",
    "        if update_cache:\n",
    "            if os.path.exists(raw_path_true):\n",
    "                os.remove(raw_path_true)\n",
    "            if os.path.exists(sum_path_true):\n",
    "                os.remove(sum_path_true)\n",
    "\n",
    "        compute_likelihoods = False\n",
    "\n",
    "        # Load pickle if it exists, otherwise compute and pickle\n",
    "        if os.path.exists(raw_path_true) and os.path.exists(sum_path_true):\n",
    "            print(\"Loading pickled true likelihoods...\")\n",
    "            with open(raw_path_true, 'rb') as fileobj:\n",
    "                raw_likelihoods_true = pickle.load(fileobj)\n",
    "            with open(sum_path_true, 'rb') as fileobj:\n",
    "                session_sums_true = pickle.load(fileobj)\n",
    "\n",
    "        combined_likelihoods_shuff = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "        raw_likelihoods_shuffs = {task_time: [] for task_time in task_times}\n",
    "\n",
    "        for i_shuffle in range(n_shuffles):\n",
    "            raw_path_shuff = os.path.join(pickle_filepath,\n",
    "                                          info.session_id+\"_raw-likelihoods_shuffled-%03d.pkl\" % i_shuffle)\n",
    "            sum_path_shuff = os.path.join(pickle_filepath,\n",
    "                                          info.session_id+\"_sum-likelihoods_shuffled-%03d.pkl\" % i_shuffle)\n",
    "\n",
    "            # Remove previous pickle if update_cache\n",
    "            if update_cache:\n",
    "                if os.path.exists(raw_path_shuff):\n",
    "                    os.remove(raw_path_shuff)\n",
    "                if os.path.exists(sum_path_shuff):\n",
    "                    os.remove(sum_path_shuff)\n",
    "\n",
    "            # Load pickle if it exists, otherwise compute and pickle\n",
    "            if os.path.exists(raw_path_shuff) and os.path.exists(sum_path_shuff):\n",
    "                print(\"Loading pickled shuffled likelihoods \"+str(i_shuffle)+\"...\")\n",
    "                with open(raw_path_shuff, 'rb') as fileobj:\n",
    "                    raw_likelihoods_shuff = pickle.load(fileobj)\n",
    "                with open(sum_path_shuff, 'rb') as fileobj:\n",
    "                    session_sums_shuff = pickle.load(fileobj)\n",
    "            else:\n",
    "                compute_likelihoods = True\n",
    "                break\n",
    "\n",
    "            for task_time in task_times:\n",
    "                raw_likelihoods_shuffs[task_time].append(raw_likelihoods_shuff[task_time])\n",
    "                for trajectory in maze_segments:\n",
    "                    combined_likelihoods_shuff[task_time][trajectory].append(np.array(session_sums_shuff[task_time][trajectory]))\n",
    "        else:\n",
    "            compute_likelihoods = True\n",
    "\n",
    "        if compute_likelihoods:\n",
    "            session_sums_true, raw_likelihoods_true, combined_likelihoods_shuff, raw_likelihoods_shuffs = save_likelihoods(info, position, spikes, phase_swrs, zones, task_times, maze_segments, n_shuffles)\n",
    "\n",
    "        compareshuffle = {task_time: {trajectory: 0 for trajectory in maze_segments} for task_time in task_times}\n",
    "        percentiles = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "        passedshuffthresh = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "\n",
    "        keep_idx = {task_time: [] for task_time in task_times}\n",
    "\n",
    "        for task_time in task_times:\n",
    "            raw_likelihoods_shuffs[task_time] = np.swapaxes(raw_likelihoods_shuffs[task_time], 0, 1)\n",
    "            for trajectory in maze_segments:\n",
    "                for idx, event in enumerate(range(len(session_sums_true[task_time][trajectory]))):\n",
    "                    percentile = scipy.stats.percentileofscore(np.sort(np.array(combined_likelihoods_shuff[task_time][trajectory])[:,event]),\n",
    "                                                               session_sums_true[task_time][trajectory][event])\n",
    "                    percentiles[task_time][trajectory].append(percentile)\n",
    "                    if percentile >= percentile_thresh:\n",
    "                        compareshuffle[task_time][trajectory] += 1\n",
    "                        all_compareshuffle[task_time][trajectory] += 1\n",
    "                        keep_idx[task_time].append(idx)\n",
    "\n",
    "        morelikelythanshuffle_proportion = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "        mean_combined_likelihoods_shuff = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "        passedshuffthresh_n_swr = {task_time: 0 for task_time in task_times}\n",
    "\n",
    "        for task_time in task_times:\n",
    "            passedshuffthresh_n_swr[task_time] += len(np.unique(keep_idx[task_time]))\n",
    "            all_likelihoods_true_passthresh_n_swr[task_time] += len(np.unique(keep_idx[task_time]))\n",
    "            for trajectory in maze_segments:\n",
    "                if len(np.sort(np.unique(keep_idx[task_time]))) > 0:\n",
    "                    passedshuffthresh[task_time][trajectory].append(np.array(session_sums_true[task_time][trajectory])[np.sort(np.unique(keep_idx[task_time]))])\n",
    "                if len(session_sums_true[task_time][trajectory]) > 0:\n",
    "                    morelikelythanshuffle_proportion[task_time][trajectory].append(compareshuffle[task_time][trajectory] / len(session_sums_true[task_time][trajectory]))\n",
    "                else:\n",
    "                    morelikelythanshuffle_proportion[task_time][trajectory].append(0.0)\n",
    "                mean_combined_likelihoods_shuff[task_time][trajectory] = np.nanmean(combined_likelihoods_shuff[task_time][trajectory], axis=0)\n",
    "\n",
    "                all_likelihoods_true[task_time][trajectory].extend(session_sums_true[task_time][trajectory])\n",
    "                if len(passedshuffthresh[task_time][trajectory]) > 0:\n",
    "                    all_likelihoods_true_passthresh[task_time][trajectory].append(passedshuffthresh[task_time][trajectory][0])\n",
    "                else:\n",
    "                    all_likelihoods_true_passthresh[task_time][trajectory].append([])\n",
    "                all_likelihoods_shuff[task_time][trajectory].extend(mean_combined_likelihoods_shuff[task_time][trajectory])\n",
    "                all_likelihoods_proportion[task_time][trajectory].extend(morelikelythanshuffle_proportion[task_time][trajectory])\n",
    "\n",
    "                if plot_individual:\n",
    "                    # plot percentiles\n",
    "                    fig, ax = plt.subplots()\n",
    "                    n = np.arange(len(percentiles[task_time][trajectory]))\n",
    "                    plt.bar(n, np.sort(percentiles[task_time][trajectory]), color=colours[trajectory])\n",
    "                    ax.axhline(percentile_thresh, ls=\"--\", lw=1.5, color=\"k\")\n",
    "                    title = info.session_id + \" individual SWR percentile with shuffle\" + str(n_shuffles) + \" for \" + task_time + \" \" + trajectory\n",
    "                    plt.title(title, fontsize=11)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(output_filepath, \"percentiles\", title))\n",
    "                    plt.close()\n",
    "\n",
    "            filepath = os.path.join(output_filepath, info.session_id+\"-average-likelihood-overspace_\"+task_time+\".png\")\n",
    "            if len(session_sums_true[task_time]) > 0:\n",
    "                if plot_individual:\n",
    "                    plot_likelihood_overspace(info, position, raw_likelihoods_true[task_time],\n",
    "                                              zones, colours, filepath)\n",
    "\n",
    "        filename = info.session_id + \" proportion of SWRs above \"+str(percentile_thresh)+\" percentile\"\n",
    "        plot_combined(morelikelythanshuffle_proportion, passedshuffthresh_n_swr,\n",
    "                      task_times, maze_segments, n_sessions=1, colours=colours, filename=filename)\n",
    "\n",
    "        filename = info.session_id + \" average posteriors during SWRs_sum-shuffled\"+str(n_shuffles)\n",
    "        plot_combined(mean_combined_likelihoods_shuff, n_swrs, task_times, maze_segments,\n",
    "                      n_sessions=1, colours=colours, filename=filename)\n",
    "\n",
    "        filename = info.session_id + \" average posteriors during SWRs_sum-true\"\n",
    "        plot_combined(session_sums_true, n_swrs, task_times, maze_segments,\n",
    "                      n_sessions=1, colours=colours, filename=filename)\n",
    "\n",
    "        filename = info.session_id + \" average posteriors during SWRs_sum-true_passthresh\"\n",
    "        plot_combined(passedshuffthresh, n_swrs, task_times, maze_segments,\n",
    "                      n_sessions=1, colours=colours, filename=filename)\n",
    "\n",
    "        if plot_individual:\n",
    "            for task_time in task_times:\n",
    "                for idx in range(phase_swrs[task_time].n_epochs):\n",
    "                    filename = info.session_id + \"_\" + task_time + \"_summary-swr\" + str(idx) + \".png\"\n",
    "                    filepath = os.path.join(output_filepath, \"swr\", filename)\n",
    "                    plot_summary_individual(info, raw_likelihoods_true[task_time][idx],\n",
    "                                            raw_likelihoods_shuffs[task_time][idx],\n",
    "                                            position, lfp, spikes,\n",
    "                                            phase_swrs[task_time].starts[idx],\n",
    "                                            phase_swrs[task_time].stops[idx],\n",
    "                                            zones, maze_segments, colours, filepath, savefig=True)\n",
    "\n",
    "    n_total = {task_time: 0 for task_time in task_times}\n",
    "    for task_time in task_times:\n",
    "        for trajectory in maze_segments:\n",
    "            n_total[task_time] += all_compareshuffle[task_time][trajectory]\n",
    "\n",
    "    all_compareshuffles = {task_time: {trajectory: [] for trajectory in maze_segments} for task_time in task_times}\n",
    "    for task_time in task_times:\n",
    "        for trajectory in maze_segments:\n",
    "            all_compareshuffles[task_time][trajectory].append(all_compareshuffle[task_time][trajectory] / n_total[task_time])\n",
    "\n",
    "\n",
    "    for task_time in task_times:\n",
    "        for trajectory in maze_segments:   \n",
    "            all_likelihoods_true_passthresh[task_time][trajectory] = np.atleast_1d(np.squeeze(np.hstack(all_likelihoods_true_passthresh[task_time][trajectory])))\n",
    "\n",
    "\n",
    "\n",
    "    filename = group + \" average posteriors during SWRs_sum-shuffled\"+str(n_shuffles)\n",
    "    plot_combined(all_likelihoods_shuff, n_all_swrs, task_times, maze_segments,\n",
    "                  n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "    filename = group + \" average posteriors during SWRs_sum-true\"\n",
    "    plot_combined(all_likelihoods_true, n_all_swrs, task_times, maze_segments,\n",
    "                  n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "    filename = group + \" average posteriors during SWRs_sum-true_passthresh\"\n",
    "    plot_combined(all_likelihoods_true_passthresh, all_likelihoods_true_passthresh_n_swr,\n",
    "                  task_times, maze_segments, n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "    filename = group + \" average posteriors during SWRs_sum-true_passthresh-overallproportion\"\n",
    "    plot_combined(all_compareshuffles, all_likelihoods_true_passthresh_n_swr,\n",
    "                  task_times, maze_segments, n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "    filename = group + \"average posteriors during SWRs_sum-stacked-shuffled\"+str(n_shuffles)\n",
    "    plot_stacked_summary(all_likelihoods_shuff, n_all_swrs, task_times, maze_segments,\n",
    "                         n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "    filename = group + \" average posteriors during SWRs_sum-stacked-true\"\n",
    "    plot_stacked_summary(all_likelihoods_true, n_all_swrs, task_times, maze_segments,\n",
    "                         n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "    filename = group + \" average posteriors during SWRs_sum-stacked-true_passthresh\"\n",
    "    plot_stacked_summary(all_likelihoods_true_passthresh, n_all_swrs, task_times,\n",
    "                         maze_segments, n_sessions=len(infos), colours=colours, filename=filename)\n",
    "\n",
    "    filename = group + \" proportion of SWRs above the \"+str(percentile_thresh)+\" percentile (shuffle\" + str(n_shuffles) + \")\"\n",
    "    plot_combined(all_likelihoods_proportion, n_all_swrs, task_times, maze_segments,\n",
    "                  n_sessions=len(infos), colours=colours, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import info.r063d2 as r063d2\n",
    "import info.r068d8 as r068d8\n",
    "# infos = [r068d8, r063d2]\n",
    "# group = \"test\"\n",
    "from run import (analysis_infos,\n",
    "                 r063_infos, r066_infos, r067_infos, r068_infos,\n",
    "                 days1234_infos, days5678_infos,\n",
    "                 day1_infos, day2_infos, day3_infos, day4_infos, day5_infos, day6_infos, day7_infos, day8_infos)\n",
    "# infos = analysis_infos\n",
    "# group = \"All\"\n",
    "\n",
    "# infos = r068_infos\n",
    "# group = \"R068\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_decoded_swr_plots(analysis_infos, \"All\")\n",
    "get_decoded_swr_plots(r063_infos, \"R063\")\n",
    "get_decoded_swr_plots(r066_infos, \"R066\")\n",
    "get_decoded_swr_plots(r067_infos, \"R067\")\n",
    "get_decoded_swr_plots(r068_infos, \"R068\")\n",
    "get_decoded_swr_plots(days1234_infos, \"Days1234\")\n",
    "get_decoded_swr_plots(days5678_infos, \"Days5678\")\n",
    "get_decoded_swr_plots(day1_infos, \"Day1\")\n",
    "get_decoded_swr_plots(day2_infos, \"Day2\")\n",
    "get_decoded_swr_plots(day3_infos, \"Day3\")\n",
    "get_decoded_swr_plots(day4_infos, \"Day4\")\n",
    "get_decoded_swr_plots(day5_infos, \"Day5\")\n",
    "get_decoded_swr_plots(day6_infos, \"Day6\")\n",
    "get_decoded_swr_plots(day7_infos, \"Day7\")\n",
    "get_decoded_swr_plots(day8_infos, \"Day8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
