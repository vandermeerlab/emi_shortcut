{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:48:53.681738Z",
     "start_time": "2018-08-03T16:48:52.498527Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import itertools\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "import nept\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "from loading_data import get_data\n",
    "from analyze_tuning_curves import get_only_tuning_curves\n",
    "from analyze_decode_bytrial import decode_trial\n",
    "from analyze_decode import get_decoded_zones\n",
    "from utils_maze import find_zones, get_trials, get_zones, get_trial_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:48:53.687735Z",
     "start_time": "2018-08-03T16:48:53.683737Z"
    }
   },
   "outputs": [],
   "source": [
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"decode-video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:50:39.724045Z",
     "start_time": "2018-08-03T16:50:39.721047Z"
    }
   },
   "outputs": [],
   "source": [
    "import info.r063d2 as info\n",
    "import info.r063d6 as r063d6\n",
    "# infos = [r063d6]\n",
    "\n",
    "from run import spike_sorted_infos\n",
    "infos = spike_sorted_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:48:54.706296Z",
     "start_time": "2018-08-03T16:48:54.696302Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_errors(all_errors, all_errors_id_shuffled, n_sessions, filename=None):\n",
    "    \n",
    "    all_errors = np.concatenate([np.concatenate(errors, axis=0) for errors in all_errors], axis=0)\n",
    "    all_errors_id_shuffled = np.concatenate([np.concatenate(errors, axis=0) for errors in all_errors_id_shuffled], axis=0)\n",
    "\n",
    "    fliersize = 1\n",
    "\n",
    "    decoded_dict = dict(error=all_errors, label='Decoded')\n",
    "    shuffled_id_dict = dict(error=all_errors_id_shuffled, label='ID shuffled')\n",
    "    decoded_errors = pd.DataFrame(decoded_dict)\n",
    "    shuffled_id = pd.DataFrame(shuffled_id_dict)\n",
    "    data = pd.concat([shuffled_id, decoded_errors])\n",
    "    colours = ['#ffffff', '#bdbdbd']\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    flierprops = dict(marker='o', markersize=fliersize, linestyle='none')\n",
    "    ax = sns.boxplot(x='label', y='error', data=data, flierprops=flierprops)\n",
    "\n",
    "\n",
    "    edge_colour = '#252525'\n",
    "    for i, artist in enumerate(ax.artists):\n",
    "        artist.set_edgecolor(edge_colour)\n",
    "        artist.set_facecolor(colours[i])\n",
    "\n",
    "        for j in range(i*6, i*6+6):\n",
    "            line = ax.lines[j]\n",
    "            line.set_color(edge_colour)\n",
    "            line.set_mfc(edge_colour)\n",
    "            line.set_mec(edge_colour)\n",
    "    \n",
    "    ax.text(1., 1., \"N sessions: %d \\nmean-error: %.1f cm \\nmedian-error: %.1f cm\" % (n_sessions, \n",
    "                                                                                      np.mean(all_errors), \n",
    "                                                                                      np.median(all_errors)),\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='top',\n",
    "            transform = ax.transAxes,\n",
    "            fontsize=10)\n",
    "\n",
    "    ax.set(xlabel=' ', ylabel=\"Error (cm)\")\n",
    "    plt.xticks(fontsize=14)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:48:55.550253Z",
     "start_time": "2018-08-03T16:48:55.544256Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_over_space(values, positions, xedges, yedges):\n",
    "    xcenters = xedges[:-1] + (xedges[1:] - xedges[:-1]) / 2\n",
    "    ycenters = yedges[:-1] + (yedges[1:] - yedges[:-1]) / 2\n",
    "\n",
    "    count_position = np.zeros((len(yedges), len(xedges)))\n",
    "    n_position = np.ones((len(yedges), len(xedges)))\n",
    "\n",
    "    for trial_values, trial_positions in zip(values, positions):\n",
    "        for these_values, x, y in zip(trial_values, trial_positions.x, trial_positions.y):\n",
    "            x_idx = nept.find_nearest_idx(xcenters, x)\n",
    "            y_idx = nept.find_nearest_idx(ycenters, y)\n",
    "            if np.isscalar(these_values):\n",
    "                count_position[y_idx][x_idx] += these_values\n",
    "            else:\n",
    "                count_position[y_idx][x_idx] += these_values[y_idx][x_idx]\n",
    "            n_position[y_idx][x_idx] += 1\n",
    "\n",
    "    return count_position / n_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:48:57.998215Z",
     "start_time": "2018-08-03T16:48:57.978227Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_animation(session_id, decoded, trial_idx, xedge, yedge, filepath):\n",
    "    decoded_position = decoded[\"decoded\"][trial_idx]\n",
    "    true_position = decoded[\"actual\"][trial_idx]\n",
    "    likelihoods = np.array(decoded[\"likelihoods\"][trial_idx])\n",
    "    n_active = decoded[\"n_active\"][trial_idx]\n",
    "    errors = decoded[\"errors\"][trial_idx]\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    gs = gridspec.GridSpec(5, 4)\n",
    "\n",
    "    xx, yy = np.meshgrid(xedge, yedge)\n",
    "\n",
    "    ax1 = plt.subplot2grid((5, 4), (0, 0), colspan=3, rowspan=3)\n",
    "\n",
    "    pad_amount = binsize*2\n",
    "    ax1.set_xlim((np.floor(np.min(true_position.x))-pad_amount, np.ceil(np.max(true_position.x))+pad_amount))\n",
    "    ax1.set_ylim((np.floor(np.min(true_position.y))-pad_amount, np.ceil(np.max(true_position.y))+pad_amount))\n",
    "\n",
    "    n_timebins = decoded_position.n_samples\n",
    "#     n_timebins = 10\n",
    "\n",
    "    n_colours = 20.\n",
    "    colours = [(1., 1., 1.)]\n",
    "    colours.extend(matplotlib.cm.copper_r(np.linspace(0, 1, n_colours-1)))\n",
    "    cmap = matplotlib.colors.ListedColormap(colours)\n",
    "\n",
    "    likelihoods_withnan = np.array(likelihoods)\n",
    "    likelihoods[np.isnan(likelihoods)] = -0.01\n",
    "\n",
    "    xcenters = xedge[:-1] + (xedge[1:] - xedge[:-1]) / 2\n",
    "    ycenters = yedge[:-1] + (yedge[1:] - yedge[:-1]) / 2\n",
    "\n",
    "    x_idx = [nept.find_nearest_idx(xcenters, true_position.x[timestep]) for timestep in range(true_position.n_samples)]\n",
    "    y_idx = [nept.find_nearest_idx(ycenters, true_position.y[timestep]) for timestep in range(true_position.n_samples)]\n",
    "\n",
    "    x_dec_idx = [nept.find_nearest_idx(xcenters, decoded_position.x[timestep]) for timestep in range(decoded_position.n_samples)]\n",
    "    y_dec_idx = [nept.find_nearest_idx(ycenters, decoded_position.y[timestep]) for timestep in range(decoded_position.n_samples)]\n",
    "\n",
    "    posterior_position = ax1.pcolormesh(xx[:-1], yy[:-1], likelihoods[0], vmax=0.2, cmap=cmap)\n",
    "    colorbar = fig.colorbar(posterior_position, ax=ax1)\n",
    "\n",
    "    estimated_position, = ax1.plot([], [], \"o\", color=\"c\")\n",
    "    rat_position, = ax1.plot([], [], \"<\", color=\"b\")\n",
    "\n",
    "    ax2 = plt.subplot2grid((5, 4), (3, 0), colspan=3)\n",
    "\n",
    "    binwidth = 5.\n",
    "    error_bins = np.arange(-binwidth, np.max(errors)+binwidth, binwidth)\n",
    "\n",
    "    _, _, errors_bin = ax2.hist([np.clip(errors, error_bins[0], error_bins[-1])], bins=error_bins, rwidth=0.9, color=\"k\")\n",
    "    errors_idx = np.digitize(errors, error_bins)\n",
    "\n",
    "    fontsize = 14\n",
    "    likelihood_at_actual = ax2.text(0.6, 1, [],\n",
    "             horizontalalignment='left',\n",
    "             verticalalignment='top',\n",
    "             transform = ax2.transAxes,\n",
    "             fontsize=fontsize)\n",
    "\n",
    "    ax2.set_xlabel(\"Error (cm)\", fontsize=fontsize)\n",
    "    ax2.set_ylabel(\"# bins\", fontsize=fontsize)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.yaxis.set_ticks_position('left')\n",
    "    ax2.xaxis.set_ticks_position('bottom')\n",
    "    xticks = binwidth * np.arange(0, len(error_bins), 4)\n",
    "    plt.xticks(xticks, fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "\n",
    "    ax3 = plt.subplot2grid((5, 4), (4, 0), colspan=3)\n",
    "\n",
    "    n_active_bins = np.arange(-0.5, np.max(n_active)+1)\n",
    "\n",
    "    _, _, n_neurons_bin = ax3.hist(n_active, bins=n_active_bins, rwidth=0.9, color=\"k\", align=\"mid\")\n",
    "\n",
    "    ax3.set_xlabel(\"Number of active neurons\", fontsize=fontsize)\n",
    "    ax3.set_ylabel(\"# bins\", fontsize=fontsize)\n",
    "    ax3.spines['right'].set_visible(False)\n",
    "    ax3.spines['top'].set_visible(False)\n",
    "    ax3.yaxis.set_ticks_position('left')\n",
    "    ax3.xaxis.set_ticks_position('bottom')\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "    def init():\n",
    "        posterior_position.set_array([])\n",
    "        estimated_position.set_data([], [])\n",
    "        rat_position.set_data([], [])\n",
    "        likelihood_at_actual.set_text([])\n",
    "        return (posterior_position, estimated_position, rat_position, likelihood_at_actual)\n",
    "\n",
    "\n",
    "    def animate(i):\n",
    "        posterior_position.set_array(likelihoods[i].ravel())\n",
    "        estimated_position.set_data(decoded_position.x[i], decoded_position.y[i])\n",
    "        rat_position.set_data(true_position.x[i], true_position.y[i])\n",
    "\n",
    "        for patch in errors_bin:\n",
    "            patch.set_fc('k')\n",
    "        errors_bin[errors_idx[i]-1].set_fc('r')\n",
    "\n",
    "        for patch in n_neurons_bin:\n",
    "            patch.set_fc('k')\n",
    "        n_neurons_bin[n_active[i]].set_fc('b')\n",
    "\n",
    "        likelihood_at_actual.set_text(\"posterior at true position: %.3f \\nposterior at decoded position: %.3f \" % \n",
    "                                      (likelihoods_withnan[i][y_idx[i]][x_idx[i]], \n",
    "                                       likelihoods_withnan[i][y_dec_idx[i]][x_dec_idx[i]]))\n",
    "\n",
    "        return (posterior_position, estimated_position, rat_position, likelihood_at_actual)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=n_timebins, interval=200, \n",
    "                                   blit=False, repeat=False)\n",
    "\n",
    "\n",
    "    writer = animation.writers['ffmpeg'](fps=10)\n",
    "    dpi = 600\n",
    "    filename = session_id+'_decoded_trial'+str(trial_idx)+'.mp4'\n",
    "    anim.save(os.path.join(filepath, filename), writer=writer, dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:49:04.986981Z",
     "start_time": "2018-08-03T16:49:04.982985Z"
    }
   },
   "outputs": [],
   "source": [
    "binsizes = [12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:49:10.170280Z",
     "start_time": "2018-08-03T16:49:09.480851Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for info in infos:\n",
    "_, position, _, _, _ = get_data(info)\n",
    "for binsize in binsizes:\n",
    "    huh = 0\n",
    "    decoded_filename = info.session_id + '_decoded_binsize' + str(binsize) + 'cm.pkl'\n",
    "    pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "    with open(pickled_decoded, 'rb') as fileobj:\n",
    "        decoded = pickle.load(fileobj)\n",
    "    print(decoded[\"decoded\"][0].n_samples)\n",
    "    for trial in decoded[\"decoded\"]:\n",
    "        huh += trial.n_samples\n",
    "\n",
    "    print(binsize, huh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:49:16.296345Z",
     "start_time": "2018-08-03T16:49:16.290349Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info.session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:49:17.407085Z",
     "start_time": "2018-08-03T16:49:17.403089Z"
    }
   },
   "outputs": [],
   "source": [
    "letstrythis = decoded[\"actual\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:49:22.216758Z",
     "start_time": "2018-08-03T16:49:22.200767Z"
    }
   },
   "outputs": [],
   "source": [
    "middlex = (np.max(position.x) - np.min(position.x)) / 2\n",
    "middley = (np.max(position.y) - np.min(position.y)) / 2\n",
    "\n",
    "data = np.array([np.ones(position.n_samples) * middlex,\n",
    "                 np.ones(position.n_samples) * middley])\n",
    "\n",
    "middle_position = nept.Position(data, position.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:49:30.001233Z",
     "start_time": "2018-08-03T16:49:29.439827Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(position.x, position.y, \".\")\n",
    "plt.plot(middlex, middley, \"r.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:49:31.277380Z",
     "start_time": "2018-08-03T16:49:31.264388Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(position.distance(middle_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:51:59.698916Z",
     "start_time": "2018-08-03T16:51:51.191279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Individual errors\n",
    "for info in infos:\n",
    "    for binsize in binsizes:\n",
    "        decoded_filename = info.session_id + '_decoded_binsize' + str(binsize) + 'cm.pkl'\n",
    "        pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "        with open(pickled_decoded, 'rb') as fileobj:\n",
    "            decoded = pickle.load(fileobj)\n",
    "\n",
    "        shuffled_filename = info.session_id + '_decoded-shuffled_binsize' + str(binsize) + 'cm.pkl'\n",
    "        shuffled_decoded = os.path.join(pickle_filepath, shuffled_filename)\n",
    "        with open(shuffled_decoded, 'rb') as fileobj:\n",
    "            shuffled = pickle.load(fileobj)\n",
    "\n",
    "        filename = info.session_id+\"-errors_binsize\"+str(binsize)+\"cm.png\"\n",
    "        filepath = os.path.join(output_filepath, \"errors\")\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "        plot_errors([decoded[\"errors\"]], [shuffled[\"errors\"]], n_sessions=1, \n",
    "                    filename=os.path.join(filepath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:50:23.981137Z",
     "start_time": "2018-08-03T16:50:23.816084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combined errors\n",
    "for binsize in binsizes:\n",
    "    n_sessions = 0\n",
    "    combined_decoded_errors = []\n",
    "    combined_shuffled_errors = []\n",
    "    for info in infos:\n",
    "        n_sessions += 1\n",
    "        decoded_filename = info.session_id + '_decoded_binsize' + str(binsize) + 'cm.pkl'\n",
    "        pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "        with open(pickled_decoded, 'rb') as fileobj:\n",
    "            decoded = pickle.load(fileobj)\n",
    "        combined_decoded_errors.append(decoded[\"errors\"])\n",
    "\n",
    "        shuffled_filename = info.session_id + '_decoded-shuffled_binsize' + str(binsize) + 'cm.pkl'\n",
    "        shuffled_decoded = os.path.join(pickle_filepath, shuffled_filename)\n",
    "        with open(shuffled_decoded, 'rb') as fileobj:\n",
    "            shuffled = pickle.load(fileobj)\n",
    "        combined_shuffled_errors.append(shuffled[\"errors\"])\n",
    "        \n",
    "    filename = \"combined-errors_binsize\"+str(binsize)+\"cm.png\"\n",
    "    filepath = os.path.join(output_filepath, \"errors\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "        \n",
    "    plot_errors(combined_decoded_errors, combined_shuffled_errors, n_sessions=n_sessions, \n",
    "                filename=os.path.join(filepath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T16:52:39.006386Z",
     "start_time": "2018-08-03T16:52:30.640255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Individual mean/median errors\n",
    "for info in infos:\n",
    "    mean_errors = []\n",
    "    median_errors = []\n",
    "    \n",
    "    mean_errors_shuffled = []\n",
    "    median_errors_shuffled = []\n",
    "    \n",
    "    for binsize in binsizes:\n",
    "        combine_errors = []\n",
    "        combine_errors_shuffled = []\n",
    "        \n",
    "        decoded_filename = info.session_id + '_decoded_binsize' + str(binsize) + 'cm.pkl'\n",
    "        pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "        with open(pickled_decoded, 'rb') as fileobj:\n",
    "            decoded = pickle.load(fileobj)\n",
    "\n",
    "        shuffled_filename = info.session_id + '_decoded-shuffled_binsize' + str(binsize) + 'cm.pkl'\n",
    "        shuffled_decoded = os.path.join(pickle_filepath, shuffled_filename)\n",
    "        with open(shuffled_decoded, 'rb') as fileobj:\n",
    "            shuffled = pickle.load(fileobj)\n",
    "            \n",
    "        for error in decoded[\"errors\"]:\n",
    "            combine_errors.extend(error)\n",
    "        mean_errors.append(np.mean(combine_errors))\n",
    "        median_errors.append(np.median(combine_errors))\n",
    "        \n",
    "        for error in shuffled[\"errors\"]:\n",
    "            combine_errors_shuffled.extend(error)\n",
    "        mean_errors_shuffled.append(np.mean(combine_errors_shuffled))\n",
    "        median_errors_shuffled.append(np.median(combine_errors_shuffled))\n",
    "        \n",
    "    filename = info.session_id+\"-mean-errors.png\"\n",
    "    filepath = os.path.join(output_filepath, \"errors\", \"average\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "        \n",
    "    plt.plot(binsizes, mean_errors)\n",
    "    plt.xlabel(\"Binsize (cm)\")\n",
    "    plt.ylabel(\"Mean error\")\n",
    "    plt.title(info.session_id+\" mean decode error (cm)\")\n",
    "    plt.savefig(os.path.join(filepath, filename))\n",
    "    plt.close()\n",
    "    \n",
    "    filename = info.session_id+\"-median-errors.png\"\n",
    "    filepath = os.path.join(output_filepath, \"errors\", \"average\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    \n",
    "    plt.plot(binsizes, median_errors)\n",
    "    plt.xlabel(\"Binsize (cm)\")\n",
    "    plt.ylabel(\"Median error\")\n",
    "    plt.title(info.session_id+\" median decode error (cm)\")\n",
    "    plt.savefig(os.path.join(filepath, filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T13:58:18.698757Z",
     "start_time": "2018-07-31T13:12:43.390Z"
    }
   },
   "outputs": [],
   "source": [
    "# All mean/median errors\n",
    "all_mean_errors = []\n",
    "all_median_errors = []\n",
    "all_mean_errors_shuffled = []\n",
    "all_median_errors_shuffled = []\n",
    "\n",
    "for binsize in binsizes:\n",
    "    mean_errors = []\n",
    "    median_errors = []\n",
    "\n",
    "    mean_errors_shuffled = []\n",
    "    median_errors_shuffled = []\n",
    "    for info in infos:\n",
    "        combine_errors = []\n",
    "        combine_errors_shuffled = []\n",
    "        \n",
    "        decoded_filename = info.session_id + '_decoded_binsize' + str(binsize) + 'cm.pkl'\n",
    "        pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "        with open(pickled_decoded, 'rb') as fileobj:\n",
    "            decoded = pickle.load(fileobj)\n",
    "\n",
    "        shuffled_filename = info.session_id + '_decoded-shuffled_binsize' + str(binsize) + 'cm.pkl'\n",
    "        shuffled_decoded = os.path.join(pickle_filepath, shuffled_filename)\n",
    "        with open(shuffled_decoded, 'rb') as fileobj:\n",
    "            shuffled = pickle.load(fileobj)\n",
    "            \n",
    "        for error in decoded[\"errors\"]:\n",
    "            combine_errors.extend(error)\n",
    "        mean_errors.append(np.mean(combine_errors))\n",
    "        median_errors.append(np.median(combine_errors))\n",
    "        \n",
    "        for error in shuffled[\"errors\"]:\n",
    "            combine_errors_shuffled.extend(error)\n",
    "        mean_errors_shuffled.append(np.mean(combine_errors_shuffled))\n",
    "        median_errors_shuffled.append(np.median(combine_errors_shuffled))\n",
    "        \n",
    "    all_mean_errors.append(np.mean(mean_errors))\n",
    "    all_median_errors.append(np.mean(median_errors))\n",
    "    all_mean_errors_shuffled.append(np.mean(mean_errors_shuffled))\n",
    "    all_median_errors_shuffled.append(np.mean(median_errors_shuffled))\n",
    "    \n",
    "    \n",
    "filename = \"all-mean-errors.png\"\n",
    "filepath = os.path.join(output_filepath, \"errors\", \"average\")\n",
    "if not os.path.exists(filepath):\n",
    "    os.makedirs(filepath)\n",
    "\n",
    "plt.plot(binsizes, all_mean_errors)\n",
    "plt.xlabel(\"Binsize (cm)\")\n",
    "plt.ylabel(\"Mean error\")\n",
    "plt.title(\"All mean decode error (cm)\")\n",
    "plt.savefig(os.path.join(filepath, filename))\n",
    "plt.close()\n",
    "\n",
    "filename = \"all-median-errors.png\"\n",
    "filepath = os.path.join(output_filepath, \"errors\")\n",
    "if not os.path.exists(filepath):\n",
    "    os.makedirs(filepath)\n",
    "\n",
    "plt.plot(binsizes, all_median_errors)\n",
    "plt.xlabel(\"Binsize (cm)\")\n",
    "plt.ylabel(\"Median error\")\n",
    "plt.title(\"All median decode error (cm)\")\n",
    "plt.savefig(os.path.join(filepath, filename))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T13:58:18.697714Z",
     "start_time": "2018-07-31T13:44:05.005720Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# proportion decoded\n",
    "for binsize in binsizes:\n",
    "    proportion_decoded = []\n",
    "    session_ids = []\n",
    "    for info in infos:\n",
    "        session_ids.append(info.session_id)\n",
    "        decoded_filename = info.session_id + '_decoded_binsize' + str(binsize) + 'cm.pkl'\n",
    "        pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "        with open(pickled_decoded, 'rb') as fileobj:\n",
    "            decoded = pickle.load(fileobj)\n",
    "\n",
    "        n_decoded = 0\n",
    "        for trial in decoded[\"decoded\"]:\n",
    "            n_decoded += trial.n_samples\n",
    "        proportion_decoded.append(n_decoded/decoded[\"session_n_running\"])\n",
    "\n",
    "    filename = \"proportion-decoded_binsize\"+str(binsize)+\"cm.png\"\n",
    "    filepath = os.path.join(output_filepath, \"proportion\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    y_pos = np.arange(n_sessions)\n",
    "    plt.bar(y_pos, proportion_decoded, align='center', alpha=0.7)\n",
    "    plt.xticks(y_pos, session_ids, rotation=90, fontsize=10)\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.title(\"Samples decoded with %d cm bins\" % binsize)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(filepath, filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T13:44:05.003738Z",
     "start_time": "2018-07-31T13:12:21.544225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Individual likelihoods/errors over space\n",
    "for info in infos:\n",
    "    events, position, spikes, _, _ = get_data(info)\n",
    "    \n",
    "    for binsize in binsizes:\n",
    "        xedge, yedge = nept.get_xyedges(position, binsize=binsize)\n",
    "        xx, yy = np.meshgrid(xedge, yedge)\n",
    "        \n",
    "        decoded_filename = info.session_id + '_decoded_binsize' + str(binsize) + 'cm.pkl'\n",
    "        pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "        with open(pickled_decoded, 'rb') as fileobj:\n",
    "            decoded = pickle.load(fileobj)\n",
    "            \n",
    "        filename = info.session_id+\"-likelihoods_byactual-\"+str(binsize)+\"cm.png\"\n",
    "        filepath = os.path.join(output_filepath, \"likelihoods\")\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "        likelihood_byactual = plot_over_space(decoded[\"likelihoods\"], decoded[\"actual\"], xedge, yedge)\n",
    "        pp = plt.pcolormesh(xx, yy, likelihood_byactual, vmin=0., cmap='bone_r')\n",
    "        plt.colorbar(pp)\n",
    "        title = info.session_id+\" posterior\"\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(filepath, filename))\n",
    "        plt.close()\n",
    "#         plt.show()\n",
    "        \n",
    "        filename = info.session_id+\"-errors_byactual-\"+str(binsize)+\"cm.png\"\n",
    "        filepath = os.path.join(output_filepath, \"errors\")\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "        errors_byactual = plot_over_space(decoded[\"errors\"], decoded[\"actual\"], xedge, yedge)\n",
    "        pp = plt.pcolormesh(xx, yy, errors_byactual, vmin=0., cmap='bone_r')\n",
    "        plt.colorbar(pp)\n",
    "        title = info.session_id+\" decoding error (cm)\"\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(filepath, filename))\n",
    "        plt.close()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T13:12:19.042179Z",
     "start_time": "2018-07-31T13:11:16.418209Z"
    }
   },
   "outputs": [],
   "source": [
    "# animations\n",
    "binsize = 8\n",
    "for info in infos:\n",
    "    \n",
    "    decoded_filename = info.session_id + '_decoded_binsize' + str(binsize) + 'cm.pkl'\n",
    "    pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "    with open(pickled_decoded, 'rb') as fileobj:\n",
    "        decoded = pickle.load(fileobj)\n",
    "\n",
    "    _, position, _, _, _ = get_data(info)\n",
    "    xedge, yedge = nept.get_xyedges(position, binsize=binsize)\n",
    "\n",
    "    filepath = os.path.join(output_filepath, \"animations\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    for trial_idx in [1]:\n",
    "        make_animation(info.session_id, decoded, trial_idx=trial_idx, xedge=xedge, yedge=yedge, filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_tuning_curves(position, spikes, xedges, yedges, epoch_of_interest):\n",
    "    sliced_position = position.time_slice(epoch_of_interest.start, epoch_of_interest.stop)\n",
    "    sliced_spikes = [spiketrain.time_slice(epoch_of_interest.start, epoch_of_interest.stop) for spiketrain in spikes]\n",
    "\n",
    "    # Limit position and spikes to only running times\n",
    "    run_epoch = nept.run_threshold(sliced_position, thresh=10., t_smooth=0.8)\n",
    "    run_position = sliced_position[run_epoch]\n",
    "    tuning_spikes = [spiketrain.time_slice(run_epoch.starts, run_epoch.stops) for spiketrain in sliced_spikes]\n",
    "\n",
    "    tuning_curves = nept.tuning_curve_2d(run_position, tuning_spikes, xedges, yedges, occupied_thresh=0.5, gaussian_std=0.3)\n",
    "\n",
    "    return tuning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for binsize in binsizes:\n",
    "    events, position, spikes, _, _ = get_data(info)\n",
    "    xedge, yedge = nept.get_xyedges(position, binsize=binsize)\n",
    "    \n",
    "    xx, yy = np.meshgrid(xedge, yedge)\n",
    "    \n",
    "    epoch_of_interest = info.task_times[\"phase3\"]\n",
    "    \n",
    "    tuning_curves = get_only_tuning_curves(position, spikes, xedge, yedge, epoch_of_interest)\n",
    "    \n",
    "    for i, tuning_curve in enumerate(tuning_curves[:3]):    \n",
    "        tuning_curve = np.array(tuning_curve)\n",
    "        tuning_curve[np.isnan(tuning_curve)] = -np.nanmax(tuning_curve) / n_colours\n",
    "\n",
    "        plt.figure()\n",
    "        pp = plt.pcolormesh(xx, yy, tuning_curve, cmap=cmap)\n",
    "\n",
    "        colourbar = plt.colorbar(pp)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_colours = 15\n",
    "colours = [(1., 1., 1.)]\n",
    "colours.extend(matplotlib.cm.copper_r(np.linspace(0, 1, n_colours-1)))\n",
    "cmap = matplotlib.colors.ListedColormap(colours)\n",
    "\n",
    "multiple_tuning_curves = np.zeros((tuning_curves.shape[1], tuning_curves.shape[2]))\n",
    "for tuning_curve in tuning_curves:\n",
    "    multiple_tuning_curves += tuning_curve\n",
    "\n",
    "multiple_tuning_curves = multiple_tuning_curves / np.nansum(multiple_tuning_curves)\n",
    "multiple_tuning_curves = np.array(multiple_tuning_curves)\n",
    "multiple_tuning_curves[np.isnan(multiple_tuning_curves)] = -np.nanmax(multiple_tuning_curves) / n_colours\n",
    "\n",
    "plt.figure()\n",
    "pp = plt.pcolormesh(xx, yy, multiple_tuning_curves, cmap=cmap)\n",
    "plt.colorbar(pp)\n",
    "plt.title(info.session_id + \" tuning curves (normalized)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:52:41.957257Z",
     "start_time": "2018-07-27T15:52:41.942267Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_decoded(info, position, spikes, xedges, yedges, shuffled_id):\n",
    "    \n",
    "    phase = info.task_times[\"phase3\"]\n",
    "    trials = get_trials(events, phase)\n",
    "    \n",
    "    error_byactual_position = np.zeros((len(yedges), len(xedges)))\n",
    "    n_byactual_position = np.ones((len(yedges), len(xedges)))\n",
    "    \n",
    "    session_n_active = []\n",
    "    session_likelihoods = []\n",
    "    session_decoded = []\n",
    "    session_actual = []\n",
    "    session_errors = []\n",
    "    session_n_running = 0\n",
    "    \n",
    "    for trial in trials:\n",
    "        epoch_of_interest = phase.excludes(trial)\n",
    "\n",
    "        tuning_curves = get_only_tuning_curves(position, \n",
    "                                               spikes, \n",
    "                                               xedges, \n",
    "                                               yedges, \n",
    "                                               epoch_of_interest)\n",
    "\n",
    "        if shuffled_id:\n",
    "            tuning_curves = np.random.permutation(tuning_curves)\n",
    "\n",
    "        sliced_position = position.time_slice(trial.start, trial.stop)\n",
    "        \n",
    "        sliced_spikes = [spiketrain.time_slice(trial.start, \n",
    "                                               trial.stop) for spiketrain in spikes]\n",
    "\n",
    "        # limit position to only times when the subject is moving faster than a certain threshold\n",
    "        run_epoch = nept.run_threshold(sliced_position, thresh=8., t_smooth=0.8)\n",
    "        sliced_position = sliced_position[run_epoch]\n",
    "        \n",
    "        session_n_running += sliced_position.n_samples\n",
    "        \n",
    "        sliced_spikes = [spiketrain.time_slice(run_epoch.start, \n",
    "                                               run_epoch.stop) for spiketrain in sliced_spikes]\n",
    "\n",
    "        epochs_interest = nept.Epoch(np.array([sliced_position.time[0], sliced_position.time[-1]]))\n",
    "\n",
    "        counts = nept.bin_spikes(sliced_spikes, sliced_position.time, dt=0.025, window=0.025,\n",
    "                                 gaussian_std=0.0075, normalized=False)\n",
    "        \n",
    "        min_neurons = 2\n",
    "        min_spikes = 2\n",
    "        \n",
    "        tc_shape = tuning_curves.shape\n",
    "        decoding_tc = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "        likelihood = nept.bayesian_prob(counts, decoding_tc, binsize=0.025, min_neurons=2, \n",
    "                                        min_spikes=min_spikes)\n",
    "\n",
    "        # Find decoded location based on max likelihood for each valid timestep\n",
    "        xcenters = (xedges[1:] + xedges[:-1]) / 2.\n",
    "        ycenters = (yedges[1:] + yedges[:-1]) / 2.\n",
    "        xy_centers = nept.cartesian(xcenters, ycenters)\n",
    "        decoded = nept.decode_location(likelihood, xy_centers, counts.time)\n",
    "\n",
    "        session_decoded.append(decoded)\n",
    "        \n",
    "        # Remove nans from likelihood and reshape for plotting\n",
    "        keep_idx = np.sum(np.isnan(likelihood), axis=1) < likelihood.shape[1]\n",
    "        likelihood = likelihood[keep_idx]\n",
    "        likelihood = likelihood.reshape(np.shape(likelihood)[0], tc_shape[1], tc_shape[2])\n",
    "\n",
    "        session_likelihoods.append(likelihood)\n",
    "        \n",
    "        n_active_neurons = np.asarray([n_active if n_active >= min_neurons else 0 \n",
    "                                       for n_active in np.sum(counts.data >= 1, axis=1)])\n",
    "        n_active_neurons = n_active_neurons[keep_idx]\n",
    "        session_n_active.append(n_active_neurons)\n",
    "\n",
    "        f_xy = scipy.interpolate.interp1d(sliced_position.time, sliced_position.data.T, kind=\"nearest\")\n",
    "        counts_xy = f_xy(decoded.time)\n",
    "        true_position = nept.Position(np.hstack((counts_xy[0][..., np.newaxis],\n",
    "                                                 counts_xy[1][..., np.newaxis])),\n",
    "                                      decoded.time)\n",
    "\n",
    "        session_actual.append(true_position)\n",
    "\n",
    "        trial_errors = true_position.distance(decoded)\n",
    "\n",
    "        for error, x, y in zip(trial_errors, true_position.x, true_position.y):\n",
    "            x_idx = nept.find_nearest_idx(xcenters, x)\n",
    "            y_idx = nept.find_nearest_idx(ycenters, y)\n",
    "            error_byactual_position[y_idx][x_idx] += error\n",
    "            n_byactual_position[y_idx][x_idx] += 1\n",
    "\n",
    "        session_errors.append(trial_errors)\n",
    "            \n",
    "#     error_byactual = error_byactual_position / n_byactual_position\n",
    "\n",
    "    return session_decoded, session_actual, session_likelihoods, session_errors, session_n_active, session_n_running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:10:12.700011Z",
     "start_time": "2018-07-27T15:10:12.693995Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_over_space(values, positions, xedges, yedges):\n",
    "    xcenters = xedges[:-1] + (xedges[1:] - xedges[:-1]) / 2\n",
    "    ycenters = yedges[:-1] + (yedges[1:] - yedges[:-1]) / 2\n",
    "\n",
    "    count_position = np.zeros((len(yedges), len(xedges)))\n",
    "    n_position = np.ones((len(yedges), len(xedges)))\n",
    "\n",
    "    for trial_values, trial_positions in zip(values, positions):\n",
    "        for these_values, x, y in zip(trial_values, trial_positions.x, trial_positions.y):\n",
    "            x_idx = nept.find_nearest_idx(xcenters, x)\n",
    "            y_idx = nept.find_nearest_idx(ycenters, y)\n",
    "            if np.isscalar(these_values):\n",
    "                count_position[y_idx][x_idx] += these_values\n",
    "            else:\n",
    "                count_position[y_idx][x_idx] += these_values[y_idx][x_idx]\n",
    "            n_position[y_idx][x_idx] += 1\n",
    "\n",
    "    return count_position / n_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T16:28:46.616572Z",
     "start_time": "2018-07-27T16:25:33.239028Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "binsize = 8\n",
    "n_sessions = 0\n",
    "session_ids = []\n",
    "xedges = []\n",
    "yedges = []\n",
    "\n",
    "all_decoded = []\n",
    "all_actual = []\n",
    "all_likelihoods = []\n",
    "all_n_active = []\n",
    "\n",
    "all_errors = []\n",
    "all_errors_id_shuffled = []\n",
    "all_errors_random_shuffled = []\n",
    "\n",
    "for info in infos:\n",
    "    print(info.session_id)\n",
    "    session_ids.append(info.session_id)\n",
    "    n_sessions += 1\n",
    "    events, position, spikes, _, _ = get_data(info)\n",
    "\n",
    "    xedge, yedge = nept.get_xyedges(position, binsize=binsize)\n",
    "    xedges.append(xedge)\n",
    "    yedges.append(yedge)\n",
    "\n",
    "    xx, yy = np.meshgrid(xedge, yedge)\n",
    "\n",
    "\n",
    "    decoded, actual, likelihoods, errors, n_active, _= get_decoded(info, \n",
    "                                             position, \n",
    "                                             spikes, \n",
    "                                             xedge, \n",
    "                                             yedge, \n",
    "                                             shuffled_id=False)\n",
    "\n",
    "    _, _, _, errors_id_shuffled, _, _ = get_decoded(info, \n",
    "                                             position, \n",
    "                                             spikes, \n",
    "                                             xedge, \n",
    "                                             yedge, \n",
    "                                             shuffled_id=True)\n",
    "\n",
    "    all_decoded.append(decoded)\n",
    "    all_actual.append(actual)\n",
    "    all_likelihoods.append(likelihoods)\n",
    "    all_n_active.append(n_active)\n",
    "\n",
    "    all_errors.append(errors)\n",
    "    all_errors_id_shuffled.append(errors_id_shuffled)\n",
    "    \n",
    "    filename = os.path.join(output_filepath, info.session_id+\"_errors-binsize\"+str(binsize)+\".png\")\n",
    "    plot_errors(errors, errors_id_shuffled, n_sessions=1, filename=filename)\n",
    "\n",
    "combined_errors = np.concatenate([np.concatenate(errors, axis=0) for errors in all_errors], axis=0)\n",
    "\n",
    "filename = os.path.join(output_filepath, \"combined_errors-binsize\"+str(binsize)+\".png\")\n",
    "plot_errors(all_errors, all_errors_id_shuffled, n_sessions, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:55:12.185499Z",
     "start_time": "2018-07-27T15:55:11.290Z"
    }
   },
   "outputs": [],
   "source": [
    "# proportion decoded & likelihood/errors over space\n",
    "binsize = 8\n",
    "    \n",
    "n_sessions = 0\n",
    "session_ids = []\n",
    "xedges = []\n",
    "yedges = []\n",
    "\n",
    "all_decoded = []\n",
    "all_actual = []\n",
    "all_likelihoods = []\n",
    "all_n_active = []\n",
    "\n",
    "all_errors = []\n",
    "all_errors_id_shuffled = []\n",
    "all_errors_random_shuffled = []\n",
    "\n",
    "proportion_decoded = []\n",
    "\n",
    "for info in infos:\n",
    "    print(info.session_id)\n",
    "    session_ids.append(info.session_id)\n",
    "    n_sessions += 1\n",
    "    events, position, spikes, _, _ = get_data(info)\n",
    "\n",
    "    xedge, yedge = nept.get_xyedges(position, binsize=binsize)\n",
    "    xedges.append(xedge)\n",
    "    yedges.append(yedge)\n",
    "\n",
    "    xx, yy = np.meshgrid(xedge, yedge)\n",
    "\n",
    "\n",
    "    decoded, actual, likelihoods, errors, n_active, session_n_running = get_decoded(info, \n",
    "                                             position, \n",
    "                                             spikes, \n",
    "                                             xedge, \n",
    "                                             yedge, \n",
    "                                             shuffled_id=False)\n",
    "    \n",
    "#     likelihood_byactual = plot_over_space(likelihoods, actual, xedge, yedge)\n",
    "#     pp = plt.pcolormesh(xx, yy, likelihood_byactual, vmin=0., cmap='bone_r')\n",
    "#     plt.colorbar(pp)\n",
    "#     title = info.session_id+\" posterior\"\n",
    "#     plt.title(title)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(os.path.join(output_filepath, info.session_id+\"_posterior-byactual.png\"))\n",
    "#     plt.close()\n",
    "\n",
    "#     errors_byactual = plot_over_space(errors, actual, xedge, yedge)\n",
    "#     pp = plt.pcolormesh(xx, yy, errors_byactual, vmin=0., cmap='bone_r')\n",
    "#     plt.colorbar(pp)\n",
    "#     title = info.session_id+\" decoding error (cm)\"\n",
    "#     plt.title(title)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(os.path.join(output_filepath, info.session_id+\"_errors-byactual.png\"))\n",
    "#     plt.close()\n",
    "\n",
    "    n_decoded = 0\n",
    "    for trial in decoded:\n",
    "        n_decoded += trial.n_samples\n",
    "    proportion_decoded.append(n_decoded/session_n_running)\n",
    "\n",
    "print(proportion_decoded)\n",
    "y_pos = np.arange(n_sessions)\n",
    "plt.bar(y_pos, proportion_decoded, align='center', alpha=0.7)\n",
    "plt.xticks(y_pos, session_ids, rotation=90, fontsize=10)\n",
    "plt.ylabel('Proportion')\n",
    "plt.title(\"Samples decoded with %d cm bins\" % binsize)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(os.path.join(output_filepath, \"proportion-decoded_\"+str(binsize)+\"cm.png\"))\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:10:13.469524Z",
     "start_time": "2018-07-27T17:07:53.404816Z"
    }
   },
   "outputs": [],
   "source": [
    "# individual session errors by binsize\n",
    "\n",
    "binsizes = [2, 5, 8, 10, 12, 15, 20, 50, 100]\n",
    "\n",
    "for info in infos:\n",
    "    \n",
    "    print(info.session_id)\n",
    "    combined_errors = []\n",
    "    mean_errors = []\n",
    "    median_errors = []\n",
    "    \n",
    "    for binsize in binsizes:\n",
    "        events, position, spikes, _, _ = get_data(info)\n",
    "\n",
    "        xedge, yedge = nept.get_xyedges(position, binsize=binsize)\n",
    "\n",
    "        decoded, actual, likelihoods, errors, n_active, session_n_running = get_decoded(info, \n",
    "                                                 position, \n",
    "                                                 spikes, \n",
    "                                                 xedge, \n",
    "                                                 yedge, \n",
    "                                                 shuffled_id=False)\n",
    "        for error in errors:\n",
    "            combined_errors.extend(error)\n",
    "        mean_errors.append(np.mean(combined_errors))\n",
    "        median_errors.append(np.median(combined_errors))\n",
    "        \n",
    "    plt.plot(binsizes, mean_errors)\n",
    "    plt.xlabel(\"Binsize (cm)\")\n",
    "    plt.ylabel(\"Mean error\")\n",
    "    plt.title(info.session_id+\" mean decode error (cm)\")\n",
    "    plt.savefig(os.path.join(output_filepath, info.session_id+\"_mean-error.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.plot(binsizes, median_errors)\n",
    "    plt.xlabel(\"Binsize (cm)\")\n",
    "    plt.ylabel(\"Median error\")\n",
    "    plt.title(info.session_id+\" median decode error (cm)\")\n",
    "    plt.savefig(os.path.join(output_filepath, info.session_id+\"_median-error.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T16:50:23.771259Z",
     "start_time": "2018-07-27T16:50:23.765280Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_errors # 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T16:51:21.748032Z",
     "start_time": "2018-07-27T16:51:21.742035Z"
    }
   },
   "outputs": [],
   "source": [
    "np.amin(mean_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T16:22:01.361947Z",
     "start_time": "2018-07-27T16:22:01.006736Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(position.x, position.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T16:04:47.747342Z",
     "start_time": "2018-07-27T16:04:46.648746Z"
    }
   },
   "outputs": [],
   "source": [
    "for trial_idx in range(2):\n",
    "    for session_idx in range(n_sessions):\n",
    "        decoded = all_decoded[session_idx][trial_idx]\n",
    "        true_position = all_actual[session_idx][trial_idx]\n",
    "        likelihoods = np.array(all_likelihoods[session_idx][trial_idx])\n",
    "        n_active = all_n_active[session_idx][trial_idx]\n",
    "        errors = all_errors[session_idx][trial_idx]\n",
    "        xedge = xedges[session_idx]\n",
    "        yedge = yedges[session_idx]\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 10))\n",
    "        gs = gridspec.GridSpec(5, 4)\n",
    "\n",
    "        xx, yy = np.meshgrid(xedge, yedge)\n",
    "\n",
    "        ax1 = plt.subplot2grid((5, 4), (0, 0), colspan=3, rowspan=3)\n",
    "\n",
    "        pad_amount = binsize*2\n",
    "        ax1.set_xlim((np.floor(np.min(true_position.x))-pad_amount, np.ceil(np.max(true_position.x))+pad_amount))\n",
    "        ax1.set_ylim((np.floor(np.min(true_position.y))-pad_amount, np.ceil(np.max(true_position.y))+pad_amount))\n",
    "\n",
    "        n_timebins = decoded.n_samples\n",
    "    #     n_timebins = 10\n",
    "\n",
    "        n_colours = 20.\n",
    "        colours = [(1., 1., 1.)]\n",
    "        colours.extend(matplotlib.cm.copper_r(np.linspace(0, 1, n_colours-1)))\n",
    "        cmap = matplotlib.colors.ListedColormap(colours)\n",
    "\n",
    "        likelihoods_withnan = np.array(likelihoods)\n",
    "        likelihoods[np.isnan(likelihoods)] = -0.01\n",
    "\n",
    "        xcenters = xedge[:-1] + (xedge[1:] - xedge[:-1]) / 2\n",
    "        ycenters = yedge[:-1] + (yedge[1:] - yedge[:-1]) / 2\n",
    "\n",
    "        x_idx = [nept.find_nearest_idx(xcenters, true_position.x[timestep]) for timestep in range(true_position.n_samples)]\n",
    "        y_idx = [nept.find_nearest_idx(ycenters, true_position.y[timestep]) for timestep in range(true_position.n_samples)]\n",
    "\n",
    "        x_dec_idx = [nept.find_nearest_idx(xcenters, decoded.x[timestep]) for timestep in range(decoded.n_samples)]\n",
    "        y_dec_idx = [nept.find_nearest_idx(ycenters, decoded.y[timestep]) for timestep in range(decoded.n_samples)]\n",
    "\n",
    "    #     cmap = plt.cm.get_cmap('bone_r')\n",
    "        posterior_position = ax1.pcolormesh(xx[:-1], yy[:-1], likelihoods[0], vmax=0.2, cmap=cmap)\n",
    "        colorbar = fig.colorbar(posterior_position, ax=ax1)\n",
    "\n",
    "        estimated_position, = ax1.plot([], [], \"o\", color=\"c\")\n",
    "        rat_position, = ax1.plot([], [], \"<\", color=\"b\")\n",
    "\n",
    "        ax2 = plt.subplot2grid((5, 4), (3, 0), colspan=3)\n",
    "\n",
    "        binwidth = 5.\n",
    "        error_bins = np.arange(-binwidth, np.max(errors)+binwidth, binwidth)\n",
    "\n",
    "        _, _, errors_bin = ax2.hist([np.clip(errors, error_bins[0], error_bins[-1])], bins=error_bins, rwidth=0.9, color=\"k\")\n",
    "        errors_idx = np.digitize(errors, error_bins)\n",
    "\n",
    "        fontsize = 14\n",
    "        likelihood_at_actual = ax2.text(0.6, 1, [],\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='top',\n",
    "                 transform = ax2.transAxes,\n",
    "                 fontsize=fontsize)\n",
    "\n",
    "        ax2.set_xlabel(\"Error (cm)\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(\"# bins\", fontsize=fontsize)\n",
    "        ax2.spines['right'].set_visible(False)\n",
    "        ax2.spines['top'].set_visible(False)\n",
    "        ax2.yaxis.set_ticks_position('left')\n",
    "        ax2.xaxis.set_ticks_position('bottom')\n",
    "        # xticks = binwidth * np.arange(0, len(xlabels)-1, 2) - binwidth\n",
    "        # xticks[0] = -binwidth/2.\n",
    "        xticks = binwidth * np.arange(0, len(error_bins), 4)\n",
    "        plt.xticks(xticks, fontsize=fontsize)\n",
    "        plt.yticks(fontsize=fontsize)\n",
    "\n",
    "        ax3 = plt.subplot2grid((5, 4), (4, 0), colspan=3)\n",
    "\n",
    "        n_active_bins = np.arange(-0.5, np.max(n_active)+1)\n",
    "\n",
    "        _, _, n_neurons_bin = ax3.hist(n_active, bins=n_active_bins, rwidth=0.9, color=\"k\", align=\"mid\")\n",
    "\n",
    "        ax3.set_xlabel(\"Number of active neurons\", fontsize=fontsize)\n",
    "        ax3.set_ylabel(\"# bins\", fontsize=fontsize)\n",
    "        ax3.spines['right'].set_visible(False)\n",
    "        ax3.spines['top'].set_visible(False)\n",
    "        ax3.yaxis.set_ticks_position('left')\n",
    "        ax3.xaxis.set_ticks_position('bottom')\n",
    "    #     xticks = binwidth * np.arange(0, len(n_active_bins)+1)\n",
    "    #     plt.xticks(xticks+binwidth/2)\n",
    "    #     ax3.set_xticklabels(xticks, fontsize=fontsize)\n",
    "        plt.yticks(fontsize=fontsize)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "\n",
    "        def init():\n",
    "            posterior_position.set_array([])\n",
    "            estimated_position.set_data([], [])\n",
    "            rat_position.set_data([], [])\n",
    "            likelihood_at_actual.set_text([])\n",
    "            return (posterior_position, estimated_position, rat_position, likelihood_at_actual)\n",
    "\n",
    "\n",
    "        def animate(i):\n",
    "            posterior_position.set_array(likelihoods[i].ravel())\n",
    "            estimated_position.set_data(decoded.x[i], decoded.y[i])\n",
    "            rat_position.set_data(true_position.x[i], true_position.y[i])\n",
    "\n",
    "            for patch in errors_bin:\n",
    "                patch.set_fc('k')\n",
    "            errors_bin[errors_idx[i]-1].set_fc('r')\n",
    "\n",
    "            for patch in n_neurons_bin:\n",
    "                patch.set_fc('k')\n",
    "            n_neurons_bin[n_active[i]].set_fc('b')\n",
    "\n",
    "            likelihood_at_actual.set_text(\"posterior at true position: %.3f \\nposterior at decoded position: %.3f \" % \n",
    "                                          (likelihoods_withnan[i][y_idx[i]][x_idx[i]], \n",
    "                                           likelihoods_withnan[i][y_dec_idx[i]][x_dec_idx[i]]))\n",
    "\n",
    "            return (posterior_position, estimated_position, rat_position, likelihood_at_actual)\n",
    "\n",
    "        anim = animation.FuncAnimation(fig, animate, frames=n_timebins, interval=200, \n",
    "                                       blit=False, repeat=False)\n",
    "\n",
    "\n",
    "#         writer = animation.writers['ffmpeg'](fps=10)\n",
    "#         dpi = 600\n",
    "#         filename = session_ids[session_idx]+'_decoded_trial'+str(trial_idx)+'.mp4'\n",
    "#         anim.save(os.path.join(output_filepath, filename), writer=writer, dpi=dpi)\n",
    "\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T16:05:24.529241Z",
     "start_time": "2018-07-27T16:04:48.967329Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Blue triangle is the true position; Cyan circle is the estimated location.\")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T16:52:21.414318Z",
     "start_time": "2018-07-23T16:52:20.739506Z"
    }
   },
   "outputs": [],
   "source": [
    "events, position, spikes, _, _ = get_data(info)\n",
    "xedge, yedge = nept.get_xyedges(position, binsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T16:52:33.572725Z",
     "start_time": "2018-07-23T16:52:33.563749Z"
    }
   },
   "outputs": [],
   "source": [
    "phase1_position = position.time_slice(info.task_times[\"phase1\"].start, info.task_times[\"phase1\"].stop)\n",
    "phase1_occupancy = nept.get_occupancy(phase1_position, yedge, xedge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3_position = position.time_slice(info.task_times[\"phase3\"].start, info.task_times[\"phase3\"].stop)\n",
    "phase3_occupancy = nept.get_occupancy(phase3_position, yedge, xedge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3_occupancy.shape, phase1_occupancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = phase3_occupancy[phase3_occupancy>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T16:48:01.666306Z",
     "start_time": "2018-07-23T16:48:01.523500Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(phase1_position.x, phase1_position.y, \".\")\n",
    "plt.plot(zones[\"u\"].exterior.xy[0], zones[\"u\"].exterior.xy[1], 'b', lw=1)\n",
    "# for intersect in zones[\"shortcut\"]:\n",
    "plt.plot(zones[\"shortcut\"].exterior.xy[0], zones[\"shortcut\"].exterior.xy[1], 'g', lw=1)\n",
    "plt.plot(zones[\"novel\"].exterior.xy[0], zones[\"novel\"].exterior.xy[1], 'r', lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T16:53:26.838165Z",
     "start_time": "2018-07-23T16:53:26.834167Z"
    }
   },
   "outputs": [],
   "source": [
    "binned_maze_shape = phase1_occupancy.shape\n",
    "u_pos = np.zeros(binned_maze_shape).astype(bool)\n",
    "novel_pos = np.zeros(binned_maze_shape).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T16:54:11.503650Z",
     "start_time": "2018-07-23T16:54:11.499653Z"
    }
   },
   "outputs": [],
   "source": [
    "u_pos[phase1_occupancy > 0] = True\n",
    "novel_pos[(phase3_occupancy > 4.) & (~u_pos)] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T16:55:13.831497Z",
     "start_time": "2018-07-23T16:55:13.659596Z"
    }
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(xedge, yedge)\n",
    "\n",
    "plt.figure()\n",
    "pp = plt.pcolormesh(xx, yy, phase1_occupancy, vmax=10., cmap=\"Greys\")\n",
    "colourbar = plt.colorbar(pp)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(xedge, yedge)\n",
    "\n",
    "plt.figure()\n",
    "pp = plt.pcolormesh(xx, yy, u_pos, cmap=\"Greys\")\n",
    "colourbar = plt.colorbar(pp)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(xedge, yedge)\n",
    "\n",
    "plt.figure()\n",
    "pp = plt.pcolormesh(xx, yy, phase3_occupancy, vmax=10., cmap=\"Greys\")\n",
    "colourbar = plt.colorbar(pp)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = \"phase3\"\n",
    "t_start = info.task_times[phase].start\n",
    "t_stop = info.task_times[phase].stop\n",
    "\n",
    "sliced_pos = position.time_slice(t_start, t_stop)\n",
    "\n",
    "feeder1_times = []\n",
    "for feeder1 in events['feeder1']:\n",
    "    if t_start < feeder1 < t_stop:\n",
    "        feeder1_times.append(feeder1)\n",
    "\n",
    "feeder2_times = []\n",
    "for feeder2 in events['feeder2']:\n",
    "    if t_start < feeder2 < t_stop:\n",
    "        feeder2_times.append(feeder2)\n",
    "\n",
    "path_pos = get_zones(info, sliced_pos)\n",
    "\n",
    "trials_idx, trial_epochs = get_trial_idx(path_pos['u'].time, \n",
    "                                         path_pos['shortcut'].time, \n",
    "                                         path_pos['novel'].time,\n",
    "                                         feeder1_times, \n",
    "                                         feeder2_times, \n",
    "                                         t_stop)\n",
    "\n",
    "shortcut_epochs = [trial_epochs[idx] for idx in trials_idx[\"shortcut\"]]\n",
    "u_epochs = [trial_epochs[idx] for idx in trials_idx[\"u\"]]\n",
    "novel_epochs = [trial_epochs[idx] for idx in trials_idx[\"novel\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_in_zones(position, zones):\n",
    "    \"\"\"Assigns points to each trajectory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    position : nept.Position\n",
    "    zones : dict\n",
    "        With u, shortcut, novel, pedestal as keys\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sorted_zones : dict\n",
    "        With u, shortcut, novel, other as keys, each a nept.Position object\n",
    "\n",
    "    \"\"\"\n",
    "    u_data = []\n",
    "    u_times = []\n",
    "    shortcut_data = []\n",
    "    shortcut_times = []\n",
    "    novel_data = []\n",
    "    novel_times = []\n",
    "    other_data = []\n",
    "    other_times = []\n",
    "\n",
    "    if not position.isempty:\n",
    "        for x, y, time in zip(position.x, position.y, position.time):\n",
    "            point = Point([x, y])\n",
    "            if zones['u'].contains(point):\n",
    "                u_data.append([x, y])\n",
    "                u_times.append(time)\n",
    "                continue\n",
    "            elif zones['shortcut'].contains(point):\n",
    "                shortcut_data.append([x, y])\n",
    "                shortcut_times.append(time)\n",
    "                continue\n",
    "            elif zones['novel'].contains(point):\n",
    "                novel_data.append([x, y])\n",
    "                novel_times.append(time)\n",
    "                continue\n",
    "            else:\n",
    "                other_data.append([x, y])\n",
    "                other_times.append(time)\n",
    "\n",
    "    sorted_zones = dict()\n",
    "    sorted_zones['u'] = nept.Position(u_data, u_times)\n",
    "    sorted_zones['shortcut'] = nept.Position(shortcut_data, shortcut_times)\n",
    "    sorted_zones['novel'] = nept.Position(novel_data, novel_times)\n",
    "    sorted_zones['other'] = nept.Position(other_data, other_times)\n",
    "\n",
    "    return sorted_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:41:44.317383Z",
     "start_time": "2018-07-23T13:37:55.533Z"
    }
   },
   "outputs": [],
   "source": [
    "trial_idx = 1\n",
    "timestep = 0\n",
    "\n",
    "xcenters = xedges[:-1] + (xedges[1:] - xedges[:-1]) / 2\n",
    "ycenters = yedges[:-1] + (yedges[1:] - yedges[:-1]) / 2\n",
    "\n",
    "x_idx = [nept.find_nearest_idx(xcenters, actual[trial_idx].x[timestep]) for timestep in range(actual[trial_idx].n_samples)]\n",
    "y_idx = [nept.find_nearest_idx(ycenters, actual[trial_idx].y[timestep]) for timestep in range(actual[trial_idx].n_samples)]\n",
    "\n",
    "print(likelihoods[timestep][y_idx[timestep]][x_idx[timestep]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:41:44.318383Z",
     "start_time": "2018-07-23T13:37:55.539Z"
    }
   },
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('bone_r')\n",
    "pp = plt.pcolormesh(xx[:-1], yy[:-1], likelihoods[timestep], cmap=cmap)\n",
    "colorbar = fig.colorbar(pp, ax=ax1)\n",
    "plt.plot(actual[trial_idx].x[timestep], actual[trial_idx].y[timestep], \"r.\", ms=10)\n",
    "colorbar = plt.colorbar(pp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T13:41:44.323380Z",
     "start_time": "2018-07-23T13:37:55.552Z"
    }
   },
   "outputs": [],
   "source": [
    "# combined_errors = []\n",
    "# binsizes = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "# for binsize in binsizes:\n",
    "#     n_sessions = 0\n",
    "#     xxs = []\n",
    "#     yys = []\n",
    "\n",
    "#     all_decoded = []\n",
    "#     all_actual = []\n",
    "#     all_likelihoods = []\n",
    "#     all_n_active = []\n",
    "\n",
    "#     all_errors = []\n",
    "\n",
    "\n",
    "#     for info in infos:\n",
    "#         print(info.session_id)\n",
    "#         n_sessions += 1\n",
    "#         events, position, spikes, _, _ = get_data(info)\n",
    "#         xedges, yedges = nept.get_xyedges(position, binsize=binsize)\n",
    "\n",
    "#         xx, yy = np.meshgrid(xedges, yedges)\n",
    "#         xxs.append(xx)\n",
    "#         yys.append(yy)\n",
    "\n",
    "#         phase = \"phase3\"\n",
    "#         trial_epochs = get_trials(events, info.task_times[phase])\n",
    "\n",
    "#         decoded, actual, likelihoods, errors, n_active = get_decoded(info, \n",
    "#                                                  position, \n",
    "#                                                  spikes, \n",
    "#                                                  xedges, \n",
    "#                                                  yedges, \n",
    "#                                                  shuffled_id=False, \n",
    "#                                                  random_shuffle=False)\n",
    "\n",
    "#         all_decoded.append(decoded)\n",
    "#         all_actual.append(actual)\n",
    "#         all_likelihoods.append(likelihoods)\n",
    "#         all_n_active.append(n_active)\n",
    "\n",
    "#         all_errors.append(errors)\n",
    "\n",
    "#     combined_errors.append(np.concatenate([np.concatenate(errors, axis=0) for errors in all_errors], axis=0))\n",
    "    \n",
    "# plt.plot(binsizes, np.mean(combined_errors, axis=1))\n",
    "# plt.xticks(binsizes)\n",
    "# plt.xlabel(\"Binsize (cm)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
