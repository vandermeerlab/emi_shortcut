{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, LineString\n",
    "import os\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nept\n",
    "\n",
    "from loading_data import get_data\n",
    "from utils_maze import get_xyedges, find_zones, expand_line, speed_threshold\n",
    "from plot_decode import load_decoded, get_zone_proportion, get_proportion_normalized\n",
    "from utils_plotting import plot_decoded_compare, plot_intersects, plot_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import info.r066d3 as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_filepath = 'C:/Users/Emily/code/emi_shortcut/cache/data'\n",
    "# pickle_filepath = 'C:/Users/Emily/code/emi_shortcut/cache/pickled'\n",
    "\n",
    "data_filepath = 'E:/code/emi_shortcut/cache/data'\n",
    "pickle_filepath = 'E:/code/emi_shortcut/cache/pickled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events, position, spikes, lfp, lfp_theta = get_data(info)\n",
    "xedges, yedges = get_xyedges(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neurons_filename = info.session_id + '_neurons.pkl'\n",
    "pickled_neurons = os.path.join(pickle_filepath, neurons_filename)\n",
    "with open(pickled_neurons, 'rb') as fileobj:\n",
    "    neurons = pickle.load(fileobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_time = 'phase3'\n",
    "decoded_filename = info.session_id + '_decode-' + experiment_time + '.pkl'\n",
    "pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "with open(pickled_decoded, 'rb') as fileobj:\n",
    "    decoded = pickle.load(fileobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infos = [info, info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_times = ['phase1', 'phase2', 'phase3']\n",
    "decodes = []\n",
    "for this_info in infos:\n",
    "    decodes.append(load_decoded(this_info, experiment_times, pickle_filepath, get_proportion_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decoded_compare(decodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_lengths(infos, combined_zones, timebin=0.025):\n",
    "    \"\"\"Compare position normalized by time spent in zone.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    infos: list of modules\n",
    "    combined_zones: list of OrderedDict of dict of lists\n",
    "        With experiment_time as keys and inner dict\n",
    "        has u, shortcut, novel as keys.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    by_track_length : list\n",
    "        Of OrderedDict with experiment_times as keys,\n",
    "        each a dict with u, shortcut, novel as keys.\n",
    "\n",
    "    \"\"\"\n",
    "    lengths = dict(u=[], shortcut=[], novel=[])\n",
    "    for info in infos:\n",
    "        lengths['u'].append(LineString(info.u_trajectory).length)\n",
    "        lengths['shortcut'].append(LineString(info.shortcut_trajectory).length)\n",
    "        lengths['novel'].append(LineString(info.novel_trajectory).length)\n",
    "\n",
    "    by_track_length = []\n",
    "\n",
    "    for i, session in enumerate(combined_zones):\n",
    "        normalized = OrderedDict()\n",
    "        for exp in combined_zones[0].keys():\n",
    "            normalized[exp] = dict()\n",
    "\n",
    "        for exp in session.keys():\n",
    "            for trajectory in session[exp].keys():\n",
    "                # normalized[exp][trajectory] = (session[exp][trajectory].n_samples * timebin) / lengths[trajectory][i]\n",
    "                normalized[exp][trajectory] = (session[exp][trajectory].n_samples) / lengths[trajectory][i]\n",
    "\n",
    "        by_track_length.append(normalized)\n",
    "\n",
    "    return by_track_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_proportion_normalized(info, decoded, experiment_time):\n",
    "    \"\"\"Finds decoded proportions normalized by track length\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    info: module\n",
    "    decoded: nept.Position\n",
    "    experiment_time: str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normalized_proportions: dict\n",
    "        With u, shortcut, novel as keys\n",
    "    \n",
    "    \"\"\"\n",
    "    print('Working on', info.session_id)\n",
    "    if experiment_time not in ['prerecord', 'phase1', 'pauseA', 'phase2', 'pauseB', 'phase3', 'postrecord']:\n",
    "        raise ValueError(\"experiment time is not recognized as a shortcut experiment time.\")\n",
    "        \n",
    "    length = dict()\n",
    "    length['u'] = LineString(info.u_trajectory).length\n",
    "    length['shortcut'] = LineString(info.shortcut_trajectory).length\n",
    "    length['novel'] = LineString(info.novel_trajectory).length\n",
    "    \n",
    "    n_total = 0\n",
    "    for zone in ['u', 'shortcut', 'novel']:\n",
    "        n_total += decoded['zones'][zone].n_samples / length[zone]\n",
    "    n_total = np.maximum(n_total, 1.0)\n",
    "\n",
    "    decoded_proportions = dict()\n",
    "    for zone in ['u', 'shortcut', 'novel']:\n",
    "        decoded_proportions[zone] = (decoded['zones'][zone].n_samples / length[zone]) / n_total\n",
    "\n",
    "    return decoded_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_proportion_normalized(info, decoded, 'phase1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "45.5+30+23.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_time = 'phase3'\n",
    "min_swr=3\n",
    "min_sequence=3\n",
    "speed_limit=0.4\n",
    "min_epochs=3\n",
    "shuffle_id=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('decoding:', info.session_id, experiment_time)\n",
    "\n",
    "track_times = ['phase1', 'phase2', 'phase3', 'tracks']\n",
    "pedestal_times = ['pauseA', 'pauseB', 'prerecord', 'postrecord']\n",
    "\n",
    "events, position, all_spikes, lfp, lfp_theta = get_data(info)\n",
    "xedges, yedges = nept.get_xyedges(position)\n",
    "\n",
    "exp_start = info.task_times[experiment_time].start\n",
    "exp_stop = info.task_times[experiment_time].stop\n",
    "\n",
    "sliced_spikes = neurons.time_slice(exp_start, exp_stop)\n",
    "\n",
    "if experiment_time in track_times:\n",
    "    run_position = speed_threshold(position, speed_limit=speed_limit)\n",
    "else:\n",
    "    run_position = position\n",
    "\n",
    "exp_position = run_position.time_slice(exp_start, exp_stop)\n",
    "\n",
    "if shuffle_id:\n",
    "    random.shuffle(neurons.tuning_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if experiment_time in track_times:\n",
    "    epochs_interest = nept.Epoch(np.hstack([exp_start, exp_stop]))\n",
    "elif experiment_time in pedestal_times:\n",
    "    sliced_lfp = lfp.time_slice(exp_start, exp_stop)\n",
    "\n",
    "    z_thresh = 3.0\n",
    "    power_thresh = 5.0\n",
    "    merge_thresh = 0.02\n",
    "    min_length = 0.05\n",
    "    swrs = nept.detect_swr_hilbert(sliced_lfp, fs=info.fs, thresh=(140.0, 250.0), z_thresh=z_thresh,\n",
    "                                  power_thresh=power_thresh, merge_thresh=merge_thresh, min_length=min_length)\n",
    "\n",
    "    print('sharp-wave ripples, total:', swrs.n_epochs)\n",
    "\n",
    "    min_involved = 4\n",
    "    epochs_interest = nept.find_multi_in_epochs(sliced_spikes, swrs, min_involved=min_involved)\n",
    "\n",
    "    print('sharp-wave ripples, min', min_involved, 'neurons :', epochs_interest.n_epochs)\n",
    "\n",
    "    if epochs_interest.n_epochs < min_swr:\n",
    "        epochs_interest = nept.Epoch(np.array([[], []]))\n",
    "\n",
    "    print('sharp-wave ripples, used :', epochs_interest.n_epochs)\n",
    "    print('sharp-wave ripples, mean durations: ', np.mean(epochs_interest.durations))\n",
    "else:\n",
    "    raise ValueError(\"unrecognized experimental phase. Must be in ['prerecord', 'phase1', 'pauseA', 'phase2', \"\n",
    "                     \"'pauseB', phase3', 'postrecord'].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode_spikes = []\n",
    "decode_tuning_curves = []\n",
    "for spiketrain, tuning_curve in zip(sliced_spikes, neurons.tuning_curves):\n",
    "    if len(spiketrain.time) > 0:\n",
    "        decode_spikes.append(spiketrain)\n",
    "        decode_tuning_curves.append(tuning_curve)\n",
    "decode_tuning_curves = np.array(decode_tuning_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_size = 0.020\n",
    "window_advance = 0.005\n",
    "# window_size = 0.025\n",
    "# window_advance = 0.025\n",
    "time_edges = nept.get_edges(exp_position, window_advance, lastbin=True)\n",
    "counts = nept.bin_spikes(decode_spikes, exp_position, window_size, \n",
    "                         window_advance, gaussian_std=None, normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tc_shape = decode_tuning_curves.shape\n",
    "decoding_tc = decode_tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "likelihood = nept.bayesian_prob(counts, decoding_tc, window_advance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pp = plt.pcolormesh(counts.data.T, cmap='pink_r')\n",
    "# plt.colorbar(pp)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xcenters = (xedges[1:] + xedges[:-1]) / 2.\n",
    "ycenters = (yedges[1:] + yedges[:-1]) / 2.\n",
    "xy_centers = nept.cartesian(xcenters, ycenters)\n",
    "\n",
    "decoded = nept.decode_location(likelihood, xy_centers, time_edges)\n",
    "nan_idx = np.logical_and(np.isnan(decoded.x), np.isnan(decoded.y))\n",
    "decoded = decoded[~nan_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(xedges, yedges)\n",
    "histogram, xs, ys = np.histogram2d(decoded.x, decoded.y, bins=xx.shape)\n",
    "\n",
    "cmap = plt.cm.get_cmap('bone_r', 25)\n",
    "pp = plt.pcolormesh(yy, xx, histogram, cmap=cmap)\n",
    "plt.colorbar(pp)\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(xedges, yedges)\n",
    "histogram, xs, ys = np.histogram2d(decoded.x, decoded.y, bins=xx.shape)\n",
    "\n",
    "cmap = plt.cm.get_cmap('bone_r', 25)\n",
    "pp = plt.pcolormesh(yy, xx, histogram, cmap=cmap)\n",
    "plt.colorbar(pp)\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(xedges, yedges)\n",
    "histogram, xs, ys = np.histogram2d(exp_position.x, exp_position.y, bins=xx.shape)\n",
    "\n",
    "cmap = plt.cm.get_cmap('bone_r', 25)\n",
    "pp = plt.pcolormesh(yy, xx, histogram, cmap=cmap)\n",
    "plt.colorbar(pp)\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(xedges, yedges)\n",
    "histogram, xs, ys = np.histogram2d(decoded.x, decoded.y, bins=xx.shape)\n",
    "\n",
    "cmap = plt.cm.get_cmap('bone_r', 25)\n",
    "pp = plt.pcolormesh(yy, xx, histogram, cmap=cmap)\n",
    "plt.colorbar(pp)\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spikes = np.hstack((np.arange(0, 10, 1.4), np.arange(0.2, 5, 0.3)))\n",
    "spikes = [nept.SpikeTrain(np.sort(spikes), 'test')]\n",
    "\n",
    "position = nept.Position(np.array([0, 2, 4, 6, 8, 10]), np.array([0, 2, 4, 6, 8, 10]))\n",
    "counts = nept.bin_spikes(spikes, position, window_size=2.,\n",
    "                             window_advance=2., gaussian_std=None, normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nept\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spikes = [nept.SpikeTrain([0.8, 1.1, 1.2, 1.2, 2.1, 3.1])]\n",
    "position = nept.Position(np.array([1., 6.]), np.array([0., 4.]))\n",
    "\n",
    "counts = nept.bin_spikes(spikes, position, window_size=2.,\n",
    "                         window_advance=0.5, gaussian_std=None)\n",
    "\n",
    "# assert np.allclose(counts.data, np.array([[0.25], [1.], [1.], [1.25], [1.], [0.5], [0.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array([[5.53572467e-01],\n",
    "       [8.87713270e-01],\n",
    "       [1.05429341e+00],\n",
    "       [1.02404730e+00],\n",
    "       [8.04477482e-01],\n",
    "       [5.62670198e-01],\n",
    "       [3.94625765e-01],\n",
    "       [2.62864946e-01],\n",
    "       [1.41733188e-01],\n",
    "       [4.50338310e-02],\n",
    "       [6.53431203e-03],\n",
    "       [3.87072740e-04],\n",
    "       [8.98294933e-06],\n",
    "       [8.01892607e-08],\n",
    "       [0.00000000e+00],\n",
    "       [0.00000000e+00],\n",
    "       [0.00000000e+00],\n",
    "       [0.00000000e+00],\n",
    "       [0.00000000e+00],\n",
    "       [0.00000000e+00],\n",
    "       [0.00000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.signal as signal\n",
    "n_gaussian_std = 5\n",
    "gaussian_std = 0.005\n",
    "\n",
    "n_points = n_gaussian_std * gaussian_std * 2 / window_advance\n",
    "n_points = max(n_points, 1.0)\n",
    "if n_points % 2 == 0:\n",
    "    n_points += 1\n",
    "gaussian_filter = signal.gaussian(n_points, gaussian_std / window_advance)\n",
    "gaussian_filter /= np.sum(gaussian_filter)\n",
    "\n",
    "smoothed_spikes = []\n",
    "for spiketrain in decode_spikes:\n",
    "    if len(spiketrain.time) > 0:\n",
    "        smoothed_spikes.append(nept.SpikeTrain(np.convolve(spiketrain.time,\n",
    "                                                           gaussian_filter, mode='same'),\n",
    "                                               spiketrain.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smoothed_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
