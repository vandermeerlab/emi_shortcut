{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, LineString\n",
    "# import itertools\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "\n",
    "from load_data import get_pos, get_spikes\n",
    "from maze_functions import find_zones, trajectory_fields\n",
    "from plotting_functions import plot_intersects, plot_zone\n",
    "from decode_functions import get_edges\n",
    "\n",
    "import vdmlab as vdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xy = np.array([[2, 7],\n",
    "               [4, 5],\n",
    "               [6, 3],\n",
    "               [8, 1],\n",
    "               [2, 4]])\n",
    "time = np.array([0., 1., 2., 3., 4.])\n",
    "position = vdm.Position(xy, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(position.x, position.y, 'b.', ms=10)\n",
    "plt.xlim(0.5, 8.5)\n",
    "plt.ylim(0.5, 7.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spikes = [vdm.SpikeTrain(np.array([3.6, 3.9])), \n",
    "          vdm.SpikeTrain(np.array([0., 0., 2.])),\n",
    "          vdm.SpikeTrain(np.array([2., 2.4]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binsize = 2\n",
    "xedges = np.arange(position.x.min(), position.x.max()+binsize, binsize)\n",
    "yedges = np.arange(position.y.min(), position.y.max()+binsize, binsize)\n",
    "\n",
    "tuning_curves = vdm.tuning_curve_2d(position, spikes, xedges, yedges, sampling_rate=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "xx, yy = np.meshgrid(xedges, yedges)\n",
    "for tuning_curve in tuning_curves:\n",
    "    pp = plt.pcolormesh(xx, yy, tuning_curve, cmap='YlGn')\n",
    "    plt.colorbar(pp)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_binsize = 0.5\n",
    "\n",
    "time_edges = get_edges(position, counts_binsize, lastbin=True)\n",
    "counts = vdm.get_counts(spikes, time_edges, apply_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(time_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoding_tc = []\n",
    "for tuning_curve in tuning_curves:\n",
    "    decoding_tc.append(np.ravel(tuning_curve))\n",
    "decoding_tc = np.array(decoding_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = tuning_curves[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoding_tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likelihood = vdm.bayesian_prob(counts, decoding_tc, counts_binsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xcenters = (xedges[1:] + xedges[:-1]) / 2.\n",
    "ycenters = (yedges[1:] + yedges[:-1]) / 2.\n",
    "xy_centers = vdm.cartesian(xcenters, ycenters)\n",
    "\n",
    "time_centers = (time_edges[1:] + time_edges[:-1]) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded = vdm.decode_location(likelihood, xy_centers, time_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded.x, decoded.y, decoded.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nan_idx = np.logical_and(np.isnan(decoded.x), np.isnan(decoded.y))\n",
    "decoded = decoded[~nan_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded.x, decoded.y, decoded.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_spline = InterpolatedUnivariateSpline(position.time, position.x)\n",
    "y_spline = InterpolatedUnivariateSpline(position.time, position.y)\n",
    "actual_position = vdm.Position(np.hstack((x_spline(time_centers)[..., np.newaxis],\n",
    "                                         (y_spline(time_centers)[..., np.newaxis]))), time_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actual_position.x, actual_position.y, actual_position.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = np.abs(decoded.data - actual_position.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_error = np.nanmean(error)\n",
    "avg_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the 1D decoding work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([2, 4, 6, 8, 3])\n",
    "time = np.array([0., 1., 2., 3., 4.])\n",
    "position = vdm.Position(x, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spikes = [vdm.SpikeTrain(np.array([3.6, 3.9])), vdm.SpikeTrain(np.array([2.2, 2.43]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_binsize = 1\n",
    "tuning_curves = vdm.tuning_curve(position, spikes, binsize=pos_binsize, sampling_rate=1., gaussian_std=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(tuning_curves[0], 'b')\n",
    "plt.plot(tuning_curves[1], 'm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts_binsize = 0.5\n",
    "time_edges = get_edges(position, counts_binsize, lastbin=True)\n",
    "counts = vdm.get_counts(spikes, time_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "likelihood = vdm.bayesian_prob(counts, tuning_curves, counts_binsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_edges = vdm.binned_position(position, pos_binsize)\n",
    "x_centers = (pos_edges[1:] + pos_edges[:-1]) / 2.\n",
    "x_centers = x_centers[..., np.newaxis]\n",
    "\n",
    "time_centers = (time_edges[1:] + time_edges[:-1]) / 2.\n",
    "\n",
    "decoded = vdm.decode_location(likelihood, x_centers, time_centers)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan_idx = np.isnan(decoded.x)\n",
    "decoded = decoded[~nan_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded.x, decoded.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spline = InterpolatedUnivariateSpline(position.time, position.x)\n",
    "actual_position = vdm.Position(spline(decoded.time), decoded.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actual_position.x, actual_position.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = np.abs(decoded.x - actual_position.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded.x, actual_position.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_error = np.mean(error)\n",
    "avg_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# check velocity 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([2, 4, 6, 8, 3])\n",
    "time = np.array([0., 1., 2., 3., 4.])\n",
    "position = vdm.Position(x, time)\n",
    "\n",
    "spikes = [vdm.SpikeTrain(np.array([3.6, 3.9])), \n",
    "          vdm.SpikeTrain(np.array([2.2, 2.4])),\n",
    "          vdm.SpikeTrain(np.array([0.6, 0.9])),\n",
    "          vdm.SpikeTrain(np.array([1., 1.1])), \n",
    "          vdm.SpikeTrain(np.array([1.7, 1.9]))]\n",
    "\n",
    "pos_binsize = 1\n",
    "tuning_curves = vdm.tuning_curve(position, spikes, binsize=pos_binsize, sampling_rate=1., gaussian_std=None)\n",
    "\n",
    "counts_binsize = 0.5\n",
    "time_edges = get_edges(position, counts_binsize, lastbin=True)\n",
    "counts = vdm.get_counts(spikes, time_edges)\n",
    "\n",
    "likelihood = vdm.bayesian_prob(counts, tuning_curves, counts_binsize)\n",
    "\n",
    "pos_edges = vdm.binned_position(position, pos_binsize)\n",
    "x_centers = (pos_edges[1:] + pos_edges[:-1]) / 2.\n",
    "x_centers = x_centers[..., np.newaxis]\n",
    "\n",
    "time_centers = (time_edges[1:] + time_edges[:-1]) / 2.\n",
    "\n",
    "decoded = vdm.decode_location(likelihood, x_centers, time_centers)\n",
    "\n",
    "nan_idx = np.isnan(decoded.x)\n",
    "decoded = decoded[~nan_idx]\n",
    "\n",
    "print(decoded.x, decoded.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode_jumps = vdm.remove_teleports(decoded, speed_thresh=0, min_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode_jumps.time, decode_jumps.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts filtering parameter check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std = [0.1, 0.025, 0.01, 0.002, None, 0.5, 1.0]\n",
    "error = [29.9113296726, 29.3032199091, 45.8427697608, 45.8427697608, 45.8427697608, 28.623598705, 29.0339763118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(std, error, '.', ms=15)\n",
    "plt.xlim(-0.1, 1.1)\n",
    "plt.ylim(25, 48)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load_data import get_pos, get_spikes, get_lfp\n",
    "\n",
    "import info.R063d2_info as r063d2\n",
    "import info.R063d3_info as r063d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from shapely.geometry import Point, LineString\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import vdmlab as vdm\n",
    "\n",
    "from load_data import get_pos, get_spikes\n",
    "from maze_functions import find_zones\n",
    "from tuning_curves_functions import get_tc_1d, find_ideal\n",
    "from decode_functions import get_edges, point_in_zones, compare_rates\n",
    "from plotting_functions import (plot_decoded, plot_decoded_pause, plot_decoded_errors,\n",
    "                                plot_compare_decoded_pauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle_filepath = 'C:\\\\Users\\\\Emily\\\\Code\\\\emi_shortcut\\\\cache\\\\pickled\\\\'\n",
    "# output_filepath = 'C:\\\\Users\\\\Emily\\\\Code\\\\emi_shortcut\\\\plots\\\\'\n",
    "pickle_filepath = 'E:\\\\code\\\\emi_shortcut\\\\cache\\\\pickled\\\\'\n",
    "output_filepath = 'E:\\\\code\\\\emi_shortcut\\\\plots\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info = r063d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_length = LineString(info.u_trajectory).length\n",
    "shortcut_length = LineString(info.shortcut_trajectory).length\n",
    "novel_length = LineString(info.novel_trajectory).length\n",
    "print(u_length, shortcut_length, novel_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_decoded(infos, experiment_time='tracks', shuffle_id=False):\n",
    "    \"\"\"Finds combined decoded from all sessions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info: list of modules\n",
    "    experiment_time: str\n",
    "    shuffle_id: bool\n",
    "        Defaults to False (not shuffled)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined_decoded: dict of vdmlab.Position objects\n",
    "        With u, shortcut, novel, other, together as keys.\n",
    "    combined_errors: list of np.arrays\n",
    "    total_times: list\n",
    "\n",
    "    \"\"\"\n",
    "    total_times = []\n",
    "    combined_errors = []\n",
    "    combined_lengths = dict(u=[], shortcut=[], novel=[], other=[], together=[])\n",
    "    combined_decoded = dict(u=[], shortcut=[], novel=[], other=[], together=[])\n",
    "\n",
    "    for info in infos:\n",
    "        print(info.session_id)\n",
    "        position = get_pos(info.pos_mat, info.pxl_to_cm)\n",
    "        spikes = get_spikes(info.spike_mat)\n",
    "\n",
    "        speed = position.speed(t_smooth=0.5)\n",
    "        run_idx = np.squeeze(speed.data) >= 0.1\n",
    "        run_pos = position[run_idx]\n",
    "\n",
    "        track_starts = [info.task_times['phase1'].start,\n",
    "                        info.task_times['phase2'].start,\n",
    "                        info.task_times['phase3'].start]\n",
    "        track_stops = [info.task_times['phase1'].stop,\n",
    "                       info.task_times['phase2'].stop,\n",
    "                       info.task_times['phase3'].stop]\n",
    "\n",
    "        # track_start = info.task_times['phase3'].start\n",
    "        # track_stop = info.task_times['phase3'].stop\n",
    "\n",
    "        track_pos = run_pos.time_slices(track_starts, track_stops)\n",
    "\n",
    "        track_spikes = [spiketrain.time_slices(track_starts, track_stops) for spiketrain in spikes]\n",
    "\n",
    "        binsize = 3\n",
    "        xedges = np.arange(track_pos.x.min(), track_pos.x.max() + binsize, binsize)\n",
    "        yedges = np.arange(track_pos.y.min(), track_pos.y.max() + binsize, binsize)\n",
    "\n",
    "        tuning_curves = vdm.tuning_curve_2d(track_pos, track_spikes, xedges, yedges, gaussian_sigma=0.1)\n",
    "\n",
    "        if shuffle_id:\n",
    "            random.shuffle(tuning_curves)\n",
    "\n",
    "        if experiment_time == 'tracks':\n",
    "            decode_spikes = track_spikes\n",
    "        else:\n",
    "            decode_spikes = [spiketrain.time_slice(info.task_times[experiment_time].start,\n",
    "                                                   info.task_times[experiment_time].stop) for spiketrain in spikes]\n",
    "\n",
    "        counts_binsize = 0.025\n",
    "        time_edges = get_edges(run_pos, counts_binsize, lastbin=True)\n",
    "        counts = vdm.get_counts(decode_spikes, time_edges, gaussian_std=0.025)\n",
    "\n",
    "        decoding_tc = []\n",
    "        for tuning_curve in tuning_curves:\n",
    "            decoding_tc.append(np.ravel(tuning_curve))\n",
    "        decoding_tc = np.array(decoding_tc)\n",
    "\n",
    "        likelihood = vdm.bayesian_prob(counts, decoding_tc, counts_binsize)\n",
    "\n",
    "        xcenters = (xedges[1:] + xedges[:-1]) / 2.\n",
    "        ycenters = (yedges[1:] + yedges[:-1]) / 2.\n",
    "        xy_centers = vdm.cartesian(xcenters, ycenters)\n",
    "\n",
    "        time_centers = (time_edges[1:] + time_edges[:-1]) / 2.\n",
    "\n",
    "        decoded = vdm.decode_location(likelihood, xy_centers, time_centers)\n",
    "        nan_idx = np.logical_and(np.isnan(decoded.x), np.isnan(decoded.y))\n",
    "        decoded = decoded[~nan_idx]\n",
    "\n",
    "        if not decoded.isempty:\n",
    "            decoded = vdm.remove_teleports(decoded, speed_thresh=10, min_length=3)\n",
    "\n",
    "        actual_x = np.interp(decoded.time, track_pos.time, track_pos.x)\n",
    "        actual_y = np.interp(decoded.time, track_pos.time, track_pos.y)\n",
    "\n",
    "        actual_position = vdm.Position(np.hstack((actual_x[..., np.newaxis], actual_y[..., np.newaxis])), decoded.time)\n",
    "\n",
    "        errors = actual_position.distance(decoded)\n",
    "\n",
    "        zones = find_zones(info, expand_by=7)\n",
    "        actual_zones = point_in_zones(actual_position, zones)\n",
    "        decoded_zones = point_in_zones(decoded, zones)\n",
    "\n",
    "        total_times.append(len(time_edges) - 1)\n",
    "\n",
    "        combined_errors.extend(errors)\n",
    "\n",
    "        combined_lengths['u'].append(LineString(info.u_trajectory).length)\n",
    "        combined_lengths['shortcut'].append(LineString(info.shortcut_trajectory).length)\n",
    "        combined_lengths['novel'].append(LineString(info.novel_trajectory).length)\n",
    "\n",
    "        # combined_actual['u'].append(actual_zones['u'])\n",
    "        # combined_actual['shortcut'].append(actual_zones['shortcut'])\n",
    "        # combined_actual['novel'].append(actual_zones['novel'])\n",
    "        # combined_actual['other'].append(actual_zones['other'])\n",
    "        # combined_actual['together'].append(len(actual_zones['u'].time) + len(actual_zones['shortcut'].time) +\n",
    "        #                                    len(actual_zones['novel'].time) + len(actual_zones['other'].time))\n",
    "\n",
    "        combined_decoded['u'].append(decoded_zones['u'])\n",
    "        combined_decoded['shortcut'].append(decoded_zones['shortcut'])\n",
    "        combined_decoded['novel'].append(decoded_zones['novel'])\n",
    "        combined_decoded['other'].append(decoded_zones['other'])\n",
    "        combined_decoded['together'].append(len(decoded_zones['u'].time) + len(decoded_zones['shortcut'].time) +\n",
    "                                            len(decoded_zones['novel'].time) + len(decoded_zones['other'].time))\n",
    "\n",
    "    return combined_decoded, combined_errors, total_times, combined_lengths\n",
    "\n",
    "\n",
    "# def compare_decoded_actual(combined_actual, combined_decoded, shuffle_id, output_filepath, pause=None):\n",
    "#     keys = ['u', 'shortcut', 'novel']\n",
    "#\n",
    "#     actual = dict(u=[], shortcut=[], novel=[])\n",
    "#     decode = dict(u=[], shortcut=[], novel=[])\n",
    "#\n",
    "#     n_sessions = len(combined_actual['together'])\n",
    "#\n",
    "#     # length_tracks = info.track_length['u'] + info.track_length['shortcut'] + info.track_length['novel']\n",
    "#\n",
    "#     for key in keys:\n",
    "#         if len(combined_actual['together']) != len(combined_decoded['together']):\n",
    "#             raise ValueError(\"must have same number of decoded and actual samples\")\n",
    "#\n",
    "#         for val in range(n_sessions):\n",
    "#             actual[key].append(len(combined_actual[key][val].time)/combined_actual['together'][val])\n",
    "#             decode[key].append(len(combined_decoded[key][val].time)/combined_decoded['together'][val])\n",
    "#\n",
    "#     if shuffle_id and pause is None:\n",
    "#         filename = 'combined-phase3-id_shuffle_decoded.png'\n",
    "#     elif shuffle_id and pause is not None:\n",
    "#         filename = 'combined-' + pause + '-id_shuffle_decoded.png'\n",
    "#     elif pause is not None and not shuffle_id:\n",
    "#         filename = 'combined-' + pause + '_decoded.png'\n",
    "#     elif not shuffle_id and pause is None:\n",
    "#         filename = 'combined-phase3_decoded.png'\n",
    "#     else:\n",
    "#         filename = 'unknown_combination.png'\n",
    "#     savepath = os.path.join(output_filepath, filename)\n",
    "#\n",
    "#     if pause is not None:\n",
    "#         plot_compare_decoded_track(decode, max_y=1.0, savepath=savepath)\n",
    "#     else:\n",
    "#         plot_compare_decoded_track(decode, actual, distance=str(round(np.mean(combined_errors), 2)),\n",
    "#                                    max_y=1.0, savepath=savepath)\n",
    "#\n",
    "#     print('Decoded errors:', combined_errors)\n",
    "\n",
    "\n",
    "def normalized_time_spent(combined_decoded, n_sessions, lengths):\n",
    "    decoded_linger = dict(u=[], shortcut=[], novel=[])\n",
    "    decoded_length = dict(u=[], shortcut=[], novel=[])\n",
    "    for val in range(n_sessions):\n",
    "        decode = dict()\n",
    "        length = dict()\n",
    "        for key in decoded_linger:\n",
    "            decode[key] = combined_decoded[key][val]\n",
    "            length[key] = lengths[key][val]\n",
    "        norm_decoded = compare_rates(decode)\n",
    "        len_decoded = compare_lengths(decode, length)\n",
    "        for key in decoded_linger:\n",
    "            decoded_linger[key].append(norm_decoded[key])\n",
    "            decoded_length[key].append(len_decoded[key])\n",
    "\n",
    "    filename = 'combined-time-norm_tracks_decoded.png'\n",
    "    savepath = os.path.join(output_filepath, filename)\n",
    "    y_label = 'Points normalized by time spent'\n",
    "    plot_decoded(decoded_linger, y_label=y_label, max_y=None, savepath=savepath)\n",
    "\n",
    "    filename = 'combined-length-norm_tracks_decoded.png'\n",
    "    savepath = os.path.join(output_filepath, filename)\n",
    "    y_label = 'Points normalized by track length'\n",
    "    plot_decoded(decoded_length, y_label=y_label, max_y=None, savepath=savepath)\n",
    "\n",
    "\n",
    "infos = [r063d2, r063d3]\n",
    "# infos = [r063d2, r063d3, r063d4, r063d5, r063d6,\n",
    "#          r066d1, r066d2, r066d3, r066d4, r066d5,\n",
    "#          r067d1, r067d2, r067d3, r067d4, r067d5,\n",
    "#          r068d1, r068d2, r068d3, r068d4, r068d5]\n",
    "\n",
    "\n",
    "# Plot decoding errors during track times\n",
    "if 0:\n",
    "    combined_decoded, combined_errors, total_times, lengths = get_decoded(infos, shuffle_id=False)\n",
    "    print('Mean decoded errors:', np.mean(combined_errors))\n",
    "    shuffled_decoded, shuffled_errors, shuffled_times, shuffled_lengths = get_decoded(infos, shuffle_id=True)\n",
    "    print('Mean shuffled errors:', np.mean(combined_errors))\n",
    "    filename = 'combined-errors_decoded.png'\n",
    "    savepath = os.path.join(output_filepath, filename)\n",
    "    plot_decoded_errors(combined_errors, shuffled_errors, savepath=savepath)\n",
    "\n",
    "# Plot proportion of pauseA and pauseB spent in each trajectory\n",
    "if 1:\n",
    "    experiment_time = 'pauseA'\n",
    "    decoded_pausea, errors_pausea, times_pausea, lengths_pausea = get_decoded(infos, experiment_time, shuffle_id=False)\n",
    "    filename = 'combined-' + experiment_time + '_decoded.png'\n",
    "    savepath = os.path.join(output_filepath, filename)\n",
    "    plot_decoded_pause(decoded_pausea, times_pausea, savepath=savepath)\n",
    "\n",
    "    experiment_time = 'pauseB'\n",
    "    decoded_pauseb, errors_pauseb, times_pauseb, lengths_pauseb = get_decoded(infos, experiment_time, shuffle_id=False)\n",
    "    filename = 'combined-' + experiment_time + '_decoded.png'\n",
    "    savepath = os.path.join(output_filepath, filename)\n",
    "    plot_decoded_pause(decoded_pauseb, times_pauseb, savepath=savepath)\n",
    "\n",
    "    filename = 'combined-pauses_decoded.png'\n",
    "    savepath = os.path.join(output_filepath, filename)\n",
    "    plot_compare_decoded_pauses(decoded_pausea, times_pausea, decoded_pauseb, times_pauseb, ['Pause A', 'Pause B'],\n",
    "                                savepath=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_compare_decoded_pauses(decoded_1, times_1, decoded_2, times_2, labels, savepath=None):\n",
    "    \"\"\"Plots barplot comparing decoded during two phases\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    decoded_1: dict\n",
    "        With u, shortcut, novel, other as keys, each a vdmlab.Position object.\n",
    "    times_1: list of floats\n",
    "    decoded_2: dict\n",
    "        With u, shortcut, novel, other as keys, each a vdmlab.Position object.\n",
    "    times_2: list of floats\n",
    "    labels: list of str\n",
    "    savepath : str or None\n",
    "        Location and filename for the saved plot.\n",
    "\n",
    "    \"\"\"\n",
    "    decode_1 = dict(u=[], shortcut=[], novel=[])\n",
    "    for key in decode_1:\n",
    "        for session in range(len(times_1)):\n",
    "            decode_1[key].append(len(decoded_1[key][session].time)/times_1[session])\n",
    "\n",
    "    u_dict1 = dict(total=decode_1['u'], trajectory='U', exp_time=labels[0])\n",
    "    shortcut_dict1 = dict(total=decode_1['shortcut'], trajectory='Shortcut', exp_time=labels[0])\n",
    "    novel_dict1 = dict(total=decode_1['novel'], trajectory='Novel', exp_time=labels[0])\n",
    "\n",
    "    u1 = pd.DataFrame(u_dict1)\n",
    "    shortcut1 = pd.DataFrame(shortcut_dict1)\n",
    "    novel1 = pd.DataFrame(novel_dict1)\n",
    "\n",
    "    decode_2 = dict(u=[], shortcut=[], novel=[])\n",
    "    for key in decode_2:\n",
    "        for session in range(len(times_2)):\n",
    "            decode_2[key].append(len(decoded_2[key][session].time)/times_2[session])\n",
    "\n",
    "    u_dict2 = dict(total=decode_2['u'], trajectory='U', exp_time=labels[1])\n",
    "    shortcut_dict2 = dict(total=decode_2['shortcut'], trajectory='Shortcut', exp_time=labels[1])\n",
    "    novel_dict2 = dict(total=decode_2['novel'], trajectory='Novel', exp_time=labels[1])\n",
    "\n",
    "    u2 = pd.DataFrame(u_dict2)\n",
    "    shortcut2 = pd.DataFrame(shortcut_dict2)\n",
    "    novel2 = pd.DataFrame(novel_dict2)\n",
    "    data = pd.concat([u1, shortcut1, novel1, u2, shortcut2, novel2])\n",
    "\n",
    "    together = data.groupby(['trajectory', 'exp_time'])['total'].mean()\n",
    "\n",
    "    def plot_v3(data, exp_labels):\n",
    "        fig = sns.FacetGrid(data, col='trajectory', col_order=['U', 'Shortcut', 'Novel'], sharex=False)\n",
    "\n",
    "        fig.map(sns.barplot, 'trajectory', 'total', 'exp_time',  hue_order=[exp_labels[0], exp_labels[1]])\n",
    "        axes = np.array(fig.axes.flat)\n",
    "\n",
    "        return plt.gcf(), axes\n",
    "\n",
    "    def set_labels(fig, axes, exp_labels):\n",
    "        labels = ['U', 'Shortcut', 'Novel']\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.set_xticks([-.2, .2])\n",
    "            ax.set_xticklabels([exp_labels[0], exp_labels[1]])\n",
    "            ax.set_xlabel(labels[i])\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_title('')\n",
    "\n",
    "        axes.flat[0].set_ylabel('Proportion of time')\n",
    "\n",
    "        sns.despine(ax=axes[1], left=True)\n",
    "        sns.despine(ax=axes[2], left=True)\n",
    "\n",
    "    def set_style():\n",
    "        plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "\n",
    "    def color_bars(axes):\n",
    "        colors = sns.color_palette('colorblind')\n",
    "        edges = sns.color_palette('deep')\n",
    "        for i in range(3):\n",
    "            p1, p2 = axes[i].patches\n",
    "\n",
    "            p1.set_color(colors[i])\n",
    "\n",
    "            p2.set_color(colors[i])\n",
    "            p2.set_edgecolor(edges[i])\n",
    "            p2.set_hatch('///')\n",
    "\n",
    "    def set_size(fig):\n",
    "        fig.set_size_inches(6, 3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    set_style()\n",
    "    fig, axes = plot_v3(data, labels)\n",
    "    set_labels(fig, axes, labels)\n",
    "    color_bars(axes)\n",
    "\n",
    "    if savepath:\n",
    "        fig.savefig(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_compare_decoded_pauses(decoded_pausea, times_pausea, decoded_pauseb, times_pauseb, \n",
    "                            ['Pause A', 'Pause B'], savepath=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded_1 = decoded_pausea\n",
    "times_1 = times_pausea\n",
    "decoded_2 = decoded_pauseb\n",
    "times_2 = times_pauseb\n",
    "labels = ['Pause A', 'Pause B']\n",
    "\n",
    "decode_1 = dict(u=[], shortcut=[], novel=[])\n",
    "for key in decode_1:\n",
    "    for session in range(len(times_1)):\n",
    "        decode_1[key].append(len(decoded_1[key][session].time)/times_1[session])\n",
    "\n",
    "u_dict1 = dict(total=decode_1['u'], trajectory='U', exp_time=labels[0])\n",
    "shortcut_dict1 = dict(total=decode_1['shortcut'], trajectory='Shortcut', exp_time=labels[0])\n",
    "novel_dict1 = dict(total=decode_1['novel'], trajectory='Novel', exp_time=labels[0])\n",
    "\n",
    "u1 = pd.DataFrame(u_dict1)\n",
    "shortcut1 = pd.DataFrame(shortcut_dict1)\n",
    "novel1 = pd.DataFrame(novel_dict1)\n",
    "\n",
    "decode_2 = dict(u=[], shortcut=[], novel=[])\n",
    "for key in decode_2:\n",
    "    for session in range(len(times_2)):\n",
    "        decode_2[key].append(len(decoded_2[key][session].time)/times_2[session])\n",
    "\n",
    "u_dict2 = dict(total=decode_2['u'], trajectory='U', exp_time=labels[1])\n",
    "shortcut_dict2 = dict(total=decode_2['shortcut'], trajectory='Shortcut', exp_time=labels[1])\n",
    "novel_dict2 = dict(total=decode_2['novel'], trajectory='Novel', exp_time=labels[1])\n",
    "\n",
    "u2 = pd.DataFrame(u_dict2)\n",
    "shortcut2 = pd.DataFrame(shortcut_dict2)\n",
    "novel2 = pd.DataFrame(novel_dict2)\n",
    "data = pd.concat([u1, shortcut1, novel1, u2, shortcut2, novel2])\n",
    "\n",
    "# together = data.groupby(['trajectory', 'exp_time'])['total'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_v2(data, exp_labels):\n",
    "    ax = sns.barplot(\n",
    "        x='trajectory', y='total', hue='exp_time',\n",
    "        order=['U', 'Shortcut', 'Novel'],\n",
    "        hue_order=[exp_labels[0], exp_labels[1]],\n",
    "        data=data)\n",
    "\n",
    "    return plt.gcf(), ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_v3(data, exp_labels):\n",
    "    g = sns.FacetGrid(\n",
    "        data,\n",
    "        col='trajectory',\n",
    "        col_order=['U', 'Shortcut', 'Novel'],\n",
    "        sharex=False)\n",
    "\n",
    "    g.map(sns.barplot,\n",
    "        'trajectory', 'total', 'exp_time',\n",
    "        hue_order=[exp_labels[0], exp_labels[1]])\n",
    "\n",
    "    axes = np.array(g.axes.flat)\n",
    "\n",
    "    return plt.gcf(), axes\n",
    "\n",
    "def set_labels(fig, axes, exp_labels):\n",
    "    labels = ['U', 'Shortcut', 'Novel']\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.set_xticks([-.2, .2])\n",
    "        ax.set_xticklabels([exp_labels[0], exp_labels[1]])\n",
    "        ax.set_xlabel(labels[i])\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_title('')\n",
    "\n",
    "    axes.flat[0].set_ylabel('Proportion of time')\n",
    "    \n",
    "    sns.despine(ax=axes[1], left=True)\n",
    "    sns.despine(ax=axes[2], left=True)\n",
    "    \n",
    "def set_style():\n",
    "    plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "\n",
    "def get_colors():\n",
    "    return np.array([\n",
    "        [0.1, 0.1, 0.1],           # black\n",
    "        [0.4, 0.4, 0.4],           # very dark gray\n",
    "        [0.984375, 0.7265625, 0],  # dark yellow\n",
    "        [1, 1, 0.9],               # light yellow\n",
    "        [0.7, 0.7, 0.7],           # dark gray\n",
    "        [0.9, 0.9, 0.9],           # light gray\n",
    "        ])\n",
    "\n",
    "def color_bars(axes, colors):\n",
    "    for i in range(3):\n",
    "        dark_color = colors[i*2]\n",
    "        light_color = colors[i*2 + 1]\n",
    "\n",
    "        p1, p2 = axes[i].patches\n",
    "\n",
    "        p1.set_color(dark_color)\n",
    "\n",
    "        p2.set_color(light_color)\n",
    "        p2.set_edgecolor(dark_color)\n",
    "        p2.set_hatch('////')\n",
    "\n",
    "def set_size(fig):\n",
    "    fig.set_size_inches(6, 3)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_style()\n",
    "fig, axes = plot_v3(data, labels)\n",
    "set_labels(fig, axes, labels)\n",
    "color_bars(axes, get_colors())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = np.random.rand(20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_idx = np.array([4, 6, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_idx = [idx for idx in np.split(np.arange(pos.shape[0]), split_idx) if idx.size > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos[np.hstack(all_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = np.linspace(0, np.pi*2, 201)\n",
    "data = np.hstack((np.sin(time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(time, data, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "position = vdm.Position(data, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speed = position.speed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(speed.time, speed.data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_idx = np.squeeze(speed.data) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "position = vdm.Position(data, time)\n",
    "speed = position.speed()\n",
    "run_idx = np.squeeze(speed.data) >= 0.7\n",
    "run_position = position[run_idx]\n",
    "\n",
    "len(run_position.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert np.allclose(len(run_position.x), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "velocity = self[1:].distance(self[:-1])\n",
    "velocity /= np.diff(self.time)\n",
    "velocity = np.hstack(([0], velocity))\n",
    "\n",
    "if t_smooth is not None:\n",
    "    dt = np.median(np.diff(self.time))\n",
    "    filter_length = np.ceil(t_smooth / dt)\n",
    "    velocity = np.convolve(velocity, np.ones(int(filter_length))/filter_length, 'same')\n",
    "\n",
    "return AnalogSignal(velocity, self.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_smooth=0.5\n",
    "velocity = np.diff(np.squeeze(position.data))\n",
    "velocity /= np.diff(position.time)\n",
    "velocity = np.hstack(([0], velocity))\n",
    "\n",
    "dt = np.median(np.diff(position.time))\n",
    "filter_length = np.ceil(t_smooth / dt)\n",
    "velocity = np.convolve(velocity, np.ones(int(filter_length))/filter_length, 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.hstack((a[0:2], a[6:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = []\n",
    "for t_start, t_stop in zip(np.array([0, 6]), np.array([2, 8])):\n",
    "    indices.append((a >= t_start) & (a <= t_stop))\n",
    "indices = np.any(np.column_stack((indices)),axis=1)\n",
    "a[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = np.any(np.column_stack((indices)),axis=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spikes = [vdm.SpikeTrain(np.array([0., 0., 1.])), vdm.SpikeTrain(np.array([3.6, 3.9]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_line = LineString([[2, 0], [2, 2], [2, 4], [2, 6], [2, 8], [2, 10]])\n",
    "\n",
    "one_start = Point([2, 2])\n",
    "one_stop = Point([2, 8])\n",
    "\n",
    "expand_by = 1\n",
    "\n",
    "one_zone = vdm.expand_line(one_start, one_stop, one_line, expand_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this_idx = []\n",
    "for pos_idx in range(len(position.time)):\n",
    "    point = Point([position.x[pos_idx], position.y[pos_idx]])\n",
    "    if one_zone.contains(point):\n",
    "        this_idx.append(pos_idx)\n",
    "        \n",
    "this_pos = position[this_idx]\n",
    "linear = this_pos.linearize(one_line, one_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(position.x, position.y, 'g.', ms=10)\n",
    "# plt.plot([2, 4, 2], [7, 5, 4], 'm.', ms=20)\n",
    "plt.plot([2, 6], [4, 3], 'm.', ms=20)\n",
    "plt.plot(one_zone.exterior.xy[0], one_zone.exterior.xy[1], 'b', lw=1)\n",
    "plt.xlim(-1, 10)\n",
    "plt.ylim(-1, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_line(start_pt, stop_pt, line, expand_by):\n",
    "    line_expanded = line.buffer(expand_by)\n",
    "    zone = start_pt.union(line_expanded).union(stop_pt)\n",
    "    \n",
    "    return zone\n",
    "\n",
    "def find_zones(info, expand_by=6):\n",
    "    u_line = LineString(info.u_trajectory)\n",
    "    shortcut_line = LineString(info.shortcut_trajectory)\n",
    "    novel_line = LineString(info.novel_trajectory)\n",
    "\n",
    "    u_start = Point(info.u_trajectory[0])\n",
    "    u_stop = Point(info.u_trajectory[-1])\n",
    "    shortcut_start = Point(info.shortcut_trajectory[0])\n",
    "    shortcut_stop = Point(info.shortcut_trajectory[-1])\n",
    "    novel_start = Point(info.novel_trajectory[0])\n",
    "    novel_stop = Point(info.novel_trajectory[-1])\n",
    "    pedestal_center = Point(info.path_pts['pedestal'][0], info.path_pts['pedestal'][1])\n",
    "    pedestal = pedestal_center.buffer(expand_by*2.2)\n",
    "\n",
    "    zone = dict()\n",
    "    zone['u'] = expand_line(u_start, u_stop, u_line, expand_by)\n",
    "    zone['shortcut'] = expand_line(shortcut_start, shortcut_stop, shortcut_line, expand_by)\n",
    "    zone['novel'] = expand_line(novel_start, novel_stop, novel_line, expand_by)\n",
    "    zone['ushort'] = zone['u'].intersection(zone['shortcut'])\n",
    "    zone['unovel'] = zone['u'].intersection(zone['novel'])\n",
    "    zone['uped'] = zone['u'].intersection(pedestal)\n",
    "    zone['shortped'] = zone['shortcut'].intersection(pedestal)\n",
    "    zone['novelped'] = zone['novel'].intersection(pedestal)\n",
    "    zone['pedestal'] = pedestal\n",
    "    \n",
    "    return zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trajectory_fields(tuning_curves, zone, xedges, yedges, field_thresh):\n",
    "    \n",
    "    xcenters = np.array((xedges[1:] + xedges[:-1]) / 2.)\n",
    "    ycenters = np.array((yedges[1:] + yedges[:-1]) / 2.)\n",
    "    \n",
    "    tuning_points = []\n",
    "    for i in itertools.product(ycenters, xcenters):\n",
    "        tuning_points.append(i)\n",
    "    tuning_points = np.array(tuning_points)\n",
    "\n",
    "    this_neuron = 0\n",
    "    fields_tc = dict(u=[], shortcut=[], novel=[], pedestal=[])\n",
    "    fields_neuron = dict(u=[], shortcut=[], novel=[], pedestal=[])\n",
    "    for neuron_tc in tuning_curves:\n",
    "        this_neuron += 1\n",
    "        field_idx = neuron_tc.flatten() > field_thresh\n",
    "        field = tuning_points[field_idx]\n",
    "        for pt in field:\n",
    "            point = Point([pt[0], pt[1]])\n",
    "            if zone['u'].contains(point) or zone['ushort'].contains(point) or zone['unovel'].contains(point):\n",
    "                if this_neuron not in fields_neuron['u']:\n",
    "                    fields_tc['u'].append(neuron_tc)\n",
    "                    fields_neuron['u'].append(this_neuron)\n",
    "            if zone['shortcut'].contains(point) or zone['shortped'].contains(point):\n",
    "                if this_neuron not in fields_neuron['shortcut']:\n",
    "                    fields_tc['shortcut'].append(neuron_tc)\n",
    "                    fields_neuron['shortcut'].append(this_neuron)\n",
    "            if zone['novel'].contains(point) or zone['novelped'].contains(point):\n",
    "                if this_neuron not in fields_neuron['novel']:\n",
    "                    fields_tc['novel'].append(neuron_tc)\n",
    "                    fields_neuron['novel'].append(this_neuron)\n",
    "            if zone['pedestal'].contains(point):\n",
    "                if this_neuron not in fields_neuron['pedestal']:\n",
    "                    fields_tc['pedestal'].append(neuron_tc)\n",
    "                    fields_neuron['pedestal'].append(this_neuron)\n",
    "                \n",
    "    return fields_tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('C:\\\\Users\\\\Emily\\\\Code\\\\emi_shortcut\\\\info')\n",
    "sys.path.append('E:\\\\code\\\\emi_shortcut\\\\info')\n",
    "import info.R063d3_info as r063d3\n",
    "info = r063d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_filepath = 'E:\\\\code\\\\emi_shortcut\\\\cache\\\\pickled'\n",
    "# pickle_filepath = 'C:\\\\Users\\\\Emily\\\\Code\\\\emi_shortcut\\\\cache\\\\pickled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "position = get_pos(info.pos_mat, info.pxl_to_cm)\n",
    "spikes = get_spikes(info.spike_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binsize = 3\n",
    "xedges = np.arange(position.x.min(), position.x.max()+binsize, binsize)\n",
    "yedges = np.arange(position.y.min(), position.y.max()+binsize, binsize)\n",
    "\n",
    "speed = position.speed(t_smooth=0.5)\n",
    "run_idx = np.squeeze(speed.data) >= info.run_threshold\n",
    "run_pos = position[run_idx]\n",
    "\n",
    "t_start = info.task_times['phase3'].start\n",
    "t_stop = info.task_times['phase3'].stop\n",
    "\n",
    "sliced_pos = run_pos.time_slice(t_start, t_stop)\n",
    "\n",
    "sliced_spikes = [spiketrain.time_slice(t_start, t_stop) for spiketrain in spikes]\n",
    "\n",
    "tuning_curves = vdm.tuning_curve_2d(sliced_pos, sliced_spikes, xedges, yedges, gaussian_sigma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zones = find_zones(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(zones['u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields_tc = trajectory_fields(tuning_curves, zones, xedges, yedges, field_thresh=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(fields_tc['novel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuning_curves[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "xx, yy = np.meshgrid(xedges, yedges)\n",
    "for tuning_curve in fields_tc['novel']:\n",
    "    pp = plt.pcolormesh(xx, yy, tuning_curve, cmap='YlGn')\n",
    "    plt.colorbar(pp)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
