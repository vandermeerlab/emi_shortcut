{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T19:41:16.007264Z",
     "start_time": "2018-09-04T19:41:14.802575Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import nept\n",
    "\n",
    "from loading_data import get_data\n",
    "from analyze_tuning_curves import get_only_tuning_curves\n",
    "from utils_maze import get_bin_centers, get_zones, get_xy_idx, get_matched_trials\n",
    "from utils_plotting import plot_over_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T19:41:16.013773Z",
     "start_time": "2018-09-04T19:41:16.007264Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"trials\", \"decoding\")\n",
    "if not os.path.exists(output_filepath):\n",
    "    os.makedirs(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T19:41:16.056748Z",
     "start_time": "2018-09-04T19:41:16.015772Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import info.r063d3 as r063d3\n",
    "import info.r068d3 as r068d3\n",
    "# infos = [r063d3, r068d3]\n",
    "from run import analysis_infos\n",
    "infos = analysis_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T19:43:42.656190Z",
     "start_time": "2018-09-04T19:41:16.058748Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for info in infos:\n",
    "    print(info.session_id)\n",
    "    events, position, spikes, _, _ = get_data(info)\n",
    "        \n",
    "    phase = info.task_times[\"phase3\"]\n",
    "    sliced_position = position.time_slice(phase.start, phase.stop)\n",
    "\n",
    "    # trials = get_trials(events, phase)\n",
    "    trials = get_matched_trials(info, sliced_position, subset=False)\n",
    "#     trials = get_matched_trials(info, sliced_position, subset=True)\n",
    "\n",
    "    error_byactual_position = np.zeros((len(info.yedges), len(info.xedges)))\n",
    "    n_byactual_position = np.ones((len(info.yedges), len(info.xedges)))\n",
    "\n",
    "    session_n_active = []\n",
    "    session_likelihoods = []\n",
    "    session_decoded = []\n",
    "    session_actual = []\n",
    "    session_errors = []\n",
    "    n_timebins = []\n",
    "\n",
    "    for trial in trials:\n",
    "        starts = [start for start in trials.starts if start != trial.start]\n",
    "        stops = [stop for stop in trials.stops if stop != trial.stop]\n",
    "        epoch_of_interest = nept.Epoch([starts, stops])\n",
    "\n",
    "        tuning_curves = get_only_tuning_curves(info,\n",
    "                                               position,\n",
    "                                               spikes,\n",
    "                                               epoch_of_interest)\n",
    "        \n",
    "        tc_shape = tuning_curves.shape\n",
    "        decoding_tc = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "        \n",
    "        \n",
    "        trial_position = position.time_slice(trial.start, trial.stop)\n",
    "\n",
    "        sliced_spikes = [spiketrain.time_slice(trial.start,\n",
    "                                               trial.stop) for spiketrain in spikes]\n",
    "\n",
    "        # limit position to only times when the subject is moving faster than a certain threshold\n",
    "#         run_epoch = nept.run_threshold(trial_position, thresh=10., t_smooth=0.8)\n",
    "#         trial_position = trial_position[run_epoch]\n",
    "\n",
    "#         sliced_spikes = [spiketrain.time_slice(run_epoch.start,\n",
    "#                                                run_epoch.stop) for spiketrain in sliced_spikes]\n",
    "\n",
    "        # epochs_interest = nept.Epoch(np.array([trial_position.time[0], trial_position.time[-1]]))\n",
    "\n",
    "        t_window = 0.1  # 0.1 for running, 0.025 for swr\n",
    "\n",
    "        counts = nept.bin_spikes(sliced_spikes, trial_position.time, dt=t_window, window=t_window,\n",
    "                                 gaussian_std=0.0075, normalized=False)\n",
    "\n",
    "        n_timebins.append(len(counts.time))\n",
    "        min_neurons=3\n",
    "        \n",
    "        likelihood = nept.bayesian_prob(counts, decoding_tc, binsize=t_window, min_neurons=min_neurons, min_spikes=1)\n",
    "\n",
    "        # Find decoded location based on max likelihood for each valid timestep\n",
    "        xcenters, ycenters = get_bin_centers(info)\n",
    "        xy_centers = nept.cartesian(xcenters, ycenters)\n",
    "        decoded = nept.decode_location(likelihood, xy_centers, counts.time)\n",
    "\n",
    "        session_decoded.append(decoded)\n",
    "\n",
    "        # Remove nans from likelihood and reshape for plotting\n",
    "        keep_idx = np.sum(np.isnan(likelihood), axis=1) < likelihood.shape[1]\n",
    "        likelihood = likelihood[keep_idx]\n",
    "        likelihood = likelihood.reshape(np.shape(likelihood)[0], tc_shape[1], tc_shape[2])\n",
    "\n",
    "        session_likelihoods.append(likelihood)\n",
    "\n",
    "        n_active_neurons = np.asarray([n_active if n_active >= min_neurons else 0\n",
    "                                       for n_active in np.sum(counts.data >= 1, axis=1)])\n",
    "        n_active_neurons = n_active_neurons[keep_idx]\n",
    "        session_n_active.append(n_active_neurons)\n",
    "\n",
    "        f_xy = scipy.interpolate.interp1d(trial_position.time, trial_position.data.T, kind=\"nearest\")\n",
    "        counts_xy = f_xy(decoded.time)\n",
    "        true_position = nept.Position(np.hstack((counts_xy[0][..., np.newaxis],\n",
    "                                                 counts_xy[1][..., np.newaxis])),\n",
    "                                      decoded.time)\n",
    "\n",
    "        session_actual.append(true_position)\n",
    "\n",
    "        trial_errors = true_position.distance(decoded)\n",
    "        session_errors.append(trial_errors)\n",
    "        \n",
    "    title = info.session_id+\"_matched-trials_decoding-error\"\n",
    "    filepath = os.path.join(output_filepath, title+\".png\")\n",
    "    errors_byactual = plot_over_space(info, sliced_position, session_errors, \n",
    "                                      session_actual, title, vmax=80., filepath=filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
