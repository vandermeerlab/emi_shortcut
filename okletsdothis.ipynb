{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T16:33:47.014971Z",
     "start_time": "2018-08-07T16:33:45.864570Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from shapely.geometry import Point, LineString\n",
    "import nept\n",
    "\n",
    "from loading_data import get_data\n",
    "from utils_plotting import plot_errors, plot_over_space, make_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T16:33:47.020601Z",
     "start_time": "2018-08-07T16:33:47.014971Z"
    }
   },
   "outputs": [],
   "source": [
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"decode-checks\")\n",
    "if not os.path.exists(output_filepath):\n",
    "    os.makedirs(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T17:49:44.621465Z",
     "start_time": "2018-08-07T17:49:44.616467Z"
    }
   },
   "outputs": [],
   "source": [
    "import info.r063d2 as r063d2\n",
    "import info.r063d6 as r063d6\n",
    "# infos = [r063d2]\n",
    "\n",
    "from run import spike_sorted_infos, r063_infos\n",
    "infos = spike_sorted_infos\n",
    "# infos = r063_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T17:49:44.855388Z",
     "start_time": "2018-08-07T17:49:44.849391Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_decoded(info, pickle_filepath, binsize=12):\n",
    "    decoded_filename = info.session_id + '_decoded_binsize' + str(binsize) + 'cm.pkl'\n",
    "    pickled_decoded = os.path.join(pickle_filepath, decoded_filename)\n",
    "    \n",
    "    with open(pickled_decoded, 'rb') as fileobj:\n",
    "        decoded = pickle.load(fileobj)\n",
    "    \n",
    "    return decoded\n",
    "\n",
    "def load_decoded_shuffled(info, pickle_filepath, binsize=12):\n",
    "    shuffled_filename = info.session_id + '_decoded-shuffled_binsize' + str(binsize) + 'cm.pkl'\n",
    "    shuffled_decoded = os.path.join(pickle_filepath, shuffled_filename)\n",
    "    \n",
    "    with open(shuffled_decoded, 'rb') as fileobj:\n",
    "        shuffled = pickle.load(fileobj)\n",
    "        \n",
    "    return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T17:49:45.118469Z",
     "start_time": "2018-08-07T17:49:45.114471Z"
    }
   },
   "outputs": [],
   "source": [
    "binsize = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T17:50:04.620734Z",
     "start_time": "2018-08-07T17:50:04.616737Z"
    }
   },
   "outputs": [],
   "source": [
    "savefig = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T17:50:05.899067Z",
     "start_time": "2018-08-07T17:50:05.102017Z"
    }
   },
   "outputs": [],
   "source": [
    "# proportion decoded\n",
    "proportions_decoded = []\n",
    "session_ids = []\n",
    "for info in infos:\n",
    "    session_ids.append(info.session_id)\n",
    "    \n",
    "    decoded = load_decoded(info, pickle_filepath, binsize=binsize)\n",
    "\n",
    "    proportion_decoded = []\n",
    "    for trial, n_timebins in zip(decoded[\"decoded\"], decoded[\"n_timebins\"]):\n",
    "        proportion_decoded.append(trial.n_samples/n_timebins)\n",
    "    proportions_decoded.append(np.mean(proportion_decoded))\n",
    "\n",
    "filename = \"proportion-decoded_binsize\"+str(binsize)+\"cm.png\"\n",
    "filepath = os.path.join(output_filepath, \"proportion\")\n",
    "if not os.path.exists(filepath):\n",
    "    os.makedirs(filepath)\n",
    "\n",
    "y_pos = np.arange(len(session_ids))\n",
    "plt.bar(y_pos, proportions_decoded, align='center', alpha=0.7)\n",
    "plt.xticks(y_pos, session_ids, rotation=90, fontsize=10)\n",
    "plt.ylabel('Proportion')\n",
    "plt.title(\"Samples decoded with %d cm bins\" % binsize)\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig:\n",
    "    plt.savefig(os.path.join(filepath, filename))\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T16:37:02.868273Z",
     "start_time": "2018-08-07T16:37:02.865275Z"
    }
   },
   "outputs": [],
   "source": [
    "proportions_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T16:37:26.022391Z",
     "start_time": "2018-08-07T16:37:26.017394Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded[\"n_timebins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T16:29:48.302879Z",
     "start_time": "2018-08-07T16:29:48.297882Z"
    }
   },
   "outputs": [],
   "source": [
    "n_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T16:30:00.004076Z",
     "start_time": "2018-08-07T16:29:49.105966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Individual likelihoods/errors over space\n",
    "for info in infos:\n",
    "    xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "    \n",
    "    decoded = load_decoded(info, pickle_filepath, binsize=binsize)\n",
    "\n",
    "    filename = info.session_id+\"-likelihoods_byactual-\"+str(binsize)+\"cm.png\"\n",
    "    filepath = os.path.join(output_filepath, \"likelihoods\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    title = info.session_id+\" posterior\"\n",
    "    likelihood_byactual = plot_over_space(info, decoded[\"likelihoods\"], decoded[\"actual\"], title, os.path.join(filepath, filename))\n",
    "\n",
    "    filename = info.session_id+\"-errors_byactual-\"+str(binsize)+\"cm.png\"\n",
    "    filepath = os.path.join(output_filepath, \"errors\", \"over-space\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    title = info.session_id+\" error\"\n",
    "    errors_byactual = plot_over_space(info, decoded[\"errors\"], decoded[\"actual\"], title, os.path.join(filepath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T16:30:38.284974Z",
     "start_time": "2018-08-07T16:30:36.457275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combined errors\n",
    "n_sessions = 0\n",
    "combined_decoded_errors = []\n",
    "combined_shuffled_errors = []\n",
    "for info in infos:\n",
    "    n_sessions += 1\n",
    "    decoded = load_decoded(info, pickle_filepath, binsize=binsize)\n",
    "    combined_decoded_errors.append(decoded[\"errors\"])\n",
    "\n",
    "    shuffled = load_decoded_shuffled(info, pickle_filepath, binsize=binsize)\n",
    "    combined_shuffled_errors.append(shuffled[\"errors\"])\n",
    "\n",
    "filename = \"combined-errors_binsize\"+str(binsize)+\"cm.png\"\n",
    "filepath = os.path.join(output_filepath, \"errors\")\n",
    "if not os.path.exists(filepath):\n",
    "    os.makedirs(filepath)\n",
    "\n",
    "plot_errors(combined_decoded_errors, combined_shuffled_errors, n_sessions=n_sessions, \n",
    "            filename=os.path.join(filepath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T16:30:42.925193Z",
     "start_time": "2018-08-07T16:30:38.520635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Individual errors\n",
    "for info in infos:\n",
    "    decoded = load_decoded(info, pickle_filepath, binsize=binsize)\n",
    "\n",
    "    shuffled = load_decoded_shuffled(info, pickle_filepath, binsize=binsize)\n",
    "\n",
    "    filename = info.session_id+\"-errors_binsize\"+str(binsize)+\"cm.png\"\n",
    "    filepath = os.path.join(output_filepath, \"errors\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    plot_errors([decoded[\"errors\"]], [shuffled[\"errors\"]], n_sessions=1, \n",
    "                filename=os.path.join(filepath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-06T18:40:21.935Z"
    }
   },
   "outputs": [],
   "source": [
    "# animation\n",
    "for info in infos:\n",
    "    decoded = load_decoded(info, pickle_filepath, binsize=binsize)\n",
    "    \n",
    "    filepath = os.path.join(output_filepath, \"animations\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    \n",
    "    trial_idx = 2\n",
    "    make_animation(info, decoded, trial_idx, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T18:28:36.191220Z",
     "start_time": "2018-08-06T18:20:36.308986Z"
    }
   },
   "outputs": [],
   "source": [
    "binsizes = [2, 12, 30]\n",
    "# Individual mean/median errors\n",
    "for info in infos:\n",
    "    mean_errors = []\n",
    "    median_errors = []\n",
    "    \n",
    "    mean_errors_shuffled = []\n",
    "    median_errors_shuffled = []\n",
    "    \n",
    "    for bins in binsizes:\n",
    "        combine_errors = []\n",
    "        combine_errors_shuffled = []\n",
    "        \n",
    "        decoded = load_decoded(info, pickle_filepath, binsize=bins)\n",
    "\n",
    "        shuffled = load_decoded_shuffled(info, pickle_filepath, binsize=bins)\n",
    "            \n",
    "        for error in decoded[\"errors\"]:\n",
    "            combine_errors.extend(error)\n",
    "        mean_errors.append(np.mean(combine_errors))\n",
    "        median_errors.append(np.median(combine_errors))\n",
    "        \n",
    "        for error in shuffled[\"errors\"]:\n",
    "            combine_errors_shuffled.extend(error)\n",
    "        mean_errors_shuffled.append(np.mean(combine_errors_shuffled))\n",
    "        median_errors_shuffled.append(np.median(combine_errors_shuffled))\n",
    "        \n",
    "    filename = info.session_id+\"-mean-errors.png\"\n",
    "    filepath = os.path.join(output_filepath, \"errors\", \"average\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "        \n",
    "    plt.plot(binsizes, mean_errors)\n",
    "    plt.xlabel(\"Binsize (cm)\")\n",
    "    plt.ylabel(\"Mean error\")\n",
    "    plt.title(info.session_id+\" mean decode error (cm)\")\n",
    "    plt.savefig(os.path.join(filepath, filename))\n",
    "    plt.close()\n",
    "    \n",
    "    filename = info.session_id+\"-median-errors.png\"\n",
    "    filepath = os.path.join(output_filepath, \"errors\", \"average\")\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    \n",
    "    plt.plot(binsizes, median_errors)\n",
    "    plt.xlabel(\"Binsize (cm)\")\n",
    "    plt.ylabel(\"Median error\")\n",
    "    plt.title(info.session_id+\" median decode error (cm)\")\n",
    "    plt.savefig(os.path.join(filepath, filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T14:00:02.747676Z",
     "start_time": "2018-08-07T14:00:02.730706Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_line(start_pt, stop_pt, line, expand_by):\n",
    "    \"\"\"Expands shapely line into a zone.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_pt : shapely.Point\n",
    "    stop_pt : shapely.Point\n",
    "    line : shapely.LineString\n",
    "    expand_by : int or float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    zone : shapely.Polygon\n",
    "\n",
    "    \"\"\"\n",
    "    line_expanded = line.buffer(expand_by)\n",
    "    zone = start_pt.union(line_expanded).union(stop_pt)\n",
    "\n",
    "    return zone\n",
    "\n",
    "\n",
    "def find_zones(info, remove_feeder, expand_by):\n",
    "    \"\"\"Finds zones from ideal trajectories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info : shortcut module\n",
    "    remove_feeder: boolean\n",
    "    expand_by : int or float\n",
    "        Amount to expand the line.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    zone : dict\n",
    "        With shapely.Polygon as values.\n",
    "        Keys are u, shortcut, novel.\n",
    "\n",
    "    \"\"\"\n",
    "    u_line = LineString(info.u_trajectory)\n",
    "    shortcut_line = LineString(info.shortcut_trajectory)\n",
    "    novel_line = LineString(info.novel_trajectory)\n",
    "\n",
    "    feeder1 = Point(info.path_pts['feeder1'][0], info.path_pts['feeder1'][1]).buffer(expand_by*1.3)\n",
    "    feeder2 = Point(info.path_pts['feeder2'][0], info.path_pts['feeder2'][1]).buffer(expand_by*1.3)\n",
    "\n",
    "    u_zone = expand_line(Point(info.u_trajectory[0]), \n",
    "                         Point(info.u_trajectory[-1]), \n",
    "                         u_line, expand_by)\n",
    "    shortcut_zone = expand_line(Point(info.shortcut_trajectory[0]), \n",
    "                                Point(info.shortcut_trajectory[-1]), \n",
    "                                shortcut_line, expand_by)\n",
    "    novel_zone = expand_line(Point(info.novel_trajectory[0]), \n",
    "                             Point(info.novel_trajectory[-1]), \n",
    "                             novel_line, expand_by)\n",
    "\n",
    "    zone = dict()\n",
    "    zone['u'] = u_zone\n",
    "    zone['shortcut'] = shortcut_zone.difference(u_zone)\n",
    "    zone['shortcut'] = zone['shortcut'].difference(novel_zone)\n",
    "    zone['novel'] = novel_zone.difference(u_zone)\n",
    "\n",
    "    if remove_feeder:\n",
    "        for feeder in [feeder1, feeder2]:\n",
    "            zone['u'] = zone['u'].difference(feeder)\n",
    "            zone['shortcut'] = zone['shortcut'].difference(feeder)\n",
    "            zone['novel'] = zone['novel'].difference(feeder)\n",
    "\n",
    "    return zone\n",
    "\n",
    "\n",
    "def find_subset_zones(info, remove_feeder, expand_by):\n",
    "    \"\"\"Finds zones from ideal trajectories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    info : shortcut module\n",
    "    remove_feeder: boolean\n",
    "    expand_by : int or float\n",
    "        Amount to expand the line.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    zone : dict\n",
    "        With shapely.Polygon as values.\n",
    "        Keys are u, shortcut, novel.\n",
    "\n",
    "    \"\"\"\n",
    "    u_line = LineString(info.u_trajectory)\n",
    "    shortcut_line = LineString(info.shortcut_trajectory)\n",
    "    novel_line = LineString(info.novel_trajectory)\n",
    "    u_subset_line = LineString(info.u_segment)\n",
    "\n",
    "    feeder1 = Point(info.path_pts['feeder1'][0], info.path_pts['feeder1'][1]).buffer(expand_by*1.3)\n",
    "    feeder2 = Point(info.path_pts['feeder2'][0], info.path_pts['feeder2'][1]).buffer(expand_by*1.3)\n",
    "\n",
    "    u_zone = expand_line(Point(info.u_trajectory[0]), \n",
    "                         Point(info.u_trajectory[-1]), \n",
    "                         u_line, expand_by)\n",
    "    shortcut_zone = expand_line(Point(info.shortcut_trajectory[0]), \n",
    "                                Point(info.shortcut_trajectory[-1]), \n",
    "                                shortcut_line, expand_by)\n",
    "    novel_zone = expand_line(Point(info.novel_trajectory[0]), \n",
    "                             Point(info.novel_trajectory[-1]), \n",
    "                             novel_line, expand_by)\n",
    "    u_subset_zone = expand_line(Point(info.u_segment[0]),\n",
    "                                Point(info.u_segment[-1]),\n",
    "                                u_subset_line, expand_by)\n",
    "\n",
    "    zone = dict()\n",
    "    zone['u'] = u_subset_zone\n",
    "    zone['shortcut'] = shortcut_zone.difference(u_zone)\n",
    "    zone['shortcut'] = zone['shortcut'].difference(novel_zone)\n",
    "    zone['novel'] = novel_zone.difference(u_zone)\n",
    "\n",
    "    if remove_feeder:\n",
    "        for feeder in [feeder1, feeder2]:\n",
    "            zone['u'] = zone['u'].difference(feeder)\n",
    "            zone['shortcut'] = zone['shortcut'].difference(feeder)\n",
    "            zone['novel'] = zone['novel'].difference(feeder)\n",
    "\n",
    "    return zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T14:01:06.838555Z",
     "start_time": "2018-08-07T14:00:03.025678Z"
    }
   },
   "outputs": [],
   "source": [
    "for info in infos:\n",
    "    print(info.session_id)\n",
    "    \n",
    "    decoded = load_decoded(info, pickle_filepath, binsize=binsize)\n",
    "    \n",
    "    events, position, spikes, _, _ = get_data(info)\n",
    "\n",
    "    binned_maze_shape = (len(info.yedges)-1, len(info.xedges)-1)\n",
    "    xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "\n",
    "    zones = find_subset_zones(info, remove_feeder=True, expand_by=15)\n",
    "\n",
    "    xcenters = info.xedges[:-1] + (info.xedges[1:] - info.xedges[:-1]) / 2\n",
    "    ycenters = info.yedges[:-1] + (info.yedges[1:] - info.yedges[:-1]) / 2\n",
    "\n",
    "    u_zone = np.zeros(binned_maze_shape).astype(bool)\n",
    "    shortcut_zone = np.zeros(binned_maze_shape).astype(bool)\n",
    "    novel_zone = np.zeros(binned_maze_shape).astype(bool)\n",
    "\n",
    "    for i, x in enumerate(xcenters):\n",
    "        for j, y in enumerate(ycenters):\n",
    "            if zones[\"u\"].contains(Point(x,y)):\n",
    "                u_zone[j][i] = True\n",
    "            elif zones[\"shortcut\"].contains(Point(x,y)):\n",
    "                shortcut_zone[j][i] = True\n",
    "            elif zones[\"novel\"].contains(Point(x,y)):\n",
    "                novel_zone[j][i] = True\n",
    "\n",
    "    sliced_position = position.time_slice(info.task_times[\"phase3\"].start, info.task_times[\"phase3\"].stop)\n",
    "    occupancy = nept.get_occupancy(sliced_position, info.yedges, info.xedges)\n",
    "\n",
    "    phase1_position = position.time_slice(info.task_times[\"phase1\"].start, info.task_times[\"phase1\"].stop)\n",
    "    phase1_occupancy = nept.get_occupancy(phase1_position, info.yedges, info.xedges)\n",
    "    phase2_position = position.time_slice(info.task_times[\"phase2\"].start, info.task_times[\"phase2\"].stop)\n",
    "    phase2_occupancy = nept.get_occupancy(phase2_position, info.yedges, info.xedges)\n",
    "    u_pos = np.zeros(binned_maze_shape).astype(bool)\n",
    "    u_pos[occupancy > 0.] = True\n",
    "    u_pos[phase1_occupancy > 0.] = True\n",
    "    u_pos[phase2_occupancy > 0.] = True\n",
    "    u_area = np.zeros(binned_maze_shape).astype(bool)\n",
    "    u_area[u_pos & u_zone] = True\n",
    "\n",
    "    shortcut_pos = np.zeros(binned_maze_shape).astype(bool)\n",
    "    shortcut_pos[(occupancy > 0.) & (~u_area)] = True\n",
    "    shortcut_area = np.zeros(binned_maze_shape).astype(bool)\n",
    "    shortcut_area[shortcut_pos & shortcut_zone] = True\n",
    "\n",
    "    novel_pos = np.zeros(binned_maze_shape).astype(bool)\n",
    "    novel_pos[(occupancy > 0.) & (~u_area)] = True\n",
    "    novel_area = np.zeros(binned_maze_shape).astype(bool)\n",
    "    novel_area[novel_pos & novel_zone] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T14:01:06.846551Z",
     "start_time": "2018-08-07T14:01:06.840555Z"
    }
   },
   "outputs": [],
   "source": [
    "novel_area.shape, xcenters.shape, ycenters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T14:01:06.864541Z",
     "start_time": "2018-08-07T14:01:06.848550Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded[\"decoded\"][0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T14:01:06.880531Z",
     "start_time": "2018-08-07T14:01:06.866540Z"
    }
   },
   "outputs": [],
   "source": [
    "xcenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T14:01:06.892525Z",
     "start_time": "2018-08-07T14:01:06.882531Z"
    }
   },
   "outputs": [],
   "source": [
    "u_decoded = []\n",
    "shortcut_decoded = []\n",
    "novel_decoded = []\n",
    "u_actual = []\n",
    "shortcut_actual = []\n",
    "novel_actual = []\n",
    "\n",
    "for i in range(len(decoded[\"decoded\"][0])):\n",
    "    x_idx = nept.find_nearest_idx(decoded[\"decoded\"][0].x[i], xcenters)\n",
    "    y_idx = nept.find_nearest_idx(decoded[\"decoded\"][0].y[i], ycenters)\n",
    "\n",
    "    if novel_area[y_idx][x_idx]:\n",
    "        novel_decoded.append([decoded[\"decoded\"][0].x[i], decoded[\"decoded\"][0].y[i], decoded[\"decoded\"][0].time[i]])\n",
    "    elif shortcut_area[y_idx][x_idx]:\n",
    "    elif u_area[y_idx][x_idx]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T14:01:13.145209Z",
     "start_time": "2018-08-07T14:01:12.913342Z"
    }
   },
   "outputs": [],
   "source": [
    "combined = np.zeros(binned_maze_shape)\n",
    "combined[u_area] = 1\n",
    "combined[shortcut_area] = 2\n",
    "combined[novel_area] = 3\n",
    "\n",
    "plt.figure()\n",
    "pp = plt.pcolormesh(xx, yy, combined, cmap=\"PuBuGn\")\n",
    "plt.plot(position.x, position.y, \"k.\", ms=2)\n",
    "plt.plot(decoded[\"decoded\"][0].x[i], decoded[\"decoded\"][0].y[i], \"r.\", ms=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
