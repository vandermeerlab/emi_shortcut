{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import vdmlab as vdm\n",
    "\n",
    "from load_data import get_pos, get_spikes, get_lfp\n",
    "from analyze_plotting import plot_cooccur, plot_cooccur_combined, plot_cooccur_weighted_pauses\n",
    "\n",
    "import sys\n",
    "sys.path.append('E:\\\\code\\\\python-vdmlab\\\\projects\\\\emily_shortcut\\\\info')\n",
    "import info.r063d2 as r063d2\n",
    "import info.r063d3 as r063d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infos = [r063d2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_filepath = 'C:\\\\Users\\\\Emily\\\\Code\\\\emi_shortcut\\\\cache\\\\pickled\\\\'\n",
    "output_filepath = 'C:\\\\Users\\\\Emily\\\\Code\\\\emi_shortcut\\\\plots\\\\'\n",
    "# pickle_filepath = 'E:\\\\code\\\\emi_shortcut\\\\cache\\\\pickled\\\\'\n",
    "# output_filepath = 'E:\\\\code\\\\emi_shortcut\\\\plots\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_cooccur_weighted(cooccurs):\n",
    "    \"\"\"Combines probabilities from multiple sessions, weighted by number of sharp-wave ripple events.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_probs: list of dicts\n",
    "        With u, shortcut, novel as keys,\n",
    "        each a dict with expected, observed, active, shuffle, zscore as keys.\n",
    "    n_epochs: list of ints\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined_weighted: dict\n",
    "\n",
    "    \"\"\"\n",
    "    combined_weighted_mean = dict(u=dict(expected=[], observed=[], active=[], shuffle=[], zscore=[]),\n",
    "                             shortcut=dict(expected=[], observed=[], active=[], shuffle=[], zscore=[]),\n",
    "                             novel=dict(expected=[], observed=[], active=[], shuffle=[], zscore=[]))\n",
    "    combined_weighted_std = dict(u=dict(expected=[], observed=[], active=[], shuffle=[], zscore=[]),\n",
    "                             shortcut=dict(expected=[], observed=[], active=[], shuffle=[], zscore=[]),\n",
    "                             novel=dict(expected=[], observed=[], active=[], shuffle=[], zscore=[]))\n",
    "\n",
    "    for trajectory in combined_weighted_mean:\n",
    "        for key in combined_weighted_mean[trajectory]:\n",
    "            for probs, n_epoch in zip(cooccurs['probs'], cooccurs['n_epochs']):\n",
    "                if np.sum(probs[trajectory][key]) > 0:\n",
    "                    combined_weighted_mean[trajectory][key].append(np.nanmean(probs[trajectory][key]) * n_epoch)\n",
    "                    combined_weighted_std[trajectory][key].append(np.sqrt(sum(n_epoch * (probs[trajectory][key]-(np.nanmean(probs[trajectory][key]) * n_epoch))**2.) / ((len(probs[trajectory][key])-1) * n_epoch/len(probs[trajectory][key]))))\n",
    "                else:\n",
    "                    combined_weighted_mean[trajectory][key].append(0.0)\n",
    "                    combined_weighted_std[trajectory][key].append(0.0)\n",
    "\n",
    "    return combined_weighted\n",
    "\n",
    "\n",
    "def combine_cooccur(cooccurs):\n",
    "    \"\"\"Combines probabilities from multiple sessions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_probs: list of dicts\n",
    "        With u, shortcut, novel as keys,\n",
    "        each a dict with expected, observed, active, shuffle, zscore as keys.\n",
    "    n_epochs: list of ints\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined: list\n",
    "\n",
    "    \"\"\"\n",
    "    combined = dict(u=dict(expected=[], observed=[], active=[], shuffle=[], zscore=[]),\n",
    "                    shortcut=dict(expected=[], observed=[], active=[], shuffle=[], zscore=[]),\n",
    "                    novel=dict(expected=[], observed=[], active=[], shuffle=[], zscore=[]))\n",
    "\n",
    "    for trajectory in combined:\n",
    "        for key in combined[trajectory]:\n",
    "            for probs in cooccurs['probs']:\n",
    "                if np.sum(probs[trajectory][key]) > 0:\n",
    "                    combined[trajectory][key].extend(probs[trajectory][key])\n",
    "                else:\n",
    "                    combined[trajectory][key].append(0.0)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tracks_tc = False\n",
    "\n",
    "cooccurs_a = dict(probs=[], n_epochs=[])\n",
    "cooccurs_b = dict(probs=[], n_epochs=[])\n",
    "experiment_time = 'pauseA'\n",
    "print('getting co-occurrence', experiment_time)\n",
    "for info in infos:\n",
    "    if all_tracks_tc:\n",
    "        cooccur_filename = info.session_id + '_cooccur-' + experiment_time + '_all-tracks.pkl'\n",
    "    else:\n",
    "        cooccur_filename = info.session_id + '_cooccur-' + experiment_time + '.pkl'\n",
    "    pickled_cooccur = os.path.join(pickle_filepath, cooccur_filename)\n",
    "    with open(pickled_cooccur, 'rb') as fileobj:\n",
    "        cooccur = pickle.load(fileobj)\n",
    "\n",
    "    cooccurs_a['probs'].append(cooccur['probs'])\n",
    "    cooccurs_a['n_epochs'].append(cooccur['n_epochs'])\n",
    "\n",
    "combined_a = combine_cooccur(cooccurs_a)\n",
    "combined_weighted_a, combined_weighted_a_std = combine_cooccur_weighted(cooccurs_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_weighted_a['u']['expected'], combined_weighted_a_std['u']['expected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country = ['usa', 'china', 'lux']\n",
    "population = [309, 1350, 0.492]\n",
    "gdp = [46000, 3920, 107000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "together = []\n",
    "for i, c in enumerate(country):\n",
    "    together.append(population[i] * gdp[i])\n",
    "weighted = sum(together) / sum(population)\n",
    "weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqrt(sum(wi(xi-meanxw)**2) / (((n-1) * sum(wi)) / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epoch = np.array([1., 2., 3.])\n",
    "total_weight = np.sum(n_epoch)\n",
    "observation = np.array([0.1, 0.5, 1.0])\n",
    "weighted_mean = np.sum(observation * n_epoch) / total_weight\n",
    "n_samples = len(observation)\n",
    "\n",
    "weighted_std = np.sqrt(np.sum(n_epoch * (observation-weighted_mean)**2) / (((n_samples-1) * total_weight) / n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_error = np.sqrt(np.sum(n_epoch**2 * (observation-weighted_mean)**2 )) / total_weight\n",
    "weighted_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = [1.]\n",
    "err = [0.1]\n",
    "means2 = [1., 2., 3.]\n",
    "err2 = [0.1, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(4.5, 2))\n",
    "\n",
    "ind = np.arange(1)\n",
    "width = 0.5\n",
    "\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    condition1 = ax.bar(ind+width, means, width, color='r', yerr=err, ecolor='k')\n",
    "    condition2 = ax.bar(ind, means, width, color='y', yerr=err, ecolor='k')\n",
    "\n",
    "ax1.yaxis.set_ticks_position('left')\n",
    "\n",
    "for ax in [ax2, ax3]:\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='y', which='both', length=0)\n",
    "\n",
    "for ax, trajectory in zip([ax1, ax2, ax3], ['U', 'Shortcut', 'Novel']):\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.set_xlabel(trajectory)\n",
    "    ax.set_xticks([ind+0.5*width, ind+width+0.5*width])\n",
    "    ax.set_xticklabels(['PauseA', 'PauseB'])\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax1.set_ylabel('this is it')\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infos = [r063d3]\n",
    "\n",
    "field_thresh = 1.\n",
    "power_thresh = 5.\n",
    "z_thresh = 3.\n",
    "merge_thresh = 0.02\n",
    "min_length = 0.01\n",
    "\n",
    "for info in infos:\n",
    "    print(info.session_id)\n",
    "\n",
    "    lfp = get_lfp(info.good_swr[0])\n",
    "    position = get_pos(info.pos_mat, info.pxl_to_cm)\n",
    "    spikes = get_spikes(info.spike_mat)\n",
    "\n",
    "    speed = position.speed(t_smooth=0.5)\n",
    "    run_idx = np.squeeze(speed.data) >= 0.1\n",
    "    run_pos = position[run_idx]\n",
    "\n",
    "    t_start_tc = info.task_times['phase3'].start\n",
    "    t_stop_tc = info.task_times['phase3'].stop\n",
    "\n",
    "    tc_pos = run_pos.time_slice(t_start_tc, t_stop_tc)\n",
    "\n",
    "    tc_spikes = [spiketrain.time_slice(t_start_tc, t_stop_tc) for spiketrain in spikes]\n",
    "\n",
    "    binsize = 3\n",
    "    xedges = np.arange(tc_pos.x.min(), tc_pos.x.max() + binsize, binsize)\n",
    "    yedges = np.arange(tc_pos.y.min(), tc_pos.y.max() + binsize, binsize)\n",
    "\n",
    "    tuning_curves = vdm.tuning_curve_2d(tc_pos, tc_spikes, xedges, yedges, gaussian_sigma=0.1)\n",
    "\n",
    "    zones = find_zones(info)\n",
    "\n",
    "    fields_tunings = categorize_fields(tuning_curves, zones, xedges, yedges, field_thresh=field_thresh)\n",
    "\n",
    "    keys = ['u', 'shortcut', 'novel']\n",
    "    unique_fields = dict()\n",
    "    unique_fields['u'] = get_unique_fields(fields_tunings['u'],\n",
    "                                           fields_tunings['shortcut'],\n",
    "                                           fields_tunings['novel'])\n",
    "    unique_fields['shortcut'] = get_unique_fields(fields_tunings['shortcut'],\n",
    "                                                  fields_tunings['novel'],\n",
    "                                                  fields_tunings['u'])\n",
    "    unique_fields['novel'] = get_unique_fields(fields_tunings['novel'],\n",
    "                                               fields_tunings['u'],\n",
    "                                               fields_tunings['shortcut'])\n",
    "\n",
    "    field_spikes = dict(u=[], shortcut=[], novel=[])\n",
    "    for field in unique_fields.keys():\n",
    "        for key in unique_fields[field]:\n",
    "            field_spikes[field].append(spikes[key])\n",
    "\n",
    "    experiment_times = ['pauseA']\n",
    "    for experiment_time in experiment_times:\n",
    "        print(experiment_time)\n",
    "\n",
    "        t_start = info.task_times[experiment_time].start\n",
    "        t_stop = info.task_times[experiment_time].stop\n",
    "\n",
    "        sliced_lfp = lfp.time_slice(t_start, t_stop)\n",
    "\n",
    "        sliced_spikes = [spiketrain.time_slice(t_start, t_stop) for spiketrain in spikes]\n",
    "\n",
    "        swrs = vdm.detect_swr_hilbert(sliced_lfp, fs=info.fs, thresh=(140.0, 250.0), z_thresh=z_thresh,\n",
    "                                      power_thresh=power_thresh, merge_thresh=merge_thresh, min_length=min_length)\n",
    "        \n",
    "        multi_swrs = vdm.find_multi_in_epochs(spikes, swrs, min_involved=3)\n",
    "\n",
    "        count_matrix = dict()\n",
    "        for key in field_spikes:\n",
    "            count_matrix[key] = vdm.spike_counts(field_spikes[key], multi_swrs)\n",
    "\n",
    "        tetrode_mask = dict()\n",
    "        for key in field_spikes:\n",
    "            tetrode_mask[key] = vdm.get_tetrode_mask(field_spikes[key])\n",
    "\n",
    "        probs = dict()\n",
    "        for key in field_spikes:\n",
    "            probs[key] = vdm.compute_cooccur(count_matrix[key], tetrode_mask[key], num_shuffles=10000)\n",
    "\n",
    "        filename = 'testing_cooccur-' + experiment_time + '.png'\n",
    "        savepath = os.path.join(output_filepath, filename)\n",
    "        plot_cooccur(probs, savepath=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(np.array([np.array([1., 2., 1.]), np.array([2.])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = []\n",
    "a = np.array([np.array([1.]), np.array([2.])])\n",
    "for val in a:\n",
    "    t.extend(val)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
