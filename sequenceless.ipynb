{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T18:58:54.523338Z",
     "start_time": "2019-02-11T18:58:47.954739Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import copy\n",
    "import warnings\n",
    "import random\n",
    "import scipy\n",
    "import pickle\n",
    "import os\n",
    "import nept\n",
    "import scalebar\n",
    "\n",
    "from loading_data import get_data\n",
    "from analyze_tuning_curves import get_only_tuning_curves\n",
    "from utils_maze import get_zones\n",
    "from utils_maze import get_bin_centers\n",
    "\n",
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"trials\", \"decoding\", \"classy\")\n",
    "if not os.path.exists(output_filepath):\n",
    "    os.makedirs(output_filepath)\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T18:58:54.582269Z",
     "start_time": "2019-02-11T18:58:54.525300Z"
    }
   },
   "outputs": [],
   "source": [
    "class Session:\n",
    "    \"\"\"A collection of LikelihoodsAtTaskTime for each session\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        task_times : dict of TaskTime\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position, task_labels, zones):\n",
    "        self.position = position\n",
    "        self.task_labels = task_labels\n",
    "        for task_label in task_labels:\n",
    "            setattr(self, task_label, TaskTime([], [], [], zones))\n",
    "\n",
    "    def pickle(self, save_path):\n",
    "        with open(save_path, 'wb') as fileobj:\n",
    "            print(\"Saving \" + save_path)\n",
    "            pickle.dump(self, fileobj)\n",
    "\n",
    "    def n_tasktimes(self):\n",
    "        return len(self.task_labels)\n",
    "\n",
    "\n",
    "class TaskTime:\n",
    "    \"\"\"A set of decoded likelihoods for a given task time\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        likelihoods : np.array\n",
    "            With shape (ntimebins, nxbins, nybins)\n",
    "        zones : dict of Zones\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        likelihoods : np.array\n",
    "            With shape (ntimebins, nxbins, nybins)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tuning_curves, swrs, likelihoods, zones):\n",
    "        self.tuning_curves = tuning_curves\n",
    "        self.swrs = swrs\n",
    "        self.likelihoods = likelihoods\n",
    "        self.zones = zones\n",
    "\n",
    "    def sums(self, zone_label):\n",
    "        return np.nansum(self.likelihoods[:, :, self.zones[zone_label]], axis=2)\n",
    "\n",
    "    def means(self, zone_label):\n",
    "        return np.nanmean(self.likelihoods[:, :, self.zones[zone_label]], axis=2)\n",
    "\n",
    "    def maxes(self, zone_label):\n",
    "        return np.nanmax(self.likelihoods[:, :, self.zones[zone_label]], axis=2)\n",
    "\n",
    "\n",
    "def get_likelihoods(info, swr_params, task_labels, n_shuffles=0, save_path=None):\n",
    "    _, position, spikes, lfp, _ = get_data(info)\n",
    "\n",
    "    zones = dict()\n",
    "    zones[\"u\"], zones[\"shortcut\"], zones[\"novel\"] = get_zones(info, position, subset=True)\n",
    "    combined_zones = zones[\"u\"] + zones[\"shortcut\"] + zones[\"novel\"]\n",
    "    zones[\"other\"] = ~combined_zones\n",
    "\n",
    "    if n_shuffles > 0:\n",
    "        n_passes = n_shuffles\n",
    "    else:\n",
    "        n_passes = 1\n",
    "\n",
    "    session = Session(position, task_labels, zones)\n",
    "\n",
    "    tuning_curves_fromdata = get_only_tuning_curves(info, position, spikes, info.task_times[\"phase3\"])\n",
    "\n",
    "    tc_shape = tuning_curves_fromdata.shape\n",
    "\n",
    "    phase_for_zthresh = \"pauseB\"\n",
    "\n",
    "    swrs = nept.detect_swr_hilbert(lfp,\n",
    "                                   fs=info.fs,\n",
    "                                   thresh=swr_params[\"swr_thresh\"],\n",
    "                                   z_thresh=info.lfp_z_thresh,\n",
    "                                   merge_thresh=swr_params[\"merge_thresh\"],\n",
    "                                   min_length=swr_params[\"min_length\"],\n",
    "                                   times_for_z=nept.Epoch(info.task_times[phase_for_zthresh].start,\n",
    "                                                          info.task_times[phase_for_zthresh].stop))\n",
    "\n",
    "    swrs = nept.find_multi_in_epochs(spikes, swrs, min_involved=swr_params[\"min_involved\"])\n",
    "\n",
    "    rest_epochs = nept.rest_threshold(position, thresh=12., t_smooth=0.8)\n",
    "\n",
    "    for task_label in task_labels:\n",
    "        epochs_of_interest = info.task_times[task_label].intersect(rest_epochs)\n",
    "\n",
    "        phase_swrs = epochs_of_interest.overlaps(swrs)\n",
    "        phase_swrs = phase_swrs[phase_swrs.durations >= 0.05]\n",
    "\n",
    "        phase_likelihoods = np.zeros((n_passes, phase_swrs.n_epochs, tc_shape[1], tc_shape[2]))\n",
    "        phase_tuningcurves = np.zeros((n_passes, tc_shape[0], tc_shape[1], tc_shape[2]))\n",
    "        for n_pass in range(n_passes):\n",
    "\n",
    "            if n_shuffles > 0:\n",
    "                tuning_curves = np.random.permutation(tuning_curves_fromdata)\n",
    "            else:\n",
    "                tuning_curves = tuning_curves_fromdata\n",
    "\n",
    "            phase_tuningcurves[n_pass, ] = tuning_curves\n",
    "            tuning_curves = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "            if phase_swrs.n_epochs == 0:\n",
    "                phase_likelihoods = np.ones((n_passes, 1, tc_shape[1], tc_shape[2])) * np.nan\n",
    "            else:\n",
    "                counts_data = []\n",
    "                counts_time = []\n",
    "                t_windows = []\n",
    "\n",
    "                for n_timebin, (start, stop) in enumerate(zip(phase_swrs.starts,\n",
    "                                                              phase_swrs.stops)):\n",
    "                    t_window = stop - start  # 0.1 for running, 0.025 for swr\n",
    "\n",
    "                    sliced_spikes = [spiketrain.time_slice(start, stop) for spiketrain in spikes]\n",
    "\n",
    "                    these_counts = nept.bin_spikes(sliced_spikes,\n",
    "                                                   start,\n",
    "                                                   stop,\n",
    "                                                   dt=t_window,\n",
    "                                                   gaussian_std=0.0075,\n",
    "                                                   normalized=False,\n",
    "                                                   lastbin=True)\n",
    "\n",
    "                    counts_data.append(these_counts.data)\n",
    "                    counts_time.append(these_counts.time)\n",
    "                    t_windows.append(t_window)\n",
    "\n",
    "                counts = nept.AnalogSignal(np.vstack(counts_data), np.hstack(counts_time))\n",
    "                likelihood = nept.bayesian_prob(counts,\n",
    "                                                tuning_curves,\n",
    "                                                binsize=t_windows,\n",
    "                                                min_neurons=3,\n",
    "                                                min_spikes=1)\n",
    "\n",
    "                phase_likelihoods[n_pass] = likelihood.reshape(phase_swrs.n_epochs, tc_shape[1], tc_shape[2])\n",
    "\n",
    "        tasktime = getattr(session, task_label)\n",
    "        tasktime.likelihoods = phase_likelihoods\n",
    "        tasktime.tuning_curves = phase_tuningcurves\n",
    "        tasktime.swrs = phase_swrs\n",
    "\n",
    "    if save_path is not None:\n",
    "        session.pickle(save_path)\n",
    "\n",
    "    return session\n",
    "\n",
    "\n",
    "def limit_by_n_swr(session, task_labels, n_swr_thresh, zone_label=\"u\"):\n",
    "    session_copy = copy.deepcopy(session)\n",
    "    \n",
    "    for task_label in task_labels:\n",
    "        if getattr(session_copy, task_label).swrs.n_epochs < n_swr_thresh:\n",
    "            zone_shape = getattr(session_copy, task_label).zones[zone_label].shape\n",
    "            getattr(session_copy, task_label).likelihoods = np.ones((1, 1, zone_shape[0], zone_shape[1])) * np.nan\n",
    "    \n",
    "    return session_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T18:58:54.600260Z",
     "start_time": "2019-02-11T18:58:54.584267Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_session(sessions, title, task_labels, zone_labels, colours, filepath=None):\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs1 = gridspec.GridSpec(1, 4)\n",
    "    gs1.update(wspace=0.3, hspace=0.)\n",
    "\n",
    "    for i, zone_label in enumerate(zone_labels):\n",
    "        sums = {task_label: [] for task_label in task_labels}\n",
    "        n_swrs = {task_label: 0 for task_label in task_labels}\n",
    "        for session in sessions:\n",
    "            for task_label in task_labels:\n",
    "                zone_sums = getattr(session, task_label).sums(zone_label)\n",
    "                if zone_sums.size == 1:\n",
    "                    sums[task_label].extend([np.nan])\n",
    "                else:\n",
    "                    sums[task_label].extend(zone_sums)\n",
    "                    n_swrs[task_label] += getattr(session, task_label).swrs.n_epochs\n",
    "\n",
    "        for task_label in task_labels:\n",
    "            sums[task_label] = np.hstack(sums[task_label])\n",
    "\n",
    "        means = [np.nanmean(sums[task_label])\n",
    "                 if n_swrs[task_label] != 0 else 0.0\n",
    "                 for task_label in task_labels]\n",
    "\n",
    "        sems = [np.nanmean(scipy.stats.sem(sums[task_label], nan_policy=\"omit\"))\n",
    "                if n_swrs[task_label] != 0 else 0.0\n",
    "                for task_label in task_labels]\n",
    "\n",
    "        ax = plt.subplot(gs1[i])\n",
    "        ax.bar(np.arange(sessions[0].n_tasktimes()),\n",
    "               means, yerr=sems, color=colours[zone_label])\n",
    "\n",
    "        ax.set_ylim([0, 1.])\n",
    "\n",
    "        ax.set_xticks(np.arange(sessions[0].n_tasktimes()))\n",
    "        ax.set_xticklabels(task_labels, rotation=90)\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "        if i > 0:\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Proportion\")\n",
    "\n",
    "        if zone_label == \"other\":\n",
    "            for n_tasktimes, task_label in enumerate(task_labels):\n",
    "                ax.text(n_tasktimes, 0.01, str(n_swrs[task_label]), ha=\"center\", fontsize=14)\n",
    "\n",
    "    plt.text(1., 1., \"n sessions: \"+ str(len(sessions)), horizontalalignment='left',\n",
    "             verticalalignment='top', fontsize=14)\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    legend_elements = [Patch(facecolor=colours[zone_label], edgecolor='k', label=zone_label)\n",
    "                       for zone_label in zone_labels]\n",
    "\n",
    "    plt.legend(handles=legend_elements, bbox_to_anchor=(1., 0.95))\n",
    "\n",
    "    gs1.tight_layout(fig)\n",
    "\n",
    "    if filepath is not None:\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T18:58:54.782170Z",
     "start_time": "2019-02-11T18:58:54.602257Z"
    }
   },
   "outputs": [],
   "source": [
    "import info.r063d2 as r063d2\n",
    "import info.r063d3 as r063d3\n",
    "from run import analysis_infos\n",
    "\n",
    "# infos = [r063d2, r063d3]\n",
    "infos = analysis_infos\n",
    "\n",
    "# swr params\n",
    "swr_params = dict()\n",
    "swr_params[\"merge_thresh\"] = 0.02\n",
    "swr_params[\"min_length\"] = 0.05\n",
    "swr_params[\"swr_thresh\"] = (140.0, 250.0)\n",
    "swr_params[\"min_involved\"] = 4\n",
    "\n",
    "colours = dict()\n",
    "colours[\"u\"] = \"#2b8cbe\"\n",
    "colours[\"shortcut\"] = \"#31a354\"\n",
    "colours[\"novel\"] = \"#d95f0e\"\n",
    "colours[\"other\"] = \"#bdbdbd\"\n",
    "\n",
    "task_labels = [\"prerecord\", \"pauseA\", \"pauseB\", \"postrecord\"]\n",
    "zone_labels = [\"u\", \"shortcut\", \"novel\", \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T18:58:55.005026Z",
     "start_time": "2019-02-11T18:58:54.783153Z"
    }
   },
   "outputs": [],
   "source": [
    "update_cache = False\n",
    "dont_save_pickle = False\n",
    "\n",
    "true_sessions = []\n",
    "\n",
    "for info in infos:\n",
    "    print(info.session_id)\n",
    "\n",
    "    # Get true data\n",
    "    true_path = os.path.join(pickle_filepath, info.session_id + \"_likelihoods_true.pkl\")\n",
    "    \n",
    "    # Remove previous pickle if update_cache\n",
    "    if update_cache:\n",
    "        if os.path.exists(true_path):\n",
    "            os.remove(true_path)\n",
    "\n",
    "    # Load pickle if it exists, otherwise compute and pickle\n",
    "    if os.path.exists(true_path):\n",
    "        print(\"Loading pickled true likelihoods...\")\n",
    "        compute_likelihoods = False\n",
    "        with open(true_path, 'rb') as fileobj:\n",
    "            true_session = pickle.load(fileobj)\n",
    "    else:\n",
    "        if dont_save_pickle:\n",
    "            true_path = None\n",
    "        true_session = get_likelihoods(info,\n",
    "                                       swr_params,\n",
    "                                       task_labels,\n",
    "                                       n_shuffles=0,\n",
    "                                       save_path=true_path)\n",
    "\n",
    "    true_sessions.append(true_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T19:01:38.924317Z",
     "start_time": "2019-02-11T19:01:35.852495Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_swr_threshs = [5, 10, 20, 30, 40, 50]\n",
    "\n",
    "for n_swr_thresh in n_swr_threshs:\n",
    "    print(\"n_swr thresh:\", n_swr_thresh)\n",
    "    sessions_copy = []\n",
    "    for session in true_sessions:\n",
    "        session_copy = limit_by_n_swr(session, task_labels, n_swr_thresh=n_swr_thresh)\n",
    "        sessions_copy.append(session_copy)\n",
    "\n",
    "    title = \"All\" + \"_average-posterior-during-SWRs_true\"\n",
    "    plot_session(sessions_copy, title, task_labels, zone_labels, colours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
