{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:00:53.188509Z",
     "start_time": "2018-08-28T17:00:52.062785Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import nept\n",
    "\n",
    "from loading_data import get_data\n",
    "from analyze_tuning_curves import get_only_tuning_curves\n",
    "from utils_plotting import plot_over_space\n",
    "from utils_maze import get_subset_zones, get_bin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:00:53.195506Z",
     "start_time": "2018-08-28T17:00:53.190509Z"
    }
   },
   "outputs": [],
   "source": [
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"decode-checks\")\n",
    "if not os.path.exists(output_filepath):\n",
    "    os.makedirs(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:15:47.019864Z",
     "start_time": "2018-08-28T17:15:47.014866Z"
    }
   },
   "outputs": [],
   "source": [
    "import info.r066d6 as r066d6\n",
    "import info.r066d5 as r066d5\n",
    "infos = [r066d5]\n",
    "from run import analysis_infos\n",
    "# infos = analysis_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:15:49.776416Z",
     "start_time": "2018-08-28T17:15:49.768442Z"
    }
   },
   "outputs": [],
   "source": [
    "def bin_spikes(spikes, time, dt, window=None, gaussian_std=None, normalized=True):\n",
    "    \"\"\"Bins spikes using a sliding window.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spikes: list\n",
    "        Of nept.SpikeTrain\n",
    "    time: np.array\n",
    "    window: float or None\n",
    "        Length of the sliding window, in seconds. If None, will default to dt.\n",
    "    dt: float\n",
    "    gaussian_std: float or None\n",
    "    normalized: boolean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    binned_spikes: nept.AnalogSignal\n",
    "\n",
    "    \"\"\"\n",
    "    if window is None:\n",
    "        window = dt\n",
    "\n",
    "    bin_edges = time\n",
    "\n",
    "    given_n_bins = window / dt\n",
    "    n_bins = int(round(given_n_bins))\n",
    "    if abs(n_bins - given_n_bins) > 0.01:\n",
    "        warnings.warn(\"dt does not divide window evenly. \"\n",
    "                      \"Using window %g instead.\" % (n_bins*dt))\n",
    "\n",
    "    if normalized:\n",
    "        square_filter = np.ones(n_bins) * (1 / n_bins)\n",
    "    else:\n",
    "        square_filter = np.ones(n_bins)\n",
    "\n",
    "    counts = np.zeros((len(spikes), len(bin_edges) - 1))\n",
    "    for idx, spiketrain in enumerate(spikes):\n",
    "        counts[idx] = np.convolve(np.histogram(spiketrain.time, bins=bin_edges)[0].astype(float),\n",
    "                                  square_filter, mode=\"same\")\n",
    "\n",
    "    if gaussian_std is not None:\n",
    "        counts = nept.gaussian_filter(counts, gaussian_std, dt=dt, normalized=normalized, axis=1)\n",
    "\n",
    "    return nept.AnalogSignal(counts, bin_edges[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:15:50.774914Z",
     "start_time": "2018-08-28T17:15:50.752945Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Average decoded likelihood for each trajectory\n",
    "def get_average_likelihoods(info):\n",
    "    events, position, spikes, lfp, _ = get_data(info)\n",
    "\n",
    "    u_zone, shortcut_zone, novel_zone = get_subset_zones(info, position)\n",
    "    combined_zones = u_zone+shortcut_zone+novel_zone\n",
    "    other_zone = ~combined_zones\n",
    "\n",
    "    tuning_curves = get_only_tuning_curves(info,\n",
    "                                           position,\n",
    "                                           spikes,\n",
    "                                           info.task_times[\"phase3\"])\n",
    "    tc_shape = tuning_curves.shape\n",
    "    decoding_tc = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "    # Find SWRs for the whole session\n",
    "    z_thresh = 2.0\n",
    "    power_thresh = 3.0\n",
    "    merge_thresh = 0.02\n",
    "    min_length = 0.05\n",
    "    swrs = nept.detect_swr_hilbert(lfp, fs=info.fs, thresh=(140.0, 250.0), z_thresh=z_thresh,\n",
    "                                   power_thresh=power_thresh, merge_thresh=merge_thresh, min_length=min_length)\n",
    "    swrs = nept.find_multi_in_epochs(spikes, swrs, min_involved=4)\n",
    "\n",
    "    rest_epochs = nept.rest_threshold(position, thresh=12., t_smooth=0.8)\n",
    "\n",
    "    # Restrict SWRs to those during epochs of interest during rest\n",
    "    task_times = [\"prerecord\", \"pauseA\", \"pauseB\", \"postrecord\"]\n",
    "    maze_segments = [\"u\", \"shortcut\", \"novel\", \"other\"]\n",
    "    data = {k: {key: [] for key in maze_segments} for k in task_times}\n",
    "\n",
    "    n_swrs = dict()\n",
    "\n",
    "    for task_time in task_times:\n",
    "        epochs_of_interest = info.task_times[task_time].intersect(rest_epochs)\n",
    "\n",
    "        phase_swrs = epochs_of_interest.overlaps(swrs)\n",
    "        phase_swrs = phase_swrs[phase_swrs.durations >= 0.05]\n",
    "\n",
    "        n_swrs[task_time] = phase_swrs.n_epochs\n",
    "\n",
    "        likelihoods = []\n",
    "\n",
    "        for start, stop in zip(phase_swrs.starts, phase_swrs.stops):\n",
    "            sliced_spikes = [spiketrain.time_slice(start, stop) for spiketrain in spikes]\n",
    "\n",
    "            t_window = stop-start # 0.1 for running, 0.025 for swr\n",
    "\n",
    "            counts = bin_spikes(sliced_spikes, np.array([start, stop]), dt=t_window, window=t_window,\n",
    "                                     gaussian_std=0.0075, normalized=False)\n",
    "\n",
    "            likelihood = nept.bayesian_prob(counts, decoding_tc, binsize=t_window, min_neurons=3, min_spikes=1)\n",
    "\n",
    "            # Remove nans from likelihood and reshape for plotting\n",
    "            keep_idx = np.sum(np.isnan(likelihood), axis=1) < likelihood.shape[1]\n",
    "            likelihood = likelihood[keep_idx]\n",
    "            likelihoods.append(likelihood.reshape(np.shape(likelihood)[0], tc_shape[1], tc_shape[2]))\n",
    "\n",
    "        for swr_likelihood in likelihoods:    \n",
    "            data[task_time][\"u\"].append(np.nansum(swr_likelihood[0][u_zone]))\n",
    "            data[task_time][\"shortcut\"].append(np.nansum(swr_likelihood[0][shortcut_zone]))\n",
    "            data[task_time][\"novel\"].append(np.nansum(swr_likelihood[0][novel_zone]))\n",
    "            data[task_time][\"other\"].append(np.nansum(swr_likelihood[0][other_zone]))\n",
    "        \n",
    "    return data, n_swrs, likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:15:51.814669Z",
     "start_time": "2018-08-28T17:15:51.799696Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_decoded_summary(all_data, n_all_swrs, task_times, maze_segments, n_sessions):\n",
    "\n",
    "    for task_time in task_times:\n",
    "        us = []\n",
    "        shortcuts = []\n",
    "        novels = []\n",
    "        others = []\n",
    "\n",
    "        n_swrs = 0\n",
    "\n",
    "        for i, session in enumerate(all_data):\n",
    "            us.extend(session[task_time][\"u\"])\n",
    "            shortcuts.extend(session[task_time][\"shortcut\"])\n",
    "            novels.extend(session[task_time][\"novel\"])\n",
    "            others.extend(session[task_time][\"other\"])\n",
    "\n",
    "            n_swrs += n_all_swrs[i][task_time]\n",
    "\n",
    "        means = [np.nanmean(us), np.nanmean(shortcuts), np.nanmean(novels), np.nanmean(others)]\n",
    "        sems = [scipy.stats.sem(us), scipy.stats.sem(shortcuts), scipy.stats.sem(novels), scipy.stats.sem(others)]\n",
    "\n",
    "        xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7,5))\n",
    "        n = np.arange(len(maze_segments))\n",
    "        plt.bar(n, means, yerr=sems, color=\"#a6bddb\")\n",
    "        plt.xticks(n, maze_segments)\n",
    "        plt.text(0.95, 0.95, \"n swrs: \"+str(n_swrs),\n",
    "             horizontalalignment='center',\n",
    "             verticalalignment='center',\n",
    "             transform = ax.transAxes,\n",
    "             fontsize=14)\n",
    "        if n_sessions == 1:\n",
    "            title = info.session_id + \" average posteriors during SWRs in \" + task_time\n",
    "        else:\n",
    "            title = \"Average posteriors during SWRs in \" + task_time\n",
    "        plt.title(title)\n",
    "        plt.ylabel(\"Proportion\")\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        plt.ylim(0, 0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(os.path.join(output_filepath, title+\".png\"))\n",
    "        plt.close()\n",
    "\n",
    "    #     plt.show()\n",
    "\n",
    "\n",
    "    for trajectory in maze_segments:\n",
    "        trajectory_means = []\n",
    "        trajectory_sems = []\n",
    "\n",
    "        tt = {key: [] for key in task_times}\n",
    "        n_swrs = {key: 0 for key in task_times}\n",
    "\n",
    "        for i, session in enumerate(all_data):\n",
    "            for task_time in task_times:\n",
    "                tt[task_time].extend(session[task_time][trajectory])\n",
    "\n",
    "                n_swrs[task_time] += n_all_swrs[i][task_time]\n",
    "        trajectory_means = [np.nanmean(tt[\"prerecord\"]), np.nanmean(tt[\"pauseA\"]), np.nanmean(tt[\"pauseB\"]), np.nanmean(tt[\"postrecord\"])]\n",
    "        trajectory_sems = [scipy.stats.sem(tt[\"prerecord\"]), scipy.stats.sem(tt[\"pauseA\"]), scipy.stats.sem(tt[\"pauseB\"]), scipy.stats.sem(tt[\"postrecord\"])]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7,5))\n",
    "        n = np.arange(len(task_times))\n",
    "        plt.bar(n, trajectory_means, yerr=trajectory_sems, color=\"#3690c0\")\n",
    "        plt.xticks(n, task_times)\n",
    "        if n_sessions == 1:\n",
    "             title = info.session_id + \" average posteriors during SWRs for \" + trajectory\n",
    "        else:\n",
    "            title = \"Average posteriors during SWRs for \" + trajectory\n",
    "        plt.title(title)\n",
    "        plt.ylabel(\"Proportion\")\n",
    "\n",
    "        for i, task_time in enumerate(task_times):\n",
    "            ax.text(i, 0.01, str(n_swrs[task_time]), ha=\"center\", fontsize=14)\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(os.path.join(output_filepath, title+\".png\"))\n",
    "        plt.close()\n",
    "\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:15:53.146877Z",
     "start_time": "2018-08-28T17:15:53.142880Z"
    }
   },
   "outputs": [],
   "source": [
    "task_times = [\"prerecord\", \"pauseA\", \"pauseB\", \"postrecord\"]\n",
    "maze_segments = [\"u\", \"shortcut\", \"novel\", \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:16:14.300248Z",
     "start_time": "2018-08-28T17:15:56.599267Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot individual sessions\n",
    "for info in infos:\n",
    "    all_data = []\n",
    "    n_all_swrs = []\n",
    "    \n",
    "    data, n_swrs, likelihoods = get_average_likelihoods(info)\n",
    "\n",
    "    all_data.append(data)\n",
    "    n_all_swrs.append(n_swrs)\n",
    "    \n",
    "    plot_decoded_summary(all_data, n_all_swrs, task_times, maze_segments, n_sessions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:14:49.446420Z",
     "start_time": "2018-08-28T17:08:00.650507Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot combined sessions\n",
    "all_data = []\n",
    "n_all_swrs = []\n",
    "\n",
    "for info in infos:\n",
    "    data, n_swrs, likelihoods = get_average_likelihoods(info)\n",
    "\n",
    "all_data.append(data)\n",
    "n_all_swrs.append(n_swrs)\n",
    "\n",
    "plot_decoded_summary(all_data, n_all_swrs, task_times, maze_segments, n_sessions=len(infos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:16:14.555560Z",
     "start_time": "2018-08-28T17:16:14.301705Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:26:38.011489Z",
     "start_time": "2018-08-28T17:26:37.272929Z"
    }
   },
   "outputs": [],
   "source": [
    "events, position, spikes, lfp, _ = get_data(info)\n",
    "plot_over_space(info, likelihoods, position, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:14:49.446420Z",
     "start_time": "2018-08-28T17:00:52.114Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assign swr to a trajectory\n",
    "def get_max_likelihoods(info):\n",
    "    events, position, spikes, lfp, _ = get_data(info)\n",
    "\n",
    "    u_zone, shortcut_zone, novel_zone = get_subset_zones(info, position)\n",
    "    combined_zones = u_zone+shortcut_zone+novel_zone\n",
    "    other_zone = ~combined_zones\n",
    "\n",
    "    tuning_curves = get_only_tuning_curves(info,\n",
    "                                           position,\n",
    "                                           spikes,\n",
    "                                           info.task_times[\"phase3\"])\n",
    "    tc_shape = tuning_curves.shape\n",
    "    decoding_tc = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "    # Find SWRs for the whole session\n",
    "    z_thresh = 2.0\n",
    "    power_thresh = 3.0\n",
    "    merge_thresh = 0.02\n",
    "    min_length = 0.05\n",
    "    swrs = nept.detect_swr_hilbert(lfp, fs=info.fs, thresh=(140.0, 250.0), z_thresh=z_thresh,\n",
    "                                   power_thresh=power_thresh, merge_thresh=merge_thresh, min_length=min_length)\n",
    "    swrs = nept.find_multi_in_epochs(spikes, swrs, min_involved=4)\n",
    "\n",
    "    rest_epochs = nept.rest_threshold(position, thresh=12., t_smooth=0.8)\n",
    "\n",
    "    # Restrict SWRs to those during epochs of interest during rest\n",
    "    task_times = [\"prerecord\", \"pauseA\", \"pauseB\", \"postrecord\"]\n",
    "    maze_segments = [\"u\", \"shortcut\", \"novel\", \"other\"]\n",
    "    data = {k: {key: 0 for key in maze_segments} for k in task_times}\n",
    "\n",
    "    n_swrs = dict()\n",
    "\n",
    "    for task_time in task_times:\n",
    "        epochs_of_interest = info.task_times[task_time].intersect(rest_epochs)\n",
    "\n",
    "        phase_swrs = epochs_of_interest.overlaps(swrs)\n",
    "        phase_swrs = phase_swrs[phase_swrs.durations >= 0.05]\n",
    "\n",
    "        n_swrs[task_time] = phase_swrs.n_epochs\n",
    "\n",
    "        likelihoods = []\n",
    "\n",
    "        for start, stop in zip(phase_swrs.starts, phase_swrs.stops):\n",
    "            sliced_spikes = [spiketrain.time_slice(start, stop) for spiketrain in spikes]\n",
    "\n",
    "            t_window = stop-start # 0.1 for running, 0.025 for swr\n",
    "\n",
    "            counts = bin_spikes(sliced_spikes, np.array([start, stop]), dt=t_window, window=t_window,\n",
    "                                     gaussian_std=0.0075, normalized=False)\n",
    "\n",
    "            likelihood = nept.bayesian_prob(counts, decoding_tc, binsize=t_window, min_neurons=3, min_spikes=1)\n",
    "\n",
    "            # Remove nans from likelihood and reshape for plotting\n",
    "            keep_idx = np.sum(np.isnan(likelihood), axis=1) < likelihood.shape[1]\n",
    "            likelihood = likelihood[keep_idx]\n",
    "            likelihoods.append(likelihood.reshape(np.shape(likelihood)[0], tc_shape[1], tc_shape[2]))\n",
    "\n",
    "        for swr_likelihood in likelihoods:\n",
    "            data[task_time][\"u\"] += int(np.any(u_zone & (swr_likelihood == np.nanmax(swr_likelihood))))\n",
    "            data[task_time][\"shortcut\"] += int(np.any(shortcut_zone & (swr_likelihood == np.nanmax(swr_likelihood))))\n",
    "            data[task_time][\"novel\"] += int(np.any(novel_zone & (swr_likelihood == np.nanmax(swr_likelihood))))\n",
    "            data[task_time][\"other\"] += int(np.any(other_zone & (swr_likelihood == np.nanmax(swr_likelihood))))\n",
    "        \n",
    "        if phase_swrs.n_epochs > 0:\n",
    "            data[task_time][\"u\"] /= phase_swrs.n_epochs\n",
    "            data[task_time][\"shortcut\"] /= phase_swrs.n_epochs\n",
    "            data[task_time][\"novel\"] /= phase_swrs.n_epochs\n",
    "            data[task_time][\"other\"] /= phase_swrs.n_epochs\n",
    "        \n",
    "    return data, n_swrs, likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:14:49.448420Z",
     "start_time": "2018-08-28T17:00:52.119Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_max_decoded_summary(all_data, n_all_swrs, task_times, maze_segments, n_sessions):\n",
    "\n",
    "    for task_time in task_times:\n",
    "        us = []\n",
    "        shortcuts = []\n",
    "        novels = []\n",
    "        others = []\n",
    "\n",
    "        n_swrs = 0\n",
    "\n",
    "        for i, session in enumerate(all_data):\n",
    "            us.append(session[task_time][\"u\"])\n",
    "            shortcuts.append(session[task_time][\"shortcut\"])\n",
    "            novels.append(session[task_time][\"novel\"])\n",
    "            others.append(session[task_time][\"other\"])\n",
    "\n",
    "        means = [np.nanmean(us), np.nanmean(shortcuts), np.nanmean(novels), np.nanmean(others)]\n",
    "        sems = [scipy.stats.sem(us), scipy.stats.sem(shortcuts), scipy.stats.sem(novels), scipy.stats.sem(others)]\n",
    "\n",
    "        xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7,5))\n",
    "        n = np.arange(len(maze_segments))\n",
    "        plt.bar(n, means, yerr=sems, color=\"#99d8c9\")\n",
    "        plt.xticks(n, maze_segments)\n",
    "        plt.text(0.95, 0.95, \"n sessions: \"+str(len(all_data)),\n",
    "             horizontalalignment='center',\n",
    "             verticalalignment='center',\n",
    "             transform = ax.transAxes,\n",
    "             fontsize=14)\n",
    "        if n_sessions == 1:\n",
    "             title = info.session_id + \"decoded zone during SWRs in \" + task_time\n",
    "        else:\n",
    "            title = \"Decoded zone during SWRs in \" + task_time\n",
    "        plt.title(title)\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(os.path.join(output_filepath, title+\".png\"))\n",
    "        plt.close()\n",
    "\n",
    "    #     plt.show()\n",
    "    \n",
    "    \n",
    "    for trajectory in maze_segments:\n",
    "        trajectory_means = []\n",
    "        trajectory_sems = []\n",
    "\n",
    "        tt = {key: [] for key in task_times}\n",
    "        n_swrs = {key: 0 for key in task_times}\n",
    "\n",
    "        for i, session in enumerate(all_data):\n",
    "            for task_time in task_times:\n",
    "                tt[task_time].append(session[task_time][trajectory])\n",
    "\n",
    "                n_swrs[task_time] += n_all_swrs[i][task_time]\n",
    "        trajectory_means = [np.nanmean(tt[\"prerecord\"]), np.nanmean(tt[\"pauseA\"]), np.nanmean(tt[\"pauseB\"]), np.nanmean(tt[\"postrecord\"])]\n",
    "        trajectory_sems = [scipy.stats.sem(tt[\"prerecord\"]), scipy.stats.sem(tt[\"pauseA\"]), scipy.stats.sem(tt[\"pauseB\"]), scipy.stats.sem(tt[\"postrecord\"])]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7,5))\n",
    "        n = np.arange(len(task_times))\n",
    "        plt.bar(n, trajectory_means, yerr=trajectory_sems, color=\"#41ae76\")\n",
    "        plt.xticks(n, task_times)\n",
    "        if n_sessions == 1:\n",
    "             title = info.session_id + \"decoded zone during SWRs for \" + trajectory\n",
    "        else:\n",
    "            title = \"Decoded zone during SWRs for \" + trajectory\n",
    "        plt.title(title)\n",
    "\n",
    "        for i, task_time in enumerate(task_times):\n",
    "            ax.text(i, 0.01, str(n_swrs[task_time]), ha=\"center\", fontsize=14)\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(os.path.join(output_filepath, title+\".png\"))\n",
    "        plt.close()\n",
    "\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:14:49.449419Z",
     "start_time": "2018-08-28T17:00:52.124Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot individual sessions\n",
    "for info in infos:\n",
    "    all_data = []\n",
    "    n_all_swrs = []\n",
    "    \n",
    "    data, n_swrs, likelihoods = get_max_likelihoods(info)\n",
    "\n",
    "    all_data.append(data)\n",
    "    n_all_swrs.append(n_swrs)\n",
    "    \n",
    "    plot_max_decoded_summary(all_data, n_all_swrs, task_times, maze_segments, n_sessions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:14:49.451418Z",
     "start_time": "2018-08-28T17:00:52.127Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot combined sessions\n",
    "all_data = []\n",
    "n_all_swrs = []\n",
    "\n",
    "for info in infos:\n",
    "    data, n_swrs, likelihoods = get_max_likelihoods(info)\n",
    "\n",
    "all_data.append(data)\n",
    "n_all_swrs.append(n_swrs)\n",
    "\n",
    "plot_max_decoded_summary(all_data, n_all_swrs, task_times, maze_segments, n_sessions=len(infos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:33:54.926868Z",
     "start_time": "2018-08-28T17:33:54.920892Z"
    }
   },
   "outputs": [],
   "source": [
    "average_likelihood = np.array(likelihoods[0][0].shape)\n",
    "for swr_likelihood in likelihoods:\n",
    "    print(\"huh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:34:00.763316Z",
     "start_time": "2018-08-28T17:34:00.521455Z"
    }
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "plt.plot(position.x, position.y, \"r.\", ms=1)\n",
    "pp = plt.pcolormesh(xx, yy, likelihoods[0][0], cmap='bone_r')\n",
    "\n",
    "plt.colorbar(pp)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:14:49.453418Z",
     "start_time": "2018-08-28T17:00:52.137Z"
    }
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "plt.plot(position.x, position.y, \"r.\", ms=1)\n",
    "plt.pcolormesh(xx, yy, other_zone, cmap='bone_r')\n",
    "\n",
    "plt.colorbar(pp)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T17:14:49.454417Z",
     "start_time": "2018-08-28T17:00:52.140Z"
    }
   },
   "outputs": [],
   "source": [
    "np.nanmean(us)/np.sum(u_zone), np.nanmean(shortcuts)/np.sum(shortcut_zone), np.nanmean(novels)/np.sum(novel_zone), np.nanmean(others)/np.sum(other_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
