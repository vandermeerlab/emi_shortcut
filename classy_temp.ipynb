{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:28.510540Z",
     "start_time": "2018-09-28T12:56:21.831951Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "import scipy\n",
    "import pickle\n",
    "import os\n",
    "import nept\n",
    "import scalebar\n",
    "\n",
    "from loading_data import get_data\n",
    "from analyze_tuning_curves import get_only_tuning_curves\n",
    "from utils_maze import get_zones\n",
    "from analyze_decode_swrs import (plot_summary_individual,\n",
    "                                 plot_likelihood_overspace,\n",
    "                                 plot_combined,\n",
    "                                 plot_stacked_summary)\n",
    "\n",
    "from utils_maze import get_bin_centers\n",
    "from analyze_classy_decode import bin_spikes\n",
    "\n",
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"trials\", \"decoding\", \"classy\")\n",
    "if not os.path.exists(output_filepath):\n",
    "    os.makedirs(output_filepath)\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:28.523533Z",
     "start_time": "2018-09-28T12:56:28.512539Z"
    }
   },
   "outputs": [],
   "source": [
    "class Session:\n",
    "    \"\"\"A collection of LikelihoodsAtTaskTime for each session\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        task_times : dict of TaskTime\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position, task_labels, zones):\n",
    "        self.position = position\n",
    "        for task_label in task_labels:\n",
    "            setattr(self, task_label, TaskTime([], [], [], zones))\n",
    "\n",
    "    def pickle(self, save_path):\n",
    "        with open(save_path, 'wb') as fileobj:\n",
    "            print(\"Saving \" + save_path)\n",
    "            pickle.dump(self, fileobj)\n",
    "\n",
    "    def n_tasktimes(self):\n",
    "        return len(task_labels)\n",
    "\n",
    "\n",
    "class TaskTime:\n",
    "    \"\"\"A set of decoded likelihoods for a given task time\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        likelihoods : np.array\n",
    "            With shape (ntimebins, nxbins, nybins)\n",
    "        zones : dict of Zones\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        likelihoods : np.array\n",
    "            With shape (ntimebins, nxbins, nybins)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tuning_curves, swrs, likelihoods, zones):\n",
    "        self.tuning_curves = tuning_curves\n",
    "        self.swrs = swrs\n",
    "        self.likelihoods = likelihoods\n",
    "        self.zones = zones\n",
    "\n",
    "    def sums(self, zone_label):\n",
    "        if len(self.likelihoods) > 0:\n",
    "            return np.nansum(self.likelihoods[:, :, self.zones[zone_label]], axis=2)\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    def means(self, zone_label):\n",
    "        if len(self.likelihoods) > 0:\n",
    "            return np.nanmean(self.likelihoods[:, :, self.zones[zone_label]], axis=2)\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    def maxs(self, zone_label):\n",
    "        if len(self.likelihoods) > 0:\n",
    "            return np.nanmax(self.likelihoods[:, :, self.zones[zone_label]], axis=2)\n",
    "        else:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:28.711426Z",
     "start_time": "2018-09-28T12:56:28.526531Z"
    }
   },
   "outputs": [],
   "source": [
    "import info.r068d7 as r068d7\n",
    "info = r068d7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:28.718422Z",
     "start_time": "2018-09-28T12:56:28.712425Z"
    }
   },
   "outputs": [],
   "source": [
    "n_shuffles = 2\n",
    "percentile_thresh = 99\n",
    "\n",
    "task_labels = [\"prerecord\", \"pauseA\", \"pauseB\", \"postrecord\"]\n",
    "zone_labels = [\"u\", \"shortcut\", \"novel\", \"other\"]\n",
    "\n",
    "swr_params = dict()\n",
    "swr_params[\"z_thresh\"] = 2.0\n",
    "swr_params[\"power_thresh\"] = 3.0\n",
    "swr_params[\"merge_thresh\"] = 0.02\n",
    "swr_params[\"min_length\"] = 0.05\n",
    "swr_params[\"swr_thresh\"] = (140.0, 250.0)\n",
    "swr_params[\"min_involved\"] = 4\n",
    "\n",
    "colours = dict()\n",
    "colours[\"u\"] = \"#2b8cbe\"\n",
    "colours[\"shortcut\"] = \"#31a354\"\n",
    "colours[\"novel\"] = \"#d95f0e\"\n",
    "colours[\"other\"] = \"#bdbdbd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:28.736412Z",
     "start_time": "2018-09-28T12:56:28.720421Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_likelihoods(info, swr_params, task_labels, zone_labels, n_shuffles=0, save_path=None):\n",
    "\n",
    "    _, position, spikes, lfp, _ = get_data(info)\n",
    "\n",
    "    zones = dict()\n",
    "    zones[\"u\"], zones[\"shortcut\"], zones[\"novel\"] = get_zones(info, position, subset=True)\n",
    "    combined_zones = zones[\"u\"] + zones[\"shortcut\"] + zones[\"novel\"]\n",
    "    zones[\"other\"] = ~combined_zones\n",
    "\n",
    "    sliced_position = position.time_slice(info.task_times[\"phase3\"].starts, info.task_times[\"phase3\"].stops)\n",
    "    session = Session(sliced_position, task_labels, zones)\n",
    "\n",
    "    tuning_curves_fromdata = get_only_tuning_curves(info, position, spikes, info.task_times[\"phase3\"])\n",
    "\n",
    "    tc_shape = tuning_curves_fromdata.shape\n",
    "\n",
    "    swrs = nept.detect_swr_hilbert(lfp,\n",
    "                                   fs=info.fs,\n",
    "                                   thresh=swr_params[\"swr_thresh\"],\n",
    "                                   z_thresh=swr_params[\"z_thresh\"],\n",
    "                                   power_thresh=swr_params[\"power_thresh\"],\n",
    "                                   merge_thresh=swr_params[\"merge_thresh\"],\n",
    "                                   min_length=swr_params[\"min_length\"])\n",
    "    swrs = nept.find_multi_in_epochs(spikes, swrs, min_involved=swr_params[\"min_involved\"])\n",
    "\n",
    "    rest_epochs = nept.rest_threshold(position, thresh=12., t_smooth=0.8)\n",
    "\n",
    "    if n_shuffles > 0:\n",
    "        n_passes = n_shuffles\n",
    "    else:\n",
    "        n_passes = 1\n",
    "\n",
    "    for task_label in task_labels:\n",
    "        epochs_of_interest = info.task_times[task_label].intersect(rest_epochs)\n",
    "\n",
    "        phase_swrs = epochs_of_interest.overlaps(swrs)\n",
    "        phase_swrs = phase_swrs[phase_swrs.durations >= 0.05]\n",
    "\n",
    "        phase_likelihoods = np.zeros((n_passes, phase_swrs.n_epochs, tc_shape[1], tc_shape[2]))\n",
    "        phase_tuningcurves = np.zeros((n_passes, tc_shape[0], tc_shape[1], tc_shape[2]))\n",
    "        for n_pass in range(n_passes):\n",
    "\n",
    "            if n_shuffles > 0:\n",
    "                tuning_curves = np.random.permutation(tuning_curves_fromdata)\n",
    "            else:\n",
    "                tuning_curves = tuning_curves_fromdata\n",
    "\n",
    "            phase_tuningcurves[n_pass, ] = tuning_curves\n",
    "            tuning_curves = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "            for n_timebin, (start, stop) in enumerate(zip(phase_swrs.starts,\n",
    "                                                          phase_swrs.stops)):\n",
    "                t_window = stop-start  # 0.1 for running, 0.025 for swr\n",
    "\n",
    "                sliced_spikes = [spiketrain.time_slice(start, stop) for spiketrain in spikes]\n",
    "\n",
    "                counts = bin_spikes(sliced_spikes, np.array([start, stop]), dt=t_window, window=t_window,\n",
    "                                    gaussian_std=0.0075, normalized=False)\n",
    "\n",
    "                likelihood = nept.bayesian_prob(counts, tuning_curves, binsize=t_window,\n",
    "                                                min_neurons=3, min_spikes=1)\n",
    "\n",
    "                phase_likelihoods[n_pass, n_timebin] = likelihood.reshape(tc_shape[1], tc_shape[2])\n",
    "\n",
    "        tasktime = getattr(session, task_label)\n",
    "        tasktime.likelihoods = phase_likelihoods\n",
    "        tasktime.tuning_curves = phase_tuningcurves\n",
    "        tasktime.swrs = phase_swrs\n",
    "\n",
    "    if save_path is not None:\n",
    "        session.pickle(save_path)\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:56.190773Z",
     "start_time": "2018-09-28T12:56:28.738410Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_individual = False\n",
    "update_cache = True\n",
    "dont_save_pickle = False\n",
    "\n",
    "\n",
    "print(info.session_id)\n",
    "\n",
    "# Get true data\n",
    "true_path = os.path.join(pickle_filepath, info.session_id+\"_likelihoods_true.pkl\")\n",
    "\n",
    "# Remove previous pickle if update_cache\n",
    "if update_cache:\n",
    "    if os.path.exists(true_path):\n",
    "        os.remove(true_path)\n",
    "\n",
    "# Load pickle if it exists, otherwise compute and pickle\n",
    "if os.path.exists(true_path):\n",
    "    print(\"Loading pickled true likelihoods...\")\n",
    "    compute_likelihoods = False\n",
    "    with open(true_path, 'rb') as fileobj:\n",
    "        true_session = pickle.load(fileobj)\n",
    "else:\n",
    "    if dont_save_pickle:\n",
    "        true_path = None\n",
    "    true_session = get_likelihoods(info,\n",
    "                                   swr_params,\n",
    "                                   task_labels,\n",
    "                                   zone_labels,\n",
    "                                   save_path=true_path)\n",
    "\n",
    "# Get shuffled data\n",
    "shuffled_path = os.path.join(pickle_filepath,\n",
    "                             info.session_id+\"_likelihoods_shuffled-%03d.pkl\" % n_shuffles)\n",
    "\n",
    "# Remove previous pickle if update_cache\n",
    "if update_cache:\n",
    "    if os.path.exists(shuffled_path):\n",
    "        os.remove(shuffled_path)\n",
    "\n",
    "# Load pickle if it exists, otherwise compute and pickle\n",
    "if os.path.exists(shuffled_path):\n",
    "    print(\"Loading pickled shuffled likelihoods...\")\n",
    "    with open(shuffled_path, 'rb') as fileobj:\n",
    "        shuffled_session = pickle.load(fileobj)\n",
    "else:\n",
    "    if dont_save_pickle:\n",
    "        shuffled_path = None\n",
    "    shuffled_session = get_likelihoods(info,\n",
    "                                       swr_params,\n",
    "                                       task_labels,\n",
    "                                       zone_labels,\n",
    "                                       n_shuffles=n_shuffles,\n",
    "                                       save_path=shuffled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:56.200768Z",
     "start_time": "2018-09-28T12:56:56.192773Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_likelihood_overspace(info, session, task_labels, filepath=None):\n",
    "    for task_label in task_labels:\n",
    "        zones = getattr(session, task_label).zones\n",
    "        likelihood = np.nanmean(np.array(getattr(session, task_label).likelihoods), axis=(0,1))\n",
    "        print(likelihood.shape)\n",
    "\n",
    "        likelihood[np.isnan(likelihood)] = 0\n",
    "\n",
    "        xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "        xcenters, ycenters = get_bin_centers(info)\n",
    "        xxx, yyy = np.meshgrid(xcenters, ycenters)\n",
    "\n",
    "        maze_highlight = \"#fed976\"\n",
    "        plt.plot(session.position.x, session.position.y, \".\", color=maze_highlight, ms=1, alpha=0.2)\n",
    "        pp = plt.pcolormesh(xx, yy, likelihood, cmap='bone_r')\n",
    "        for label in [\"u\", \"shortcut\", \"novel\"]:\n",
    "            plt.contour(xxx, yyy, zones[label], levels=0, linewidths=2, colors=colours[label])\n",
    "        plt.colorbar(pp)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if filepath is not None:\n",
    "            filename = info.session_id + \"_\" + task_label + \"_likelihoods-overspace.png\"\n",
    "            plt.savefig(os.path.join(output_filepath, filename))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.058279Z",
     "start_time": "2018-09-28T12:56:56.202767Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_likelihood_overspace(info, shuffled_session, task_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.067275Z",
     "start_time": "2018-09-28T12:56:57.061278Z"
    }
   },
   "outputs": [],
   "source": [
    "true_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.082267Z",
     "start_time": "2018-09-28T12:56:57.069274Z"
    }
   },
   "outputs": [],
   "source": [
    "percentile_thresh = 95\n",
    "\n",
    "keep_idx = {task_label: [] for task_label in task_labels}\n",
    "\n",
    "for task_label in task_labels:\n",
    "    for zone_label in zone_labels:\n",
    "        zones = getattr(true_session, task_label).zones\n",
    "        true_sums = np.array(getattr(true_session, task_label).sums(zone_label))\n",
    "        shuffled_sums = np.array(getattr(shuffled_session, task_label).sums(zone_label))\n",
    "        for idx in range(true_sums.shape[1]):\n",
    "            percentile = scipy.stats.percentileofscore(np.sort(shuffled_sums[:, idx]),\n",
    "                                                       true_sums[:, idx])\n",
    "            if percentile >= percentile_thresh:\n",
    "                keep_idx[task_label].append(idx)\n",
    "\n",
    "passthresh_session = Session(true_session.position, task_labels, zones)\n",
    "\n",
    "for task_label in task_labels:\n",
    "    passthresh_idx = [np.sort(np.unique(keep_idx[task_label]))]\n",
    "    passthresh_likelihoods = np.array(getattr(true_session, task_label).likelihoods)[:, passthresh_idx]\n",
    "    passthresh_swrs = getattr(true_session, task_label).swrs[passthresh_idx]\n",
    "    passthresh_tuningcurves = getattr(true_session, task_label).tuning_curves\n",
    "    \n",
    "    tasktime = getattr(passthresh_session, task_label)\n",
    "    tasktime.likelihoods = passthresh_likelihoods\n",
    "    tasktime.swrs = passthresh_swrs\n",
    "    tasktime.tuning_curves = passthresh_tuningcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T13:53:37.384737Z",
     "start_time": "2018-09-28T13:53:37.378741Z"
    }
   },
   "outputs": [],
   "source": [
    "true_session.pauseA.sums(\"u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T14:53:26.563129Z",
     "start_time": "2018-09-28T14:53:26.556133Z"
    }
   },
   "outputs": [],
   "source": [
    "np.squeeze(np.nansum(true_session.pauseA.likelihoods[:, :, true_session.pauseA.zones[\"u\"]], axis=2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T14:53:07.938084Z",
     "start_time": "2018-09-28T14:53:07.933106Z"
    }
   },
   "outputs": [],
   "source": [
    "true_session.pauseA.likelihoods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T14:09:00.131389Z",
     "start_time": "2018-09-28T14:09:00.125391Z"
    }
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "a.extend([np.nan])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T15:02:19.345110Z",
     "start_time": "2018-09-28T15:02:19.341112Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.ones(3) * np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T15:02:23.113519Z",
     "start_time": "2018-09-28T15:02:23.109521Z"
    }
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T15:03:27.403350Z",
     "start_time": "2018-09-28T15:03:27.398354Z"
    }
   },
   "outputs": [],
   "source": [
    "a.reshape(a.shape[0], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T13:49:03.749145Z",
     "start_time": "2018-09-28T13:49:03.732156Z"
    }
   },
   "outputs": [],
   "source": [
    "np.nanmean([np.array([]), np.array([2.,1.])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.093260Z",
     "start_time": "2018-09-28T12:56:57.084265Z"
    }
   },
   "outputs": [],
   "source": [
    "passthresh_swrs = getattr(true_session, task_label).swrs[passthresh_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.105253Z",
     "start_time": "2018-09-28T12:56:57.095259Z"
    }
   },
   "outputs": [],
   "source": [
    "passthresh_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.260164Z",
     "start_time": "2018-09-28T12:56:57.107252Z"
    }
   },
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.260164Z",
     "start_time": "2018-09-28T12:56:21.920Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for zone_label in zone_labels:\n",
    "    sums = {task_label: [] for task_label in task_labels}\n",
    "    for session in sessions:\n",
    "        for task_label in task_labels:\n",
    "            sums[task_label].extend(getattr(session, task_label).sums(zone_label))\n",
    "    print(np.array(sums[task_label]).shape)\n",
    "    \n",
    "    means = [np.nanmean(sums[task_label])\n",
    "             if len(sums[task_label]) > 0 else 0.0\n",
    "             for task_label in task_labels]\n",
    "    print(means)\n",
    "    sems = [np.nanmean(scipy.stats.sem(np.array(sums[task_label]), axis=0,\n",
    "                                       nan_policy=\"omit\"))\n",
    "            if len(sums[task_label]) > 1 else 0.0\n",
    "            for task_label in task_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.262163Z",
     "start_time": "2018-09-28T12:56:21.927Z"
    }
   },
   "outputs": [],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.263163Z",
     "start_time": "2018-09-28T12:56:21.937Z"
    }
   },
   "outputs": [],
   "source": [
    "[np.nanmean(scipy.stats.sem(sums[task_label], axis=0,\n",
    "                                           nan_policy=\"omit\"))\n",
    "                if len(sums[task_label]) > 1 else 0.0\n",
    "                for task_label in task_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.264163Z",
     "start_time": "2018-09-28T12:56:21.942Z"
    }
   },
   "outputs": [],
   "source": [
    "[np.nanmean(sums[task_label])\n",
    "                 if len(sums[task_label]) > 0 else 0.0\n",
    "                 for task_label in task_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.266161Z",
     "start_time": "2018-09-28T12:56:21.954Z"
    }
   },
   "outputs": [],
   "source": [
    "sessions = [session, session]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.267161Z",
     "start_time": "2018-09-28T12:56:21.960Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_means = {zone_label: [] for zone_label in zone_labels}\n",
    "trajectory_means = {zone_label: [] for zone_label in zone_labels}\n",
    "for session in sessions:\n",
    "    zone_label = \"u\"\n",
    "#     for zone_label in zone_labels:\n",
    "    task_label = \"pauseA\"\n",
    "#         for task_label in task_labels:\n",
    "    combined_means[zone_label].append(getattr(session, task_label).sums(zone_label))\n",
    "#         print([np.nanmean(getattr(session, task_label).sums(zone_label))\n",
    "#                                         if len(getattr(session, task_label).sums(zone_label)) > 0 else 0.0\n",
    "#                                         for task_label in task_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.268160Z",
     "start_time": "2018-09-28T12:56:21.963Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_means[\"u\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.269160Z",
     "start_time": "2018-09-28T12:56:21.969Z"
    }
   },
   "outputs": [],
   "source": [
    "session.pauseA.likelihoods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.269160Z",
     "start_time": "2018-09-28T12:56:21.975Z"
    }
   },
   "outputs": [],
   "source": [
    "_, position, spikes, lfp, _ = get_data(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.270159Z",
     "start_time": "2018-09-28T12:56:21.980Z"
    }
   },
   "outputs": [],
   "source": [
    "buffer = 0.1\n",
    "for start, stop in zip(session.pauseA.swrs[0].start, session.pauseA.swrs[0].stop):\n",
    "    sliced_lfp = lfp.time_slice(start-buffer, stop+buffer)\n",
    "    swr_trace = lfp.time_slice(start, stop)\n",
    "    \n",
    "    plt.plot(sliced_lfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.271158Z",
     "start_time": "2018-09-28T12:56:21.990Z"
    }
   },
   "outputs": [],
   "source": [
    "swr_highlight = \"#fc4e2a\"\n",
    "for swr_idx in range(session.pauseA.swrs.n_epochs):\n",
    "    \n",
    "    start = session.pauseA.swrs[swr_idx].start\n",
    "    stop = session.pauseA.swrs[swr_idx].stop\n",
    "    \n",
    "    start_idx = nept.find_nearest_idx(lfp.time, start - buffer)\n",
    "    stop_idx = nept.find_nearest_idx(lfp.time, stop + buffer)\n",
    "    plt.plot(lfp.time[start_idx:stop_idx], lfp.data[start_idx:stop_idx], color=\"k\", lw=0.3, alpha=0.9)\n",
    "\n",
    "    start_idx = nept.find_nearest_idx(lfp.time, start)\n",
    "    stop_idx = nept.find_nearest_idx(lfp.time, stop)\n",
    "    plt.plot(lfp.time[start_idx:stop_idx], lfp.data[start_idx:stop_idx], color=swr_highlight, lw=0.6)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.272158Z",
     "start_time": "2018-09-28T12:56:21.996Z"
    }
   },
   "outputs": [],
   "source": [
    "add_rows = int(len(sliced_spikes) / 8)\n",
    "\n",
    "ms = 600 / len(sliced_spikes)\n",
    "mew = 0.7\n",
    "spike_loc = 1\n",
    "\n",
    "sliced_spikes = [spiketrain.time_slice(start-buffer, stop+buffer) for spiketrain in spikes]\n",
    "\n",
    "for idx, neuron_spikes in enumerate(sliced_spikes):\n",
    "    plt.plot(neuron_spikes.time, np.ones(len(neuron_spikes.time)) + (idx * spike_loc), '|',\n",
    "             color='k', ms=ms, mew=mew)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.273158Z",
     "start_time": "2018-09-28T12:56:22.007Z"
    }
   },
   "outputs": [],
   "source": [
    "session.pauseA.likelihoods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.274157Z",
     "start_time": "2018-09-28T12:56:22.013Z"
    }
   },
   "outputs": [],
   "source": [
    "session.pauseA.swrs[0].starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.274157Z",
     "start_time": "2018-09-28T12:56:22.020Z"
    }
   },
   "outputs": [],
   "source": [
    "session.pauseA.swrs.n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.275156Z",
     "start_time": "2018-09-28T12:56:22.025Z"
    }
   },
   "outputs": [],
   "source": [
    "session.pauseA.likelihoods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.276155Z",
     "start_time": "2018-09-28T12:56:22.030Z"
    }
   },
   "outputs": [],
   "source": [
    "session.pauseA.sums(\"u\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.277155Z",
     "start_time": "2018-09-28T12:56:22.035Z"
    }
   },
   "outputs": [],
   "source": [
    "session.pauseA.zones[\"u\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.278154Z",
     "start_time": "2018-09-28T12:56:22.041Z"
    }
   },
   "outputs": [],
   "source": [
    "np.nansum(session.pauseA.likelihoods[:, :, session.pauseA.zones[\"u\"]], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.279153Z",
     "start_time": "2018-09-28T12:56:22.046Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(session.pauseA.swrs.n_epochs):\n",
    "    a = session.pauseA.likelihoods[:, i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.280153Z",
     "start_time": "2018-09-28T12:56:22.051Z"
    }
   },
   "outputs": [],
   "source": [
    "xx, yy = get_bin_centers(info)\n",
    "print(xx.shape, yy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.281152Z",
     "start_time": "2018-09-28T12:56:22.056Z"
    }
   },
   "outputs": [],
   "source": [
    "p = plt.pcolormesh(xx, yy, a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.282152Z",
     "start_time": "2018-09-28T12:56:22.062Z"
    }
   },
   "outputs": [],
   "source": [
    "likelihood_shuffled = getattr(shuffled_session, task_label).likelihoods[:, swr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.284151Z",
     "start_time": "2018-09-28T12:56:22.068Z"
    }
   },
   "outputs": [],
   "source": [
    "zones = getattr(session, task_label).zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.285151Z",
     "start_time": "2018-09-28T12:56:22.074Z"
    }
   },
   "outputs": [],
   "source": [
    "likelihood_shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.286150Z",
     "start_time": "2018-09-28T12:56:22.079Z"
    }
   },
   "outputs": [],
   "source": [
    "shuffled_means = [scipy.stats.sem(np.nansum(likelihood_shuffled[:, zones[zone_label]], axis=1))\n",
    "                  for zone_label in zone_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.287149Z",
     "start_time": "2018-09-28T12:56:22.085Z"
    }
   },
   "outputs": [],
   "source": [
    "scipy.stats.sem(np.nansum(likelihood_shuffled[:, zones[\"other\"]], axis=1), nan_policy=\"omit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.288149Z",
     "start_time": "2018-09-28T12:56:22.091Z"
    }
   },
   "outputs": [],
   "source": [
    "shuffled_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.289148Z",
     "start_time": "2018-09-28T12:56:22.096Z"
    }
   },
   "outputs": [],
   "source": [
    "n = np.arange(len(means))\n",
    "plt.bar(n, means,\n",
    "        color=[colours[\"u\"], colours[\"shortcut\"], colours[\"novel\"], colours[\"other\"]], edgecolor='k')\n",
    "plt.set_xticks(n)\n",
    "plt.set_xticklabels([], rotation=90)\n",
    "plt.set_ylim([0, 1.])\n",
    "plt.set_title(\"True proportion\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.290148Z",
     "start_time": "2018-09-28T12:56:22.103Z"
    }
   },
   "outputs": [],
   "source": [
    "session.prerecord.likelihoods.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.291147Z",
     "start_time": "2018-09-28T12:56:22.110Z"
    }
   },
   "outputs": [],
   "source": [
    "session.prerecord.swrs.n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.292147Z",
     "start_time": "2018-09-28T12:56:22.117Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_summary_individual(info, session_true, session_shuffled, zone_labels, task_labels, colours, filepath=None):\n",
    "    \n",
    "    _, position, spikes, lfp, _ = get_data(info)\n",
    "    \n",
    "    buffer = 0.1\n",
    "\n",
    "    for task_label in task_labels:\n",
    "        print(task_label)\n",
    "        swrs = getattr(session, task_label).swrs\n",
    "        zones = getattr(session, task_label).zones\n",
    "\n",
    "        for swr_idx in range(swrs.n_epochs):\n",
    "            print(\"swr:\" + str(swr_idx))\n",
    "            start = swrs[swr_idx].start\n",
    "            stop = swrs[swr_idx].stop\n",
    "\n",
    "            sliced_spikes = [spiketrain.time_slice(start-buffer, stop+buffer) for spiketrain in spikes]\n",
    "\n",
    "            add_rows = int(len(sliced_spikes) / 8)\n",
    "\n",
    "            ms = 600 / len(sliced_spikes)\n",
    "            mew = 0.7\n",
    "            spike_loc = 1\n",
    "\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            gs1 = gridspec.GridSpec(3, 2)\n",
    "            gs1.update(wspace=0.3, hspace=0.3)\n",
    "\n",
    "            ax1 = plt.subplot(gs1[1:, 0])\n",
    "            for idx, neuron_spikes in enumerate(sliced_spikes):\n",
    "                ax1.plot(neuron_spikes.time, np.ones(len(neuron_spikes.time)) + (idx * spike_loc), '|',\n",
    "                         color='k', ms=ms, mew=mew)\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(gs1[0, 0], sharex=ax1)\n",
    "\n",
    "            swr_highlight = \"#fc4e2a\"\n",
    "            start_idx = nept.find_nearest_idx(lfp.time, start - buffer)\n",
    "            stop_idx = nept.find_nearest_idx(lfp.time, stop + buffer)\n",
    "            ax2.plot(lfp.time[start_idx:stop_idx], lfp.data[start_idx:stop_idx], color=\"k\", lw=0.3, alpha=0.9)\n",
    "\n",
    "            start_idx = nept.find_nearest_idx(lfp.time, start)\n",
    "            stop_idx = nept.find_nearest_idx(lfp.time, stop)\n",
    "            ax2.plot(lfp.time[start_idx:stop_idx], lfp.data[start_idx:stop_idx], color=swr_highlight, lw=0.6)\n",
    "            ax2.axis(\"off\")\n",
    "\n",
    "            ax1.axvline(lfp.time[start_idx], linewidth=1, color=swr_highlight)\n",
    "            ax1.axvline(lfp.time[stop_idx], linewidth=1, color=swr_highlight)\n",
    "            ax1.axvspan(lfp.time[start_idx], lfp.time[stop_idx], alpha=0.2, color=swr_highlight)\n",
    "\n",
    "            scalebar.add_scalebar(ax2, matchy=False, bbox_transform=fig.transFigure,\n",
    "                                  bbox_to_anchor=(0.25, 0.05), units='ms')\n",
    "            \n",
    "            likelihood_true = np.array(getattr(session_true, task_label).likelihoods[:, swr_idx])\n",
    "\n",
    "            likelihood_true[np.isnan(likelihood_true)] = 0\n",
    "\n",
    "            xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "            xcenters, ycenters = get_bin_centers(info)\n",
    "            xxx, yyy = np.meshgrid(xcenters, ycenters)\n",
    "\n",
    "            maze_highlight = \"#fed976\"\n",
    "            ax3 = plt.subplot(gs1[0, 1])\n",
    "            sliced_position = position.time_slice(info.task_times[\"phase3\"].starts, info.task_times[\"phase3\"].stops)\n",
    "            ax3.plot(sliced_position.x, sliced_position.y, \".\", color=maze_highlight, ms=1, alpha=0.2)\n",
    "            pp = ax3.pcolormesh(xx, yy, likelihood_true[0], cmap='bone_r')\n",
    "            for label in [\"u\", \"shortcut\", \"novel\"]:\n",
    "                ax3.contour(xxx, yyy, zones[label], levels=0, linewidths=2, colors=colours[label])\n",
    "            plt.colorbar(pp)\n",
    "            ax3.axis('off')\n",
    "            \n",
    "            likelihood_true = getattr(session_true, task_label).likelihoods[:, swr_idx]\n",
    "            \n",
    "            means_true = [np.nanmean(np.nansum(likelihood_true[:, zones[zone_label]], axis=1))\n",
    "                          for zone_label in zone_labels]\n",
    "\n",
    "            ax4 = plt.subplot(gs1[1:2, 1])\n",
    "            n = np.arange(len(zone_labels))\n",
    "            ax4.bar(n, means_true,\n",
    "                    color=[colours[zone_label] for zone_label in zone_labels], edgecolor='k')\n",
    "            ax4.set_xticks(n)\n",
    "            ax4.set_xticklabels([], rotation=90)\n",
    "            ax4.set_ylim([0, 1.])\n",
    "            ax4.set_title(\"True proportion\", fontsize=14)\n",
    "            \n",
    "            likelihood_shuffled = getattr(session_shuffled, task_label).likelihoods[:, swr_idx]\n",
    "            \n",
    "            means_shuffled = [np.nanmean(np.nansum(likelihood_shuffled[:, zones[zone_label]], axis=1))                    \n",
    "                              for zone_label in zone_labels]\n",
    "            sems_shuffled = [scipy.stats.sem(np.nansum(likelihood_shuffled[:, zones[zone_label]], axis=1))                                          \n",
    "                             for zone_label in zone_labels]\n",
    "\n",
    "            ax5 = plt.subplot(gs1[2:, 1], sharey=ax4)\n",
    "            n = np.arange(len(zone_labels))\n",
    "            ax5.bar(n, means_shuffled,\n",
    "                    yerr=sems_shuffled,\n",
    "                    color=[colours[zone_label] for zone_label in zone_labels], edgecolor='k')\n",
    "            ax5.set_xticks(n)                                                                                                                                            \n",
    "            ax5.set_xticklabels(zone_labels, rotation=90)\n",
    "            ax5.set_ylim([0, 1.])\n",
    "            ax5.set_title(\"Shuffled proportion\", fontsize=14)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if filepath is not None:\n",
    "                filename = info.session_id+\"_\"+task_label+\"_summary-swr\"+str(swr_idx)+\".png\"\n",
    "                plt.savefig(os.path.join(output_filepath, filename))\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T12:56:57.292147Z",
     "start_time": "2018-09-28T12:56:22.126Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_summary_individual(info, true_session, shuffled_session, zone_labels, task_labels, colours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T20:00:04.788913Z",
     "start_time": "2018-09-28T19:59:12.063200Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "import scipy\n",
    "import pickle\n",
    "import os\n",
    "import nept\n",
    "import scalebar\n",
    "\n",
    "from loading_data import get_data\n",
    "from analyze_tuning_curves import get_only_tuning_curves\n",
    "from utils_maze import get_zones\n",
    "from utils_maze import get_bin_centers\n",
    "\n",
    "thisdir = os.getcwd()\n",
    "pickle_filepath = os.path.join(thisdir, \"cache\", \"pickled\")\n",
    "output_filepath = os.path.join(thisdir, \"plots\", \"trials\", \"decoding\", \"shuffled\")\n",
    "if not os.path.exists(output_filepath):\n",
    "    os.makedirs(output_filepath)\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class Session:\n",
    "    \"\"\"A collection of LikelihoodsAtTaskTime for each session\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        task_times : dict of TaskTime\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position, task_labels, zones):\n",
    "        self.position = position\n",
    "        for task_label in task_labels:\n",
    "            setattr(self, task_label, TaskTime([], [], [], zones))\n",
    "\n",
    "    def pickle(self, save_path):\n",
    "        with open(save_path, 'wb') as fileobj:\n",
    "            print(\"Saving \" + save_path)\n",
    "            pickle.dump(self, fileobj)\n",
    "\n",
    "    def n_tasktimes(self):\n",
    "        return len(task_labels)\n",
    "\n",
    "\n",
    "class TaskTime:\n",
    "    \"\"\"A set of decoded likelihoods for a given task time\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        likelihoods : np.array\n",
    "            With shape (ntimebins, nxbins, nybins)\n",
    "        zones : dict of Zones\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        likelihoods : np.array\n",
    "            With shape (ntimebins, nxbins, nybins)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tuning_curves, swrs, likelihoods, zones):\n",
    "        self.tuning_curves = tuning_curves\n",
    "        self.swrs = swrs\n",
    "        self.likelihoods = likelihoods\n",
    "        self.zones = zones\n",
    "\n",
    "    def sums(self, zone_label):\n",
    "        if self.swrs.n_epochs > 0:\n",
    "            return np.squeeze(np.nansum(self.likelihoods[:, :, self.zones[zone_label]], axis=2))\n",
    "        else:\n",
    "            return np.ones((self.likelihoods.shape[0], 1)) * np.nan\n",
    "\n",
    "    def means(self, zone_label):\n",
    "        if self.swrs.n_epochs > 0:\n",
    "            return np.squeeze(np.nanmean(self.likelihoods[:, :, self.zones[zone_label]], axis=2))\n",
    "        else:\n",
    "            return np.ones((self.likelihoods.shape[0], 1)) * np.nan\n",
    "\n",
    "    def maxs(self, zone_label):\n",
    "        if self.swrs.n_epochs > 0:\n",
    "            return np.squeeze(np.nanmax(self.likelihoods[:, :, self.zones[zone_label]], axis=2))\n",
    "        else:\n",
    "            return np.ones((self.likelihoods.shape[0], 1)) * np.nan\n",
    "\n",
    "\n",
    "def bin_spikes(spikes, time, dt, window=None, gaussian_std=None, normalized=True):\n",
    "    \"\"\"Bins spikes using a sliding window.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spikes: list\n",
    "        Of nept.SpikeTrain\n",
    "    time: np.array\n",
    "    window: float or None\n",
    "        Length of the sliding window, in seconds. If None, will default to dt.\n",
    "    dt: float\n",
    "    gaussian_std: float or None\n",
    "    normalized: boolean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    binned_spikes: nept.AnalogSignal\n",
    "\n",
    "    \"\"\"\n",
    "    if window is None:\n",
    "        window = dt\n",
    "\n",
    "    bin_edges = time\n",
    "\n",
    "    given_n_bins = window / dt\n",
    "    n_bins = int(round(given_n_bins))\n",
    "    if abs(n_bins - given_n_bins) > 0.01:\n",
    "        warnings.warn(\"dt does not divide window evenly. \"\n",
    "                      \"Using window %g instead.\" % (n_bins*dt))\n",
    "\n",
    "    if normalized:\n",
    "        square_filter = np.ones(n_bins) * (1 / n_bins)\n",
    "    else:\n",
    "        square_filter = np.ones(n_bins)\n",
    "\n",
    "    counts = np.zeros((len(spikes), len(bin_edges) - 1))\n",
    "    for idx, spiketrain in enumerate(spikes):\n",
    "        counts[idx] = np.convolve(np.histogram(spiketrain.time, bins=bin_edges)[0].astype(float),\n",
    "                                  square_filter, mode=\"same\")\n",
    "\n",
    "    if gaussian_std is not None:\n",
    "        counts = nept.gaussian_filter(counts, gaussian_std, dt=dt, normalized=normalized, axis=1)\n",
    "\n",
    "    return nept.AnalogSignal(counts, bin_edges[:-1])\n",
    "\n",
    "\n",
    "def get_likelihoods(info, swr_params, task_labels, zone_labels, n_shuffles=0, save_path=None):\n",
    "\n",
    "    _, position, spikes, lfp, _ = get_data(info)\n",
    "\n",
    "    zones = dict()\n",
    "    zones[\"u\"], zones[\"shortcut\"], zones[\"novel\"] = get_zones(info, position, subset=True)\n",
    "    combined_zones = zones[\"u\"] + zones[\"shortcut\"] + zones[\"novel\"]\n",
    "    zones[\"other\"] = ~combined_zones\n",
    "\n",
    "    session = Session(position, task_labels, zones)\n",
    "\n",
    "    tuning_curves_fromdata = get_only_tuning_curves(info, position, spikes, info.task_times[\"phase3\"])\n",
    "\n",
    "    tc_shape = tuning_curves_fromdata.shape\n",
    "\n",
    "    swrs = nept.detect_swr_hilbert(lfp,\n",
    "                                   fs=info.fs,\n",
    "                                   thresh=swr_params[\"swr_thresh\"],\n",
    "                                   z_thresh=swr_params[\"z_thresh\"],\n",
    "                                   power_thresh=swr_params[\"power_thresh\"],\n",
    "                                   merge_thresh=swr_params[\"merge_thresh\"],\n",
    "                                   min_length=swr_params[\"min_length\"])\n",
    "    swrs = nept.find_multi_in_epochs(spikes, swrs, min_involved=swr_params[\"min_involved\"])\n",
    "\n",
    "    rest_epochs = nept.rest_threshold(position, thresh=12., t_smooth=0.8)\n",
    "\n",
    "    if n_shuffles > 0:\n",
    "        n_passes = n_shuffles\n",
    "    else:\n",
    "        n_passes = 1\n",
    "\n",
    "    for task_label in task_labels:\n",
    "        epochs_of_interest = info.task_times[task_label].intersect(rest_epochs)\n",
    "\n",
    "        phase_swrs = epochs_of_interest.overlaps(swrs)\n",
    "        phase_swrs = phase_swrs[phase_swrs.durations >= 0.05]\n",
    "\n",
    "        phase_likelihoods = np.zeros((n_passes, phase_swrs.n_epochs, tc_shape[1], tc_shape[2]))\n",
    "        phase_tuningcurves = np.zeros((n_passes, tc_shape[0], tc_shape[1], tc_shape[2]))\n",
    "        for n_pass in range(n_passes):\n",
    "\n",
    "            if n_shuffles > 0:\n",
    "                tuning_curves = np.random.permutation(tuning_curves_fromdata)\n",
    "            else:\n",
    "                tuning_curves = tuning_curves_fromdata\n",
    "\n",
    "            phase_tuningcurves[n_pass, ] = tuning_curves\n",
    "            tuning_curves = tuning_curves.reshape(tc_shape[0], tc_shape[1] * tc_shape[2])\n",
    "\n",
    "            for n_timebin, (start, stop) in enumerate(zip(phase_swrs.starts,\n",
    "                                                          phase_swrs.stops)):\n",
    "                t_window = stop-start  # 0.1 for running, 0.025 for swr\n",
    "\n",
    "                sliced_spikes = [spiketrain.time_slice(start, stop) for spiketrain in spikes]\n",
    "\n",
    "                counts = bin_spikes(sliced_spikes, np.array([start, stop]), dt=t_window, window=t_window,\n",
    "                                    gaussian_std=0.0075, normalized=False)\n",
    "\n",
    "                likelihood = nept.bayesian_prob(counts, tuning_curves, binsize=t_window,\n",
    "                                                min_neurons=3, min_spikes=1)\n",
    "\n",
    "                phase_likelihoods[n_pass, n_timebin] = likelihood.reshape(tc_shape[1], tc_shape[2])\n",
    "\n",
    "        tasktime = getattr(session, task_label)\n",
    "        tasktime.likelihoods = phase_likelihoods\n",
    "        tasktime.tuning_curves = phase_tuningcurves\n",
    "        tasktime.swrs = phase_swrs\n",
    "\n",
    "    if save_path is not None:\n",
    "        session.pickle(save_path)\n",
    "\n",
    "    return session\n",
    "\n",
    "\n",
    "def plot_likelihood_overspace(info, session, task_labels,  colours, filepath=None):\n",
    "    for task_label in task_labels:\n",
    "        zones = getattr(session, task_label).zones\n",
    "        likelihood = np.nanmean(np.array(getattr(session, task_label).likelihoods), axis=(0, 1))\n",
    "\n",
    "        likelihood[np.isnan(likelihood)] = 0\n",
    "\n",
    "        xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "        xcenters, ycenters = get_bin_centers(info)\n",
    "        xxx, yyy = np.meshgrid(xcenters, ycenters)\n",
    "\n",
    "        maze_highlight = \"#fed976\"\n",
    "        plt.plot(session.position.x, session.position.y, \".\", color=maze_highlight, ms=1, alpha=0.2)\n",
    "        pp = plt.pcolormesh(xx, yy, likelihood, cmap='bone_r')\n",
    "        for label in [\"u\", \"shortcut\", \"novel\"]:\n",
    "            plt.contour(xxx, yyy, zones[label], levels=0, linewidths=2, colors=colours[label])\n",
    "        plt.colorbar(pp)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if filepath is not None:\n",
    "            filename = info.session_id + \"_\" + task_label + \"_likelihoods-overspace.png\"\n",
    "            plt.savefig(os.path.join(output_filepath, filename))\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def plot_summary_individual(info, session_true, session_shuffled, zone_labels, task_labels, colours, filepath=None):\n",
    "    _, position, spikes, lfp, _ = get_data(info)\n",
    "\n",
    "    buffer = 0.1\n",
    "\n",
    "    for task_label in task_labels:\n",
    "        swrs = getattr(session_true, task_label).swrs\n",
    "        zones = getattr(session_true, task_label).zones\n",
    "\n",
    "        for swr_idx in range(swrs.n_epochs):\n",
    "            start = swrs[swr_idx].start\n",
    "            stop = swrs[swr_idx].stop\n",
    "\n",
    "            sliced_spikes = [spiketrain.time_slice(start - buffer, stop + buffer) for spiketrain in spikes]\n",
    "\n",
    "            ms = 600 / len(sliced_spikes)\n",
    "            mew = 0.7\n",
    "            spike_loc = 1\n",
    "\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            gs1 = gridspec.GridSpec(3, 2)\n",
    "            gs1.update(wspace=0.3, hspace=0.3)\n",
    "\n",
    "            ax1 = plt.subplot(gs1[1:, 0])\n",
    "            for idx, neuron_spikes in enumerate(sliced_spikes):\n",
    "                ax1.plot(neuron_spikes.time, np.ones(len(neuron_spikes.time)) + (idx * spike_loc), '|',\n",
    "                         color='k', ms=ms, mew=mew)\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(gs1[0, 0], sharex=ax1)\n",
    "\n",
    "            swr_highlight = \"#fc4e2a\"\n",
    "            start_idx = nept.find_nearest_idx(lfp.time, start - buffer)\n",
    "            stop_idx = nept.find_nearest_idx(lfp.time, stop + buffer)\n",
    "            ax2.plot(lfp.time[start_idx:stop_idx], lfp.data[start_idx:stop_idx], color=\"k\", lw=0.3, alpha=0.9)\n",
    "\n",
    "            start_idx = nept.find_nearest_idx(lfp.time, start)\n",
    "            stop_idx = nept.find_nearest_idx(lfp.time, stop)\n",
    "            ax2.plot(lfp.time[start_idx:stop_idx], lfp.data[start_idx:stop_idx], color=swr_highlight, lw=0.6)\n",
    "            ax2.axis(\"off\")\n",
    "\n",
    "            ax1.axvline(lfp.time[start_idx], linewidth=1, color=swr_highlight)\n",
    "            ax1.axvline(lfp.time[stop_idx], linewidth=1, color=swr_highlight)\n",
    "            ax1.axvspan(lfp.time[start_idx], lfp.time[stop_idx], alpha=0.2, color=swr_highlight)\n",
    "\n",
    "            scalebar.add_scalebar(ax2, matchy=False, bbox_transform=fig.transFigure,\n",
    "                                  bbox_to_anchor=(0.25, 0.05), units='ms')\n",
    "\n",
    "            likelihood_true = np.array(getattr(session_true, task_label).likelihoods[:, swr_idx])\n",
    "\n",
    "            likelihood_true[np.isnan(likelihood_true)] = 0\n",
    "\n",
    "            xx, yy = np.meshgrid(info.xedges, info.yedges)\n",
    "            xcenters, ycenters = get_bin_centers(info)\n",
    "            xxx, yyy = np.meshgrid(xcenters, ycenters)\n",
    "\n",
    "            maze_highlight = \"#fed976\"\n",
    "            ax3 = plt.subplot(gs1[0, 1])\n",
    "\n",
    "            ax3.plot(session_true.position.x, session_true.position.y, \".\",\n",
    "                     color=maze_highlight, ms=1, alpha=0.2)\n",
    "            pp = ax3.pcolormesh(xx, yy, likelihood_true[0], cmap='bone_r')\n",
    "            for label in [\"u\", \"shortcut\", \"novel\"]:\n",
    "                ax3.contour(xxx, yyy, zones[label], levels=0, linewidths=2, colors=colours[label])\n",
    "            plt.colorbar(pp)\n",
    "            ax3.axis('off')\n",
    "\n",
    "            likelihood_true = getattr(session_true, task_label).likelihoods[:, swr_idx]\n",
    "            means_true = [np.nanmean(np.nansum(likelihood_true[:, zones[zone_label]], axis=1))\n",
    "                          for zone_label in zone_labels]\n",
    "\n",
    "            ax4 = plt.subplot(gs1[1:2, 1])\n",
    "            ax4.bar(np.arange(len(zone_labels)),\n",
    "                    means_true,\n",
    "                    color=[colours[zone_label] for zone_label in zone_labels], edgecolor='k')\n",
    "            ax4.set_xticks(np.arange(len(zone_labels)))\n",
    "            ax4.set_xticklabels([], rotation=90)\n",
    "            ax4.set_ylim([0, 1.])\n",
    "            ax4.set_title(\"True proportion\", fontsize=14)\n",
    "\n",
    "            likelihood_shuffled = getattr(session_shuffled, task_label).likelihoods[:, swr_idx]\n",
    "\n",
    "            means_shuffled = [np.nanmean(np.nansum(likelihood_shuffled[:, zones[zone_label]], axis=1))\n",
    "                              for zone_label in zone_labels]\n",
    "            sems_shuffled = [scipy.stats.sem(np.nansum(likelihood_shuffled[:, zones[zone_label]], axis=1))\n",
    "                             for zone_label in zone_labels]\n",
    "\n",
    "            ax5 = plt.subplot(gs1[2:, 1], sharey=ax4)\n",
    "            ax5.bar(np.arange(len(zone_labels)),\n",
    "                    means_shuffled,\n",
    "                    yerr=sems_shuffled,\n",
    "                    color=[colours[zone_label] for zone_label in zone_labels], edgecolor='k')\n",
    "            ax5.set_xticks(np.arange(len(zone_labels)))\n",
    "            ax5.set_xticklabels(zone_labels, rotation=90)\n",
    "            ax5.set_ylim([0, 1.])\n",
    "            ax5.set_title(\"Shuffled proportion\", fontsize=14)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if filepath is not None:\n",
    "                filename = info.session_id + \"_\" + task_label + \"_summary-swr\" + str(swr_idx) + \".png\"\n",
    "                plt.savefig(os.path.join(filepath, filename))\n",
    "                plt.close()\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "def plot_session(sessions, title, filepath=None):\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs1 = gridspec.GridSpec(1, 4)\n",
    "    gs1.update(wspace=0.3, hspace=0.)\n",
    "\n",
    "    for i, zone_label in enumerate(zone_labels):\n",
    "        sums = {task_label: [] for task_label in task_labels}\n",
    "        n_swrs = {task_label: 0 for task_label in task_labels}\n",
    "        for session in sessions:\n",
    "            for task_label in task_labels:\n",
    "                zone_sums = getattr(session, task_label).sums(zone_label)\n",
    "                sums[task_label].extend(zone_sums)\n",
    "                n_swrs[task_label] += getattr(session, task_label).swrs.n_epochs\n",
    "\n",
    "        means = [np.nanmean(sums[task_label])\n",
    "                 if len(sums[task_label]) > 0 else 0.0\n",
    "                 for task_label in task_labels]\n",
    "\n",
    "        sems = [np.nanmean(scipy.stats.sem(sums[task_label], axis=0, nan_policy=\"omit\"))\n",
    "                if len(sums[task_label]) > 1 else 0.0\n",
    "                for task_label in task_labels]\n",
    "\n",
    "        ax = plt.subplot(gs1[i])\n",
    "        ax.bar(np.arange(sessions[0].n_tasktimes()),\n",
    "               means, yerr=sems, color=colours[zone_label])\n",
    "\n",
    "        ax.set_ylim([0, 1.])\n",
    "\n",
    "        ax.set_xticks(np.arange(sessions[0].n_tasktimes()))\n",
    "        ax.set_xticklabels(task_labels, rotation=90)\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "        if i > 0:\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Proportion\")\n",
    "\n",
    "        if zone_label == \"other\":\n",
    "            for n_tasktimes, task_label in enumerate(task_labels):\n",
    "                ax.text(n_tasktimes, 0.01, str(n_swrs[task_label]), ha=\"center\", fontsize=14)\n",
    "\n",
    "    plt.text(1., 1., \"n sessions: \"+ str(len(sessions)), horizontalalignment='left',\n",
    "             verticalalignment='top', fontsize=14)\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    legend_elements = [Patch(facecolor=colours[zone_label], edgecolor='k', label=zone_label)\n",
    "                       for zone_label in zone_labels]\n",
    "\n",
    "    plt.legend(handles=legend_elements, bbox_to_anchor=(1., 0.95))\n",
    "\n",
    "    gs1.tight_layout(fig)\n",
    "\n",
    "    if filepath is not None:\n",
    "        plt.savefig(filepath)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from run import (analysis_infos,\n",
    "                     r063_infos, r066_infos, r067_infos, r068_infos,\n",
    "                     days1234_infos, days5678_infos,\n",
    "                     day1_infos, day2_infos, day3_infos, day4_infos, day5_infos, day6_infos, day7_infos, day8_infos)\n",
    "    import info.r063d2 as r063d2\n",
    "    import info.r068d8 as r068d8\n",
    "    infos = [r068d8, r063d2]\n",
    "    group = \"test\"\n",
    "\n",
    "    update_cache = True\n",
    "    dont_save_pickle = False\n",
    "    plot_individual = False\n",
    "    plot_individual_passthresh = False\n",
    "    plot_overspace = False\n",
    "    plot_summary = True\n",
    "\n",
    "    n_shuffles = 2\n",
    "    percentile_thresh = 95\n",
    "\n",
    "    colours = dict()\n",
    "    colours[\"u\"] = \"#2b8cbe\"\n",
    "    colours[\"shortcut\"] = \"#31a354\"\n",
    "    colours[\"novel\"] = \"#d95f0e\"\n",
    "    colours[\"other\"] = \"#bdbdbd\"\n",
    "\n",
    "    # swr params\n",
    "    swr_params = dict()\n",
    "    swr_params[\"z_thresh\"] = 2.0\n",
    "    swr_params[\"power_thresh\"] = 3.0\n",
    "    swr_params[\"merge_thresh\"] = 0.02\n",
    "    swr_params[\"min_length\"] = 0.05\n",
    "    swr_params[\"swr_thresh\"] = (140.0, 250.0)\n",
    "    swr_params[\"min_involved\"] = 4\n",
    "\n",
    "    task_labels = [\"prerecord\", \"pauseA\", \"pauseB\", \"postrecord\"]\n",
    "    zone_labels = [\"u\", \"shortcut\", \"novel\", \"other\"]\n",
    "\n",
    "    true_sessions = []\n",
    "    shuffled_sessions = []\n",
    "    passthresh_sessions = []\n",
    "\n",
    "    for info in infos:\n",
    "        print(info.session_id)\n",
    "\n",
    "        # Get true data\n",
    "        true_path = os.path.join(pickle_filepath, info.session_id+\"_likelihoods_true.pkl\")\n",
    "\n",
    "        # Remove previous pickle if update_cache\n",
    "        if update_cache:\n",
    "            if os.path.exists(true_path):\n",
    "                os.remove(true_path)\n",
    "\n",
    "        # Load pickle if it exists, otherwise compute and pickle\n",
    "        if os.path.exists(true_path):\n",
    "            print(\"Loading pickled true likelihoods...\")\n",
    "            compute_likelihoods = False\n",
    "            with open(true_path, 'rb') as fileobj:\n",
    "                true_session = pickle.load(fileobj)\n",
    "        else:\n",
    "            if dont_save_pickle:\n",
    "                true_path = None\n",
    "            true_session = get_likelihoods(info,\n",
    "                                           swr_params,\n",
    "                                           task_labels,\n",
    "                                           zone_labels,\n",
    "                                           save_path=true_path)\n",
    "\n",
    "        true_sessions.append(true_session)\n",
    "\n",
    "        # Get shuffled data\n",
    "        shuffled_path = os.path.join(pickle_filepath,\n",
    "                                     info.session_id+\"_likelihoods_shuffled-%03d.pkl\" % n_shuffles)\n",
    "\n",
    "        # Remove previous pickle if update_cache\n",
    "        if update_cache:\n",
    "            if os.path.exists(shuffled_path):\n",
    "                os.remove(shuffled_path)\n",
    "\n",
    "        # Load pickle if it exists, otherwise compute and pickle\n",
    "        if os.path.exists(shuffled_path):\n",
    "            print(\"Loading pickled shuffled likelihoods...\")\n",
    "            with open(shuffled_path, 'rb') as fileobj:\n",
    "                shuffled_session = pickle.load(fileobj)\n",
    "        else:\n",
    "            if dont_save_pickle:\n",
    "                shuffled_path = None\n",
    "            shuffled_session = get_likelihoods(info,\n",
    "                                               swr_params,\n",
    "                                               task_labels,\n",
    "                                               zone_labels,\n",
    "                                               n_shuffles=n_shuffles,\n",
    "                                               save_path=shuffled_path)\n",
    "\n",
    "        shuffled_sessions.append(shuffled_session)\n",
    "\n",
    "        if plot_individual:\n",
    "            filepath = os.path.join(output_filepath, \"individual\")\n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)\n",
    "            plot_summary_individual(info, true_session, shuffled_session,\n",
    "                                    zone_labels, task_labels, colours, filepath)\n",
    "\n",
    "        if plot_overspace:\n",
    "            filepath = os.path.join(output_filepath, \"overspace\")\n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)\n",
    "            plot_likelihood_overspace(info, true_session, task_labels, colours, filepath)\n",
    "\n",
    "        keep_idx = {task_label: [] for task_label in task_labels}\n",
    "\n",
    "        for task_label in task_labels:\n",
    "            for zone_label in zone_labels:\n",
    "                zones = getattr(true_session, task_label).zones\n",
    "                true_sums = np.array(getattr(true_session, task_label).sums(zone_label))\n",
    "                shuffled_sums = np.array(getattr(shuffled_session, task_label).sums(zone_label))\n",
    "                if true_sums.size == 1 and np.isnan(true_sums).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    for idx in range(true_sums.shape[0]):\n",
    "                        percentile = scipy.stats.percentileofscore(np.sort(shuffled_sums[:, idx]), true_sums[idx])\n",
    "                        if percentile >= percentile_thresh:\n",
    "                            keep_idx[task_label].append(idx)\n",
    "\n",
    "        passthresh_session = Session(true_session.position, task_labels, zones)\n",
    "\n",
    "        for task_label in task_labels:\n",
    "            passthresh_idx = np.sort(np.unique(keep_idx[task_label]))\n",
    "            if len(passthresh_idx) > 0:\n",
    "                passthresh_likelihoods = np.array(getattr(true_session, task_label).likelihoods)[:, passthresh_idx]\n",
    "                passthresh_swrs = getattr(true_session, task_label).swrs[passthresh_idx]\n",
    "            else:\n",
    "                passthresh_likelihoods = np.ones((1, 1) + getattr(true_session, task_label).likelihoods.shape[2:]) * np.nan\n",
    "                passthresh_swrs = None\n",
    "\n",
    "            passthresh_tuningcurves = getattr(true_session, task_label).tuning_curves\n",
    "\n",
    "            tasktime = getattr(passthresh_session, task_label)\n",
    "            tasktime.likelihoods = passthresh_likelihoods\n",
    "            tasktime.swrs = passthresh_swrs\n",
    "            tasktime.tuning_curves = passthresh_tuningcurves\n",
    "\n",
    "        passthresh_sessions.append(passthresh_session)\n",
    "\n",
    "        if plot_individual_passthresh:\n",
    "            filepath = os.path.join(output_filepath, \"passthresh\")\n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)\n",
    "            plot_summary_individual(info, passthresh_session, shuffled_session, zone_labels,\n",
    "                                    task_labels, colours, filepath)\n",
    "\n",
    "    if plot_summary:\n",
    "#         title = group + \"_average-posterior-during-SWRs_true\"\n",
    "#         filepath = os.path.join(output_filepath, title+\".png\")\n",
    "#         plot_session(true_sessions, title, filepath)\n",
    "\n",
    "        title = group + \"_average-posterior-during-SWRs_shuffled-%03d\" % n_shuffles\n",
    "        filepath = os.path.join(output_filepath, title+\".png\")\n",
    "        plot_session(shuffled_sessions, title)\n",
    "\n",
    "        title = group + \"_average-posterior-during-SWRs_passthresh\"\n",
    "        filepath = os.path.join(output_filepath, title + \".png\")\n",
    "        plot_session(passthresh_sessions, title, filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T19:58:24.387330Z",
     "start_time": "2018-09-28T19:58:24.381352Z"
    }
   },
   "outputs": [],
   "source": [
    "np.isnan(true_sums).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T19:58:25.347716Z",
     "start_time": "2018-09-28T19:58:25.342700Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[np.nan]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T19:58:29.787880Z",
     "start_time": "2018-09-28T19:58:29.774907Z"
    }
   },
   "outputs": [],
   "source": [
    "np.ones((true_session.pauseA.likelihoods.shape[0]), 1) * np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T19:58:31.195375Z",
     "start_time": "2018-09-28T19:58:31.190379Z"
    }
   },
   "outputs": [],
   "source": [
    "true_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T19:58:35.762664Z",
     "start_time": "2018-09-28T19:58:35.749691Z"
    }
   },
   "outputs": [],
   "source": [
    "for session in true_sessions:\n",
    "    for task_label in task_labels:\n",
    "        zone_sums = getattr(session, task_label).sums(zone_label)\n",
    "        print(zone_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T19:05:23.239158Z",
     "start_time": "2018-09-28T19:05:23.235161Z"
    }
   },
   "outputs": [],
   "source": [
    "zone_sums[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
